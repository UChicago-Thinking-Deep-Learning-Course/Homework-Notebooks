{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TDL8.1_sampling_human_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-LKD78Z3kjS"
      },
      "source": [
        "# TDL 8.1 Thinking with Deep Learning: Week 8 Part 1\n",
        "# Sampling and Reliability \n",
        "\n",
        "__Instructor:__ James Evans\n",
        "\n",
        "__Teaching Assistants & Content Creators/Organisers:__ Bhargav Srinivasa Desikan (Notebook Author), James Evans (Notebook Editor), Likun Cao (Notebook Reviewer)\n",
        "\n",
        "In this notebook we will explore some crucial aspects of dealing with data - sampling, annotations, and reliability. This will lay the groundwork for when we begin to work with Active Learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fbtABCrKtr7"
      },
      "source": [
        "# empty cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oZjw0cFK45m"
      },
      "source": [
        "# Sampling \n",
        "\n",
        "[Sampling](https://en.wikipedia.org/wiki/Sampling_(statistics)) is a popular method which we've likely come across in our previous data science or social science explorations. In this section we will be exploring some of the more popular techniques that are used in industry and research. There is no one standard package for sampling with python, and we will be using the rich PyData ecosystem in different ways to get what we want done.\n",
        "\n",
        "Here are some links you may wish to explore:\n",
        "\n",
        "- Krippendorff, Klaus. 2004. Content Analysis: An Introduction to its Methodology. Thousand Oaks, CA: Sage: [“Sampling”](https://canvas.uchicago.edu/courses/33672/files/4767016/download?wrap=1) \n",
        "- [Data Scientist's guide to 8 sampling techniques](https://www.analyticsvidhya.com/blog/2019/09/data-scientists-guide-8-types-of-sampling-techniques/\n",
        ")\n",
        "- [KDnuggets - 5 sampling algorithms](https://www.kdnuggets.com/2019/09/5-sampling-algorithms.html)\n",
        "\n",
        "Sampling procedures are often divided into probabilistic and non-probabilistic methods, and we will start with the same division before jumping into more machine and deep learning relevant methods.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2Grz6YyclG4"
      },
      "source": [
        "## Probabilistic Sampling\n",
        "\n",
        "A lot of times we hear about sampling in the context of sampling from a probability distribution. Such methods can often, if not always, be extended to sampling from real world data. For the examples in this section, we will use both a real world dataset, as well as sample from random distributions, both of which are useful tools. The power of these methods in the context of machine learning and deep learning (and its human aspects!) will become more clear as we go through them.  \n",
        "\n",
        "### Dataset\n",
        "\n",
        "One of the datasets we will be using is the \"Mental Health in Tech Survey\" data, which is an open source survey data about mental health conditions of workers in the tech industries. We also worked with this data for the week 5 hint. You can find the data at Kaggle:\n",
        "\n",
        "https://www.kaggle.com/osmi/mental-health-in-tech-survey\n",
        "\n",
        "[google drive link of cleaned data](https://drive.google.com/file/d/1-0r2C4z9-vAedJ4uum-TfI4Zy4gKDGaQ/view?usp=sharing)\n",
        "\n",
        "We provided a cleaned version of this data. The predictors contain 1 continuous variable (age), 3 dummies (Do you work remotely? Is your employer primarily a tech company? Does your employer provide any mental health benefits?) and 2 categorical variables (gender-male/female/other; can you discuss your mental health issue with supervisors-yes/sometimes/no).\n",
        "\n",
        "The outcome is an answer to the question: If you have a mental health condition, do you feel it interferes with your work? The DV is measured with a 5-categorical variable: NA (no mental health condition), never, rarely, sometimes, often. \n",
        "\n",
        "Don't worry too much about the data - it's just useful for us to have a dataset to refer to when we are sampling. **An important note**: very often we are sampling from a very large dataset or population - in this case, our dataset is small enough that we can use the whole dataset, and the sampling is purely illustratory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR0Nx7nuzZdC"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l65fxXujzSuV"
      },
      "source": [
        "df=pd.read_csv('/content/mental health.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "t1EOtp5f0hX6",
        "outputId": "d3db71f8-c3c3-4311-c8a5-8ed524009ae7"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>remote</th>\n",
              "      <th>benefits</th>\n",
              "      <th>tech</th>\n",
              "      <th>gender</th>\n",
              "      <th>supervisor</th>\n",
              "      <th>interfere</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>yes</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>some of them</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>no</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>some of them</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>yes</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1254</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>yes</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1255</th>\n",
              "      <td>26</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>some of them</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1256</th>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>yes</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1257</th>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>yes</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1258</th>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>female</td>\n",
              "      <td>some of them</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1259 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      age  remote  benefits  tech  gender    supervisor  interfere\n",
              "0       8       1         1     1   other           yes          4\n",
              "1      21       0         0     1   other  some of them          3\n",
              "2      32       0         0     1   other            no          4\n",
              "3      28       0         0     1   other  some of them          2\n",
              "4      27       1         1     1   other           yes          4\n",
              "...   ...     ...       ...   ...     ...           ...        ...\n",
              "1254   32       1         1     1   other           yes          4\n",
              "1255   26       1         0     1   other  some of them          3\n",
              "1256   30       0         1     1   other           yes          2\n",
              "1257   18       1         1     1   other           yes          0\n",
              "1258   34       0         1     0  female  some of them          2\n",
              "\n",
              "[1259 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ETCWp78mk4U"
      },
      "source": [
        "### Random\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRRThElqmlOi"
      },
      "source": [
        "sample_df = df.sample(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "IRhtmI060iqb",
        "outputId": "916a46b3-1432-4c89-e3ee-d06c21be2c6d"
      },
      "source": [
        "sample_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>remote</th>\n",
              "      <th>benefits</th>\n",
              "      <th>tech</th>\n",
              "      <th>gender</th>\n",
              "      <th>supervisor</th>\n",
              "      <th>interfere</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>896</th>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>male</td>\n",
              "      <td>no</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>yes</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1125</th>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>some of them</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>644</th>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>male</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1124</th>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>737</th>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>yes</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>236</th>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>some of them</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>915</th>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>some of them</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      age  remote  benefits  tech  gender    supervisor  interfere\n",
              "896    25       1         0     0    male            no          4\n",
              "79     28       0         1     1  female           yes          0\n",
              "1125   35       0         0     1    male           yes          3\n",
              "214    23       1         1     1    male  some of them          0\n",
              "644    37       0         1     0    male            no          0\n",
              "...   ...     ...       ...   ...     ...           ...        ...\n",
              "1124   25       0         0     1    male            no          0\n",
              "737    29       0         1     1    male           yes          4\n",
              "122    26       0         1     1  female           yes          3\n",
              "236    40       0         1     1    male  some of them          1\n",
              "915    56       0         1     1   other  some of them          2\n",
              "\n",
              "[100 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGvzo0Eu02Ej"
      },
      "source": [
        "[SciPy](https://docs.scipy.org/doc/numpy-1.10.1/reference/routines.random.html) and [numpy](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) also offer many popular sampling techniques, usually to be applied on a list or array, or sampled from a distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9HBR4xz0z2o"
      },
      "source": [
        "import numpy as np\n",
        "import scipy as sp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dhjhlv828nP"
      },
      "source": [
        "random_vals = np.random.rand(500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBTR-LFg3E9k",
        "outputId": "4b80d0bc-e2fd-46e4-aeaf-53c300fe9632"
      },
      "source": [
        "random_vals"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.43699714, 0.47060992, 0.24905942, 0.22239904, 0.44296175,\n",
              "       0.22545222, 0.22134929, 0.96157734, 0.62211869, 0.44807806,\n",
              "       0.50607784, 0.73835084, 0.9380274 , 0.45296616, 0.29843266,\n",
              "       0.13536347, 0.25888749, 0.05993663, 0.60588416, 0.41785586,\n",
              "       0.06071734, 0.15378653, 0.88856166, 0.37703047, 0.02557786,\n",
              "       0.33333586, 0.41360987, 0.09058795, 0.98022939, 0.2889466 ,\n",
              "       0.05141591, 0.65477716, 0.26518663, 0.43644931, 0.87321368,\n",
              "       0.94961348, 0.19790819, 0.91365039, 0.46810631, 0.45035853,\n",
              "       0.42123755, 0.72920887, 0.55388844, 0.7647531 , 0.51071366,\n",
              "       0.64269204, 0.42339985, 0.73069075, 0.50256078, 0.52905678,\n",
              "       0.48488404, 0.72724541, 0.40514898, 0.77297362, 0.23124549,\n",
              "       0.86063728, 0.76898662, 0.4613524 , 0.04014646, 0.1411483 ,\n",
              "       0.13068955, 0.32412105, 0.63241511, 0.16702814, 0.30841922,\n",
              "       0.43568952, 0.8862674 , 0.20357862, 0.85145767, 0.76444359,\n",
              "       0.92301258, 0.69283349, 0.2460816 , 0.25595245, 0.51492928,\n",
              "       0.96346586, 0.67589064, 0.84456943, 0.29518401, 0.64207654,\n",
              "       0.53567599, 0.93375216, 0.87844311, 0.58708958, 0.82256011,\n",
              "       0.98747488, 0.09223062, 0.00807503, 0.58997761, 0.46053584,\n",
              "       0.40432682, 0.30033025, 0.87831919, 0.60897061, 0.14258206,\n",
              "       0.02237976, 0.29945886, 0.33273905, 0.90560039, 0.42191088,\n",
              "       0.40306636, 0.5581488 , 0.7270466 , 0.04759426, 0.82775892,\n",
              "       0.66465713, 0.14819651, 0.5167319 , 0.02584536, 0.24582874,\n",
              "       0.00812881, 0.61208845, 0.90139769, 0.445858  , 0.56673699,\n",
              "       0.22805436, 0.19234755, 0.25163646, 0.28854493, 0.42402673,\n",
              "       0.04297532, 0.14349239, 0.35538373, 0.29901894, 0.86967401,\n",
              "       0.11537545, 0.95278292, 0.57400751, 0.18898313, 0.84981662,\n",
              "       0.94977486, 0.48383479, 0.50599085, 0.84084175, 0.90103577,\n",
              "       0.62141442, 0.65177083, 0.57926548, 0.58982743, 0.8588135 ,\n",
              "       0.29129295, 0.99024506, 0.24061585, 0.3546913 , 0.65368296,\n",
              "       0.27133834, 0.52197435, 0.06301272, 0.32552216, 0.75381158,\n",
              "       0.38714816, 0.02573275, 0.73007384, 0.6808883 , 0.83525187,\n",
              "       0.25707184, 0.8682889 , 0.13438765, 0.01322433, 0.40280584,\n",
              "       0.9549389 , 0.78621249, 0.77395448, 0.50812382, 0.75053042,\n",
              "       0.96133861, 0.42538108, 0.95107283, 0.15771461, 0.81409243,\n",
              "       0.83780817, 0.3381522 , 0.13720292, 0.32716402, 0.45090672,\n",
              "       0.34876947, 0.05930255, 0.01893277, 0.43604127, 0.61657872,\n",
              "       0.9920094 , 0.91505117, 0.06317809, 0.31274479, 0.13422667,\n",
              "       0.60086681, 0.60570188, 0.66511434, 0.01414913, 0.49282587,\n",
              "       0.47919519, 0.26775588, 0.53231719, 0.57748821, 0.78890931,\n",
              "       0.62794373, 0.41271794, 0.28746695, 0.57618915, 0.99072712,\n",
              "       0.5445257 , 0.28060734, 0.51757684, 0.11706135, 0.5833084 ,\n",
              "       0.91605404, 0.48204715, 0.73843773, 0.42541294, 0.90090089,\n",
              "       0.65639869, 0.25486589, 0.90789598, 0.90809275, 0.97171028,\n",
              "       0.22423733, 0.90999521, 0.66151348, 0.26194127, 0.66523002,\n",
              "       0.51574851, 0.02317713, 0.90320914, 0.33904532, 0.99727021,\n",
              "       0.4710648 , 0.81365892, 0.50005898, 0.3730483 , 0.24354601,\n",
              "       0.24593067, 0.35299883, 0.92242584, 0.86324499, 0.07558839,\n",
              "       0.36409949, 0.40210696, 0.70589652, 0.50957204, 0.71458589,\n",
              "       0.7861445 , 0.17498564, 0.94862431, 0.71674611, 0.34143064,\n",
              "       0.09438175, 0.5730382 , 0.62668347, 0.94296452, 0.03837618,\n",
              "       0.98748676, 0.30754117, 0.35661138, 0.71609558, 0.82864872,\n",
              "       0.38373349, 0.97966622, 0.26864325, 0.38718862, 0.66759884,\n",
              "       0.71272669, 0.0769136 , 0.10879657, 0.46060766, 0.75281699,\n",
              "       0.81244016, 0.73878709, 0.71999639, 0.98231736, 0.95253342,\n",
              "       0.81669368, 0.7120451 , 0.93819458, 0.53210415, 0.61455783,\n",
              "       0.76128631, 0.60869006, 0.02287249, 0.22135481, 0.52379826,\n",
              "       0.80426509, 0.68798266, 0.47598006, 0.51256815, 0.45309324,\n",
              "       0.62324616, 0.04492432, 0.54608757, 0.37754439, 0.30491629,\n",
              "       0.23058739, 0.02965379, 0.58554641, 0.34657333, 0.79148046,\n",
              "       0.34131513, 0.08991888, 0.02197477, 0.32070658, 0.13464174,\n",
              "       0.19755114, 0.37430611, 0.32914927, 0.60791166, 0.91865047,\n",
              "       0.45722863, 0.21614539, 0.66207762, 0.14665147, 0.46424769,\n",
              "       0.63370864, 0.31647978, 0.66589598, 0.61752548, 0.23717687,\n",
              "       0.19785748, 0.20080017, 0.22439719, 0.05580325, 0.40160998,\n",
              "       0.32925536, 0.01014992, 0.12970947, 0.83674432, 0.40968929,\n",
              "       0.80750426, 0.42382474, 0.05526605, 0.32226419, 0.11998206,\n",
              "       0.70951218, 0.18058599, 0.87993789, 0.92092189, 0.46827426,\n",
              "       0.78749102, 0.12554515, 0.34810803, 0.72932525, 0.16209192,\n",
              "       0.27608719, 0.48189153, 0.22184123, 0.57966104, 0.20628298,\n",
              "       0.56807645, 0.15836391, 0.84672772, 0.2127533 , 0.487098  ,\n",
              "       0.72228238, 0.24240901, 0.66927279, 0.33574947, 0.35932238,\n",
              "       0.56920229, 0.45642692, 0.83114504, 0.0879558 , 0.70498178,\n",
              "       0.49981496, 0.04522059, 0.02159684, 0.07251009, 0.68176993,\n",
              "       0.24041892, 0.51388567, 0.38940558, 0.10193747, 0.81190901,\n",
              "       0.44747603, 0.11472192, 0.69995115, 0.68559019, 0.66382606,\n",
              "       0.25173741, 0.81348716, 0.72437176, 0.74722682, 0.51968108,\n",
              "       0.80034671, 0.6601833 , 0.77948091, 0.9014056 , 0.06939051,\n",
              "       0.70816378, 0.30082898, 0.78685338, 0.36310668, 0.48346889,\n",
              "       0.31665783, 0.48955268, 0.94799983, 0.99913656, 0.87422769,\n",
              "       0.60223953, 0.3391899 , 0.35331815, 0.44680208, 0.17449005,\n",
              "       0.90526674, 0.84973993, 0.52902779, 0.92271613, 0.79534691,\n",
              "       0.70517574, 0.7011417 , 0.68248454, 0.0779934 , 0.85894243,\n",
              "       0.58758872, 0.68030334, 0.47681776, 0.33629721, 0.90921322,\n",
              "       0.96316363, 0.26950526, 0.69710617, 0.85017081, 0.33502169,\n",
              "       0.36459427, 0.54530825, 0.94252355, 0.35179948, 0.98076033,\n",
              "       0.46139426, 0.58822653, 0.52452637, 0.41930362, 0.71365631,\n",
              "       0.74533939, 0.83256995, 0.20569316, 0.70870666, 0.48670546,\n",
              "       0.9271254 , 0.11929868, 0.65545042, 0.44974437, 0.68400029,\n",
              "       0.73061646, 0.58337532, 0.61584602, 0.31896705, 0.49353051,\n",
              "       0.01911122, 0.44198689, 0.61590495, 0.23533011, 0.90009642,\n",
              "       0.42305963, 0.95686524, 0.03040861, 0.85085552, 0.26755242,\n",
              "       0.59582778, 0.0439409 , 0.74887848, 0.56992302, 0.81938222,\n",
              "       0.85950224, 0.93476535, 0.57141701, 0.77222584, 0.28163259,\n",
              "       0.92594488, 0.65063864, 0.05678431, 0.43422952, 0.57236327,\n",
              "       0.71441764, 0.72956594, 0.38921033, 0.63768938, 0.08429215,\n",
              "       0.46031196, 0.80648054, 0.47466093, 0.86272163, 0.34652514,\n",
              "       0.68612271, 0.29880189, 0.30172355, 0.83866243, 0.07550178,\n",
              "       0.74237736, 0.86417273, 0.81901099, 0.62499626, 0.40549994,\n",
              "       0.43571818, 0.19888839, 0.8305243 , 0.65522832, 0.26008441,\n",
              "       0.82310874, 0.96027421, 0.45245787, 0.43630392, 0.72267738])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsMjsDLc2H32",
        "outputId": "b315c161-3261-42ca-9771-6c833a7bd3df"
      },
      "source": [
        "np.random.choice(random_vals, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.73069075, 0.34131513, 0.80750426, 0.77948091, 0.9920094 ,\n",
              "       0.95253342, 0.81190901, 0.11929868, 0.75053042, 0.44680208])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg83S1girXg-"
      },
      "source": [
        "In this case, we first generated an array of size 500 by randomly sampling between 0 and 1, and then sampled 10 values from this list. We can similarly sample values from any list by using the scipy and numpy random module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZQ2GM6bmUGu"
      },
      "source": [
        "### Stratified\n",
        "\n",
        "Stratified random sampling is a method of sampling that involves the division of a population into smaller sub-groups known as strata. In stratified random sampling, or stratification, the strata are formed based on members' shared attributes or characteristics such as, in our example, age or interference. [Here](https://www.investopedia.com/terms/stratified_random_sampling.asp) is a source for reading more about it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N26WSOVjmWiL"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPB-iVc3n7U_"
      },
      "source": [
        "stratified_sample, _ = train_test_split(df, train_size=0.10, stratify=df[['remote']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "HA0rPAn6ojb6",
        "outputId": "02312f34-4e69-40fa-bbfb-917c79afc45f"
      },
      "source": [
        "stratified_sample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>remote</th>\n",
              "      <th>benefits</th>\n",
              "      <th>tech</th>\n",
              "      <th>gender</th>\n",
              "      <th>supervisor</th>\n",
              "      <th>interfere</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1176</th>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>yes</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>some of them</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>470</th>\n",
              "      <td>33</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>some of them</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>866</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>yes</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>26</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>no</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>some of them</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>125 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      age  remote  benefits  tech  gender    supervisor  interfere\n",
              "1176   38       0         0     1    male           yes          0\n",
              "86     30       1         0     1  female  some of them          0\n",
              "470    33       0         1     1    male  some of them          3\n",
              "866    58       0         1     1    male           yes          2\n",
              "71     26       1         0     1  female           yes          3\n",
              "...   ...     ...       ...   ...     ...           ...        ...\n",
              "221    27       0         0     1    male            no          2\n",
              "35     31       0         1     1  female           yes          3\n",
              "206    34       0         1     1    male            no          3\n",
              "568    30       1         1     1    male            no          0\n",
              "83     28       0         1     1  female  some of them          3\n",
              "\n",
              "[125 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd49ZfhxuODS"
      },
      "source": [
        "In this case we get stratified results for remote work, and our remote work attribute is represented as per the original ratio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU5mFfsqmXNP"
      },
      "source": [
        "### Varying probability sampling (weighted by variables of interest)\n",
        "\n",
        "We can also sample by weighting certain attributes so that our sample represents it accordingly. With Pandas we can do this using a DataFrame column as weights. Rows with larger value in the  column are more likely to be sampled.\n",
        "\n",
        "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjR6bzOgmXvE"
      },
      "source": [
        "weight_sample = df.sample(n=125, weights='age')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "eXA4XGSVSuic",
        "outputId": "a13d2216-4b94-416f-9050-042375399c1f"
      },
      "source": [
        "weight_sample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>remote</th>\n",
              "      <th>benefits</th>\n",
              "      <th>tech</th>\n",
              "      <th>gender</th>\n",
              "      <th>supervisor</th>\n",
              "      <th>interfere</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>578</th>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>some of them</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>611</th>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>no</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>female</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>521</th>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>yes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1018</th>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>yes</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>some of them</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>956</th>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>female</td>\n",
              "      <td>some of them</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>468</th>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>yes</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>125 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      age  remote  benefits  tech  gender    supervisor  interfere\n",
              "578    26       0         0     1    male  some of them          3\n",
              "502    32       0         0     1    male           yes          3\n",
              "55     38       0         1     1  female           yes          3\n",
              "611    23       0         0     1    male            no          3\n",
              "116    28       0         1     0  female            no          2\n",
              "...   ...     ...       ...   ...     ...           ...        ...\n",
              "521    28       0         0     1    male           yes          1\n",
              "1018   29       0         0     1    male           yes          0\n",
              "49     31       0         0     1  female  some of them          3\n",
              "956    34       0         1     0  female  some of them          3\n",
              "468    31       1         1     1    male           yes          0\n",
              "\n",
              "[125 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2c1lzdPS5qh",
        "outputId": "6a914eb4-fd13-4433-c52c-a51659bde618"
      },
      "source": [
        "weight_sample['age'].mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32.568"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BXKprSbS_a3",
        "outputId": "efca0d8a-d8ca-488d-c592-e025301eadea"
      },
      "source": [
        "df['age'].mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32.01906274821287"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjfqPLPeTKFQ"
      },
      "source": [
        "Because we weighted by age, older ages will be prioritised, which leads to that small bump in the mean age of the weighted sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgeOZuoymYPe"
      },
      "source": [
        "### Cluster Sampling (lists of large groups of units)\n",
        "\n",
        "Cluster sampling is a type of probability sampling in which every and each element of the population is selected equally, we use the subsets of the population as the sampling part rather than the individual elements for sampling.\n",
        "\n",
        "The population is divided into subsets or subgroups that are considered as clusters, and from the numbers of clusters, we select the individual cluster for the next step to be performed.\n",
        "\n",
        "You can read more about cluster sampling [here](https://www.geeksforgeeks.org/cluster-sampling-in-pandas/). \n",
        "\n",
        "There are a couple of ways to do cluster sampling - one way is to somehow cluster the dataset, and then choose that whole cluster as your sample. In this case, we will cluster based on age and then choose samples from one of these clusters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwsjdIbDcS0R"
      },
      "source": [
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5R2F3z2mYms"
      },
      "source": [
        "age_cluster = KMeans(n_clusters=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLTAIhV6c6fe",
        "outputId": "1f27a222-2701-4594-cc1e-39a5164ac129"
      },
      "source": [
        "age_cluster.fit(np.array(df['age']).reshape(-1, 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
              "       n_clusters=12, n_init=10, n_jobs=None, precompute_distances='auto',\n",
              "       random_state=None, tol=0.0001, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxs2u1krdHgC"
      },
      "source": [
        "df['cluster'] = age_cluster.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "RDzMZUagdpDL",
        "outputId": "14c98865-f1dd-4a68-839f-ed48409b7ec9"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>remote</th>\n",
              "      <th>benefits</th>\n",
              "      <th>tech</th>\n",
              "      <th>gender</th>\n",
              "      <th>supervisor</th>\n",
              "      <th>interfere</th>\n",
              "      <th>cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>yes</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>some of them</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>no</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>some of them</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>yes</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1254</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>yes</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1255</th>\n",
              "      <td>26</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>some of them</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1256</th>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>yes</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1257</th>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>yes</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1258</th>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>female</td>\n",
              "      <td>some of them</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1259 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      age  remote  benefits  tech  gender    supervisor  interfere  cluster\n",
              "0       8       1         1     1   other           yes          4       11\n",
              "1      21       0         0     1   other  some of them          3        4\n",
              "2      32       0         0     1   other            no          4        3\n",
              "3      28       0         0     1   other  some of them          2        8\n",
              "4      27       1         1     1   other           yes          4        1\n",
              "...   ...     ...       ...   ...     ...           ...        ...      ...\n",
              "1254   32       1         1     1   other           yes          4        3\n",
              "1255   26       1         0     1   other  some of them          3        1\n",
              "1256   30       0         1     1   other           yes          2        8\n",
              "1257   18       1         1     1   other           yes          0        5\n",
              "1258   34       0         1     0  female  some of them          2        7\n",
              "\n",
              "[1259 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "gZR0MrBRdqsU",
        "outputId": "4853c152-ddff-45da-8649-6dd1d6668a63"
      },
      "source": [
        "df[df['cluster'] == 3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>remote</th>\n",
              "      <th>benefits</th>\n",
              "      <th>tech</th>\n",
              "      <th>gender</th>\n",
              "      <th>supervisor</th>\n",
              "      <th>interfere</th>\n",
              "      <th>cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>no</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>33</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>no</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>no</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>33</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>some of them</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1229</th>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>some of them</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1238</th>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1247</th>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>no</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1250</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>no</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1254</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>yes</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>224 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      age  remote  benefits  tech  gender    supervisor  interfere  cluster\n",
              "2      32       0         0     1   other            no          4        3\n",
              "5      33       0         0     1   other            no          3        3\n",
              "8      31       0         0     1   other            no          3        3\n",
              "23     33       0         0     1  female           yes          3        3\n",
              "27     32       0         0     1  female  some of them          3        3\n",
              "...   ...     ...       ...   ...     ...           ...        ...      ...\n",
              "1229   31       0         1     1    male  some of them          3        3\n",
              "1238   32       0         0     1    male            no          0        3\n",
              "1247   32       0         1     1    male            no          3        3\n",
              "1250   32       1         0     1   other            no          3        3\n",
              "1254   32       1         1     1   other           yes          4        3\n",
              "\n",
              "[224 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "yY2Cc28peZXe",
        "outputId": "8789b890-a94b-4aa1-c216-32873fa8029e"
      },
      "source": [
        "df[df['cluster'] == 3].sample(10) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>remote</th>\n",
              "      <th>benefits</th>\n",
              "      <th>tech</th>\n",
              "      <th>gender</th>\n",
              "      <th>supervisor</th>\n",
              "      <th>interfere</th>\n",
              "      <th>cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>402</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>882</th>\n",
              "      <td>33</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>male</td>\n",
              "      <td>some of them</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1050</th>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>yes</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>818</th>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1107</th>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>yes</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>382</th>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>33</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>other</td>\n",
              "      <td>some of them</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>939</th>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>female</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>some of them</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1084</th>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>some of them</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      age  remote  benefits  tech  gender    supervisor  interfere  cluster\n",
              "402    32       1         0     1    male           yes          3        3\n",
              "882    33       0         0     0    male  some of them          1        3\n",
              "1050   31       0         0     1    male           yes          0        3\n",
              "818    31       1         0     1    male            no          2        3\n",
              "1107   32       0         0     1    male           yes          1        3\n",
              "382    31       0         0     1    male            no          0        3\n",
              "47     33       0         0     1   other  some of them          0        3\n",
              "939    32       0         1     0  female           yes          3        3\n",
              "56     32       0         0     1  female  some of them          3        3\n",
              "1084   31       0         1     1    male  some of them          3        3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar7nTWsvmY1h"
      },
      "source": [
        "### Systematic\n",
        "\n",
        "Systematic Sampling is defined as the type of Probability Sampling where a researcher can research on a targeted data from large set of data. Targeted data is chosen by selecting random starting point and from that after certain interval next element is chosen for sample. In this a small subset (sample) is extracted from large data.\n",
        "\n",
        "You can read more about systematic sampling [here](https://www.geeksforgeeks.org/systematic-sampling-in-pandas/).\n",
        "\n",
        "For an example using our current dataset, suppose we sample from only those who work remote."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ45C1xSmelz"
      },
      "source": [
        "remote_sample = df[df['remote'] == 1].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "pm_HwMF3mPbz",
        "outputId": "3dd95020-302e-463b-ef1c-7b3d1478fc14"
      },
      "source": [
        "remote_sample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>remote</th>\n",
              "      <th>benefits</th>\n",
              "      <th>tech</th>\n",
              "      <th>gender</th>\n",
              "      <th>supervisor</th>\n",
              "      <th>interfere</th>\n",
              "      <th>cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>469</th>\n",
              "      <td>36</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>962</th>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>no</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1232</th>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>yes</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>26</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>yes</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>no</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>603</th>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>male</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>838</th>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>no</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>35</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>male</td>\n",
              "      <td>no</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>some of them</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1094</th>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>some of them</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      age  remote  benefits  tech  gender    supervisor  interfere  cluster\n",
              "469    36       1         1     1    male            no          2        7\n",
              "962    25       1         0     1  female            no          4        1\n",
              "1232   39       1         1     1    male           yes          2        0\n",
              "763    26       1         0     1    male           yes          2        1\n",
              "193    39       1         0     1    male            no          1        0\n",
              "603    38       1         0     0    male            no          2        0\n",
              "838    27       1         0     1    male            no          1        1\n",
              "253    35       1         0     0    male            no          1        7\n",
              "61     41       1         1     1  female  some of them          2       10\n",
              "1094   45       1         0     1    male  some of them          1        6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UFHVlFWIz_t"
      },
      "source": [
        "### Graph Sampling\n",
        "\n",
        "Sometimes we come across data that needs to be sampled as a graph - we've seen earlier how we can also represent tables as graphs and vice-versa, so this should not be news to you. In this section we will explore some random ways of sampling from graphs. To illustrate these examples, we will be sampling from a graph based dataset. It is possible to extend these methods to your data that might not be (at first) represented in a graph-like way by restructuring your data in a way that supports these methods.\n",
        "\n",
        "We will be using a python package that we briefly touched upon earlier, [Little Ball of Fur](https://arxiv.org/pdf/2006.04311.pdf).\n",
        "\n",
        "#### Data\n",
        "\n",
        "We will be using a dataset based on GitHub data. In this graph nodes represent GitHub developers and the edges between them are mutual follower relationships. For details about the dataset see this [paper](https://arxiv.org/abs/1909.13021Z)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1OYZi_mOoz9",
        "outputId": "86ee40ed-a807-4fa7-f9ca-d1c658d6ce34"
      },
      "source": [
        "!pip install littleballoffur"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting littleballoffur\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/d9/8419d13ab504fe91bcc8e82d91a41abe25db9c810cdeab262cb36f35f4a8/littleballoffur-2.1.8.tar.gz\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.7/dist-packages (from littleballoffur) (3.12.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from littleballoffur) (0.29.23)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from littleballoffur) (2.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from littleballoffur) (4.41.1)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.7/dist-packages (from littleballoffur) (0.15)\n",
            "Collecting scikit-network\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/f4/403eabdee313432825b6a35f4004d2f1715242e30d372e7bc22c6dccc744/scikit_network-0.23.1-cp37-cp37m-manylinux2014_x86_64.whl (8.4MB)\n",
            "\u001b[K     |████████████████████████████████| 8.4MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from littleballoffur) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from littleballoffur) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from littleballoffur) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from littleballoffur) (1.4.1)\n",
            "Collecting networkit==7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/fc/dcc0888eb006c8c27fec37e872920c794421670a9819f1de47fdd62938f1/networkit-7.1.tar.gz (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 38.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->littleballoffur) (4.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->littleballoffur) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->littleballoffur) (2018.9)\n",
            "Building wheels for collected packages: littleballoffur, networkit\n",
            "  Building wheel for littleballoffur (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleballoffur: filename=littleballoffur-2.1.8-cp37-none-any.whl size=40212 sha256=d6e3cd9136de854a67b5f0909d27f0efb2d3e713ad63f10d0e44037e2221d5a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/09/b2/b2a80c4b1d51cc7de09f8de267455bcb40cd0b43c19dc79fa7\n",
            "  Building wheel for networkit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for networkit: filename=networkit-7.1-cp37-cp37m-linux_x86_64.whl size=8043401 sha256=b68e3b9d70f3e5d68fa934276a58935c1cd2727015a3d8118012d50014142ee7\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/ca/1c/a1096b1823f845c29784ce5106866b01eaad26d101a79032b8\n",
            "Successfully built littleballoffur networkit\n",
            "\u001b[31mERROR: scikit-network 0.23.1 has requirement numpy>=1.20.2, but you'll have numpy 1.19.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: scikit-network 0.23.1 has requirement scipy>=1.6.2, but you'll have scipy 1.4.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scikit-network, networkit, littleballoffur\n",
            "Successfully installed littleballoffur-2.1.8 networkit-7.1 scikit-network-0.23.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XVPLOimOFx_"
      },
      "source": [
        "from littleballoffur import GraphReader\n",
        "\n",
        "reader = GraphReader(\"github\")\n",
        "\n",
        "graph = reader.get_graph()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUOIx6hQNwmf"
      },
      "source": [
        "#### Node Sampling\n",
        "\n",
        "We will first look at some popular node sampling algorithms. Let’s use the PageRank Proportional Node Sampling method from [Sampling From Large Graphs](https://cs.stanford.edu/people/jure/pubs/sampling-kdd06.pdf). We will sample approximately 50% of the original nodes from the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soAQ6ZoPOskH"
      },
      "source": [
        "from littleballoffur import PageRankBasedSampler, RandomNodeSampler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USz8v9vPP2S8"
      },
      "source": [
        "number_of_nodes = int(0.5*graph.number_of_nodes())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dna2SCRsP0wQ"
      },
      "source": [
        "pagerank_sampler = PageRankBasedSampler(number_of_nodes = number_of_nodes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ6lvPDuQD9Z"
      },
      "source": [
        "randomnode_sampler = RandomNodeSampler(number_of_nodes = number_of_nodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70v_mpumQN1D"
      },
      "source": [
        "randomnodes_graph = pagerank_sampler.sample(graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvvPdrWmP3Ti"
      },
      "source": [
        "pagerank_graph = pagerank_sampler.sample(graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_fG651iTzBm"
      },
      "source": [
        "#### Sub-graph Sampling\n",
        "\n",
        "We will look at a series of exploration sampling algorithms that sample sub-graphs from larger graphs.\n",
        "\n",
        "First is an implementation of node sampling by random walks. A simple random walker which creates an induced subgraph by walking around. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS-58urUT1P_"
      },
      "source": [
        "from littleballoffur import RandomWalkSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnEKzdnOUbjk"
      },
      "source": [
        "randomwalk_sampler = RandomWalkSampler(number_of_nodes=number_of_nodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsaOhfIndz_H"
      },
      "source": [
        "randomwalk_graph = randomnode_sampler.sample(graph)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7kTlXrbcvBJ"
      },
      "source": [
        "Now let’s use the Metropolis-Hastings Random Walk Sampler method from [Metropolis Algorithms for Representative Subgraph Sampling](https://ieeexplore.ieee.org/document/4781123).  The random walker has a probabilistic acceptance condition for adding new nodes to the sampled node set. This constraint can be parametrized by the rejection constraint exponent. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-00F5tBVYtyy"
      },
      "source": [
        "from littleballoffur import MetropolisHastingsRandomWalkSampler"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9WTUPhvdrq3"
      },
      "source": [
        "mhrw_sampler = MetropolisHastingsRandomWalkSampler(number_of_nodes = number_of_nodes)\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRCLLE6pdzBq"
      },
      "source": [
        "mhrw_graph = mhrw_sampler.sample(graph)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHHficx6d9gR"
      },
      "source": [
        "You will find many other sub-graph sampling algorithms, and many variations of the Random Walk algorithm (RandomWalkWithJumpSampler, RandomNodeNeighborSampler, RandomWalkWithRestartSampler) in the [Exploration Sampling](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmyrbLrucmSy"
      },
      "source": [
        "\n",
        "## Non-probabilistic Sampling and Extensions\n",
        "\n",
        "A lot of the sampling we will see in this section involves sampling over a network or graph. For example, in a survey, we may ask one respondent to choose the next, and in that way create a sub-graph within the full social graph we are sampling from. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihMhbstQmoJV"
      },
      "source": [
        "### Snowball Sampling \n",
        "\n",
        "Snowball sampling is where research participants recruit other participants for a test or study. It is used where potential participants are hard to find. It’s called snowball sampling because (in theory) once you have the ball rolling, it picks up more “snow” along the way and becomes larger and larger. Snowball sampling is a non-probability sampling method. ([link to explanation](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/snowball-sampling/))\n",
        "\n",
        "If we manually inspect the sampling, we can add our own criteria for stopping; in the package implementation, it stops when we have enough samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvkz6fylmohQ"
      },
      "source": [
        "from littleballoffur import SnowBallSampler"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XQtFjzhgFlQ"
      },
      "source": [
        "snowball_sampler = SnowBallSampler(number_of_nodes = number_of_nodes)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RGAV9__gJss"
      },
      "source": [
        "snowball_graph = snowball_sampler.sample(graph)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUs5SWZAgQO-"
      },
      "source": [
        "littleballoffur also offers stochastic extensions of the Snowball Sampler. The Forest Fire Sampler is a stochastic snowball sampling method where the expansion is proportional to the burning probability, described [here](https://cs.stanford.edu/people/jure/pubs/sampling-kdd06.pdf). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8gpVJNfh75H"
      },
      "source": [
        "from littleballoffur import ForestFireSampler, SpikyBallSampler"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve6JSUt-g7kr"
      },
      "source": [
        "forestfire_sampler = ForestFireSampler(number_of_nodes=number_of_nodes)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05TOh5q1hpPn"
      },
      "source": [
        "forestfire_graph = forestfire_sampler.sample(graph)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjeB-nDKg8Mj"
      },
      "source": [
        "Below we use spiky ball sampling. The procedure is a filtered breadth-first search sampling method where the expansion is is performed over a random subset of neighbors. Originally described [here](https://www.mdpi.com/1999-4893/13/11/275)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QDoFFxxhr2c"
      },
      "source": [
        "spikyball_sampler = SpikyBallSampler(number_of_nodes=number_of_nodes)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVKsBsYFhiW3"
      },
      "source": [
        "spikyball_graph = spikyball_sampler.sample(graph)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbN75A6DiAka"
      },
      "source": [
        "That's the last of our graph based sampling. We will now discuss some other methods used. Note that the following sections are theoretical with no code, but the previous code can help you implement these methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxKMTZ6Lmo8f"
      },
      "source": [
        "### Convenience\n",
        "\n",
        "This simply refers to using a sample that is at-hand and easy to analyse. For example, you may choose a free sub-graph of the full Facebook friend graph to run experiments because you do not have access to more data. You can refine your methods on this data before moving on to a larger dataset.\n",
        "\n",
        "You can read more about it [here](https://methods.sagepub.com/reference/encyclopedia-of-survey-research-methods/n105.xml)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoihUKvCkdo_"
      },
      "source": [
        "# empty cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdLqu74LmpuI"
      },
      "source": [
        "### Quota Sampling\n",
        "\n",
        "Very similar to stratified sampling, but we may not randonly sample from the strata and choose the whole sample. Here are some links to read more:\n",
        "\n",
        "- [QuestionPro: Quota Sampling definition](https://www.questionpro.com/blog/quota-sampling/#:~:text=Quota%20sampling%20is%20defined%20as,to%20specific%20traits%20or%20qualities.)\n",
        "- [Statistics How To: Quota Sampling](https://www.statisticshowto.com/quota-sampling/)\n",
        "- [humansofdata: Quota Sampling](https://humansofdata.atlan.com/2016/04/quota-sampling-when-to-use-how-to-do-correctly/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NtZnnbwmqJ5"
      },
      "source": [
        "# empty cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzKk-htomqp1"
      },
      "source": [
        "### Purposive / Relevance / Judegement sampling\n",
        "\n",
        "[text below from this link](https://www.alchemer.com/resources/blog/purposive-sampling-101/)\n",
        "\n",
        "[paper: Purposeful sampling for qualitative data collection and analysis in mixed method implementation research](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4012002/)\n",
        "\n",
        "Purposive sampling, also known as judgmental, selective, or subjective sampling, is a form of non-probability sampling in which researchers rely on their own judgment when choosing members of the population to participate in their surveys. \n",
        "\n",
        "This survey sampling method requires researchers to have prior knowledge about the purpose of their studies so that they can properly choose and approach eligible participants for surveys conducted.\n",
        "\n",
        "Researchers use purposive sampling when they want to access a particular subset of people, as all participants of a survey are selected because they fit a particular profile. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oONM120mq9A"
      },
      "source": [
        "# empty cell"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvz1OBOkcvk8"
      },
      "source": [
        "## Sampling Imbalanced Classes\n",
        "\n",
        "A common problem we deal with in real world datasets is imbalance, when we have one (or a few) class, category or sections that have higher or lower representation than the other classes. This situation is often referred to as having imbalancing classes, and the python package [imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn) is specifically built to tackle this.\n",
        "\n",
        "In this section we will be following a tutorial by [Investigate.ai](https://investigate.ai/), a school for teaching data science to journalists. The tutorial we will follow is in this [repository](https://github.com/littlecolumns/ds4j-notebooks/blob/master/classification/notebooks/Correcting%20for%20imbalanced%20datasets.ipynb).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-iit6vmcwSY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtzCFtxYlXgB"
      },
      "source": [
        "### Classification problems with imbalanced inputs\n",
        "\n",
        "Oftentimes when we're doing real-world classification problems, we have the problem of **\"imbalanced classes\"**.\n",
        "\n",
        "Let's say we're analyzing a document dump, and trying to find the documents that are interesting to us. Maybe we're only interested in 10% of them! The fact that there's such a bias - 90% of them are uninteresting - **will mess with our classifier.** Let's take a look at [imbalanced-learn](https://imbalanced-learn.readthedocs.io/en/stable/) library to help fix this problem!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTjM7VWilXgK"
      },
      "source": [
        "### Prep work: Downloading necessary files\n",
        "Before we get started, we need to download all of the data we'll be using.\n",
        "* **recipes-indian.csv:** Indian classification recipes - a selection of recipe ingredient lists, with half of them being labeled as Indian cuisine\n",
        "* **recipes.csv:** recipes - a selection of recipe ingredient lists, with each labeled with the cuisine its from\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SjZT-AOlXgK"
      },
      "source": [
        "# Make data directory if it doesn't exist\n",
        "!mkdir -p data\n",
        "!wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/classification/data/recipes-indian.csv -P data\n",
        "!wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/classification/data/recipes.csv -P data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PsIDs05lXgL"
      },
      "source": [
        "We're going to go through this pretty quickly, so you should be familiar with vectorizing, classification, and confusion matrices going in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWkm-k7alXgM"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNUVdiZ-lXgN"
      },
      "source": [
        "### Our datasets\n",
        "\n",
        "We're going to be looking at two datasets today. They're both **recipes and ingredient lists**, and with both we're predicting whether we can **accurate determine which recipes are Indian**.\n",
        "\n",
        "Let's read them both in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiTKNr-JlXgO",
        "outputId": "8aee1322-8e62-4b02-e977-91f569d4b63b"
      },
      "source": [
        "df_balanced = pd.read_csv(\"data/recipes-indian.csv\")\n",
        "df_balanced['is_indian'] = (df_balanced.cuisine == \"indian\").astype(int)\n",
        "\n",
        "df_balanced.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cuisine</th>\n",
              "      <th>id</th>\n",
              "      <th>ingredient_list</th>\n",
              "      <th>is_indian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>indian</td>\n",
              "      <td>23348</td>\n",
              "      <td>minced ginger, garlic, oil, coriander powder, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>indian</td>\n",
              "      <td>18869</td>\n",
              "      <td>chicken, chicken breasts</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>indian</td>\n",
              "      <td>36405</td>\n",
              "      <td>flour, rose essence, frying oil, powdered milk...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>indian</td>\n",
              "      <td>11494</td>\n",
              "      <td>soda, ghee, sugar, khoa, maida flour, milk, oil</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>indian</td>\n",
              "      <td>32675</td>\n",
              "      <td>tumeric, garam masala, salt, chicken, curry le...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  cuisine     id                                    ingredient_list  is_indian\n",
              "0  indian  23348  minced ginger, garlic, oil, coriander powder, ...          1\n",
              "1  indian  18869                           chicken, chicken breasts          1\n",
              "2  indian  36405  flour, rose essence, frying oil, powdered milk...          1\n",
              "3  indian  11494    soda, ghee, sugar, khoa, maida flour, milk, oil          1\n",
              "4  indian  32675  tumeric, garam masala, salt, chicken, curry le...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpfIFvb0lXgP",
        "outputId": "30f5aa93-a278-4e54-cb8c-96e322c16086"
      },
      "source": [
        "df_unbalanced = pd.read_csv(\"data/recipes.csv\")\n",
        "df_unbalanced['is_indian'] = (df_unbalanced.cuisine == \"indian\").astype(int)\n",
        "\n",
        "df_unbalanced.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cuisine</th>\n",
              "      <th>id</th>\n",
              "      <th>ingredient_list</th>\n",
              "      <th>is_indian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>greek</td>\n",
              "      <td>10259</td>\n",
              "      <td>romaine lettuce, black olives, grape tomatoes,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>southern_us</td>\n",
              "      <td>25693</td>\n",
              "      <td>plain flour, ground pepper, salt, tomatoes, gr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>filipino</td>\n",
              "      <td>20130</td>\n",
              "      <td>eggs, pepper, salt, mayonaise, cooking oil, gr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>indian</td>\n",
              "      <td>22213</td>\n",
              "      <td>water, vegetable oil, wheat, salt</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>indian</td>\n",
              "      <td>13162</td>\n",
              "      <td>black pepper, shallots, cornflour, cayenne pep...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       cuisine     id                                    ingredient_list  \\\n",
              "0        greek  10259  romaine lettuce, black olives, grape tomatoes,...   \n",
              "1  southern_us  25693  plain flour, ground pepper, salt, tomatoes, gr...   \n",
              "2     filipino  20130  eggs, pepper, salt, mayonaise, cooking oil, gr...   \n",
              "3       indian  22213                  water, vegetable oil, wheat, salt   \n",
              "4       indian  13162  black pepper, shallots, cornflour, cayenne pep...   \n",
              "\n",
              "   is_indian  \n",
              "0          0  \n",
              "1          0  \n",
              "2          0  \n",
              "3          1  \n",
              "4          1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nwLuwb2lXgQ"
      },
      "source": [
        "They both look similar enough, right? A list of ingredients and an `is_indian` target column we'll be using as our label.\n",
        "\n",
        "### Finding the imbalance\n",
        "\n",
        "The real difference is how many of the recipes are Indian in each dataset. Let's take a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUraeo17lXgQ",
        "outputId": "37017690-37e3-42b3-e471-04aaf8e0b844"
      },
      "source": [
        "df_balanced.is_indian.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    3000\n",
              "0    3000\n",
              "Name: is_indian, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeV5TLxdlXgR",
        "outputId": "d27c896d-d8d8-4cc2-8142-986ad352eb8e"
      },
      "source": [
        "df_unbalanced.is_indian.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    36771\n",
              "1     3003\n",
              "Name: is_indian, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjPx43gclXgT"
      },
      "source": [
        "Ouch! That second dataset is really uneven - over ten times as many non-Indian recipes as there are Indian recipes!\n",
        "\n",
        "The thing is: **this is usually how data looks in the real world.** You rarely have even numbers between your classes, and you often thing \"more data is better data.\" We'll see how it plays out when we actually run our classifiers!\n",
        "\n",
        "### Testing our datasets\n",
        "\n",
        "We're going to use a `TfidfVectorizer` to convert ingredient lists to numbers, run a test/train split, and then train (and test) a `LinearSVC` classifier on the results. We'll start with the **balanced dataset**.\n",
        "\n",
        "### Balanced dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCOlcbUVlXgU",
        "outputId": "ba3e7ef0-b78f-413d-908d-4153c7582faf"
      },
      "source": [
        "# Create a vectorizer and train it\n",
        "vectorizer = TfidfVectorizer()\n",
        "matrix = vectorizer.fit_transform(df_balanced.ingredient_list)\n",
        "\n",
        "# Features are our matrix of tf-idf values\n",
        "# labels are whether each recipe is Indian or not\n",
        "X = matrix\n",
        "y = df_balanced.is_indian\n",
        "\n",
        "# How many are Indian?\n",
        "y.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    3000\n",
              "0    3000\n",
              "Name: is_indian, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPrrl2TslXgU"
      },
      "source": [
        "We still have an even split, 3000 non-Indian recipes and 3000 Indian recipes. Let's run a test/train split and see how the results look."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92ZgFvURlXgU",
        "outputId": "19790e1e-24bc-4281-d621-b1cdfac39dc3"
      },
      "source": [
        "# Split into test and train data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "# Build a classifier and train it\n",
        "clf = LinearSVC()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Test our classifier and build a confusion matrix\n",
        "y_true = y_test\n",
        "y_pred = clf.predict(X_test)\n",
        "matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "label_names = pd.Series(['not indian', 'indian'])\n",
        "pd.DataFrame(matrix,\n",
        "     columns='Predicted ' + label_names,\n",
        "     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted not indian</th>\n",
              "      <th>Predicted indian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Is not indian</th>\n",
              "      <td>0.962815</td>\n",
              "      <td>0.037185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is indian</th>\n",
              "      <td>0.048193</td>\n",
              "      <td>0.951807</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Predicted not indian  Predicted indian\n",
              "Is not indian              0.962815          0.037185\n",
              "Is indian                  0.048193          0.951807"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMWQIZSBlXgV"
      },
      "source": [
        "**Our classifier looks pretty good!** Around 96% accuracy for predicting non-Indian food, and around 95% correctly predicting Indian food. High quality *and* even.\n",
        "\n",
        "Let's move on to see how it looks with our **unabalanced dataset**.\n",
        "\n",
        "### Unbalanced dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqMK8mAElXgb",
        "outputId": "85c2b30a-c47a-4232-c294-4c56ea54bf40"
      },
      "source": [
        "# Create a vectorizer and train it\n",
        "vectorizer = TfidfVectorizer()\n",
        "matrix = vectorizer.fit_transform(df_unbalanced.ingredient_list)\n",
        "\n",
        "# Features are our matrix of tf-idf values\n",
        "# labels are whether each recipe is Indian or not\n",
        "X = matrix\n",
        "y = df_unbalanced.is_indian\n",
        "\n",
        "# How many are Indian?\n",
        "y.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    36771\n",
              "1     3003\n",
              "Name: is_indian, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UETmtKDQlXgc"
      },
      "source": [
        "Again: around 36k non-Indian recipes really really outweighing the 3,003 Indian recipes. While we love the world of \"more more more data,\" let's see what that imbalance does to our classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSKtfoZflXgc",
        "outputId": "4225ba60-97cc-4533-d43b-b66f7b86eb30"
      },
      "source": [
        "# Split our dataset is train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf = LinearSVC()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Test our classifier and build a confusion matrix\n",
        "y_true = y_test\n",
        "y_pred = clf.predict(X_test)\n",
        "matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "label_names = pd.Series(['not indian', 'indian'])\n",
        "pd.DataFrame(matrix,\n",
        "     columns='Predicted ' + label_names,\n",
        "     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted not indian</th>\n",
              "      <th>Predicted indian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Is not indian</th>\n",
              "      <td>0.992150</td>\n",
              "      <td>0.007850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is indian</th>\n",
              "      <td>0.180052</td>\n",
              "      <td>0.819948</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Predicted not indian  Predicted indian\n",
              "Is not indian              0.992150          0.007850\n",
              "Is indian                  0.180052          0.819948"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1i26ZjOlXge"
      },
      "source": [
        "Ouch!!! While we're doing **really well** at predicting non-Indian dishes, our ability to predict Indian dishes has plummeted to just over 80%.\n",
        "\n",
        "Why does this happen? An easy way to think about it is **when it's a risky decision, it's always safest to guess \"not Indian.\"** In fact, if we *always guessed non-Indian*, no matter what, we'd be right..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtkhjNhmlXge",
        "outputId": "68639461-fa65-4c66-b089-bd54dc82b7cd"
      },
      "source": [
        "36771/(36771+3003)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9244984160506864"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW_n1fGjlXgf"
      },
      "source": [
        "About 92% of the time! So how do we solve this problem?\n",
        "\n",
        "### Solving the problem\n",
        "\n",
        "Solving the problem of unbalanced (or biased) input classes is actually not too hard! There's a nice library that can give us a hand, [imbalanced-learn](https://imbalanced-learn.readthedocs.io/en/stable/).\n",
        "\n",
        "imbalanced-learn will **resample** our dataset, either generating new datapoints or pruning out existing datapoints, until the classes are evened out.\n",
        "\n",
        "#### What do we resample?\n",
        "\n",
        "An important thing to note is that **the problem with bias happens when we train our model.** If we show our model a skewed view of the world, it'll carry that bias when making judgments in the future. When we add or remove datapoints to even out the problem, **we only need to do this for the training data.**\n",
        "\n",
        "We want to show the model an even view of the world, so we give it even data. The test data should still reflect the \"real\" world. Before we were looking at how imblanaced our overall dataset was, but now let's **just look at how biased the training data is.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2f_pnUOlXgf",
        "outputId": "34943474-ed1c-49ce-c04a-03b76ceb4f50"
      },
      "source": [
        "y_train.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    27599\n",
              "1     2231\n",
              "Name: is_indian, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uW4j_m2lXgf",
        "outputId": "56ef0175-0031-4137-dad2-2da3ef5a2cdf"
      },
      "source": [
        "y_train.value_counts(normalize=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.92521\n",
              "1    0.07479\n",
              "Name: is_indian, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qSepHs9lXgg"
      },
      "source": [
        "Looks like a little over 7% of our training data is Indian - we'd like to get that up to 50%, so let's see what the imbalanced-learn library can do for us!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYBZm6uUlXgg"
      },
      "source": [
        "#### Undersampling\n",
        "\n",
        "If we're feeling guilty that there are so many additional non-Indian recipes, *we could always get rid of those extra non-Indian recipes!* In fact, the balanced dataset was me manually creating a new CSV from an even split of Indian/non-Indian recipes..\n",
        "\n",
        "Instead of manually digging through our dataset to even things out, though, we can rely on imbalanced-learn to do it automatically. We'll use the technique of **undersampling** to take those ~28k non-Indian recipes and randomly filter them down to around 2,000 to match the number of Indian recipes. (Remember we're only doing this with training data!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXqPIdAglXgg",
        "outputId": "dfdd0efa-80d4-4c36-8ed6-f04cc21621f8"
      },
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "resampler = RandomUnderSampler()\n",
        "# Resample X and y so there are equal numbers of each y\n",
        "X_train_resampled, y_train_resampled = resampler.fit_resample(X_train, y_train)\n",
        "\n",
        "y_train_resampled.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    2231\n",
              "0    2231\n",
              "Name: is_indian, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye7k1OEelXgg"
      },
      "source": [
        "Okay, cool, equal numbers! Let's see how the classifier performs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvuBj3t5lXgh",
        "outputId": "8eab6511-fdfe-4cbf-e488-d4f598628c27"
      },
      "source": [
        "# We already split our data, so we don't need to do that again\n",
        "\n",
        "# Train the classifier on the resampled training data\n",
        "clf = LinearSVC()\n",
        "clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Build a confusion matrix\n",
        "y_true = y_test\n",
        "y_pred = clf.predict(X_test)\n",
        "matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "label_names = pd.Series(['not indian', 'indian'])\n",
        "pd.DataFrame(matrix,\n",
        "     columns='Predicted ' + label_names,\n",
        "     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted not indian</th>\n",
              "      <th>Predicted indian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Is not indian</th>\n",
              "      <td>0.957479</td>\n",
              "      <td>0.042521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is indian</th>\n",
              "      <td>0.051813</td>\n",
              "      <td>0.948187</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Predicted not indian  Predicted indian\n",
              "Is not indian              0.957479          0.042521\n",
              "Is indian                  0.051813          0.948187"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8rGaSZFlXgh"
      },
      "source": [
        "Looking good! It performs as well as our other 3,000/3,000 split because, well, it's more or less the same thing (although the test data is \"realistically\" unbalanced)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsffOpWglXgi"
      },
      "source": [
        "#### Oversampling\n",
        "\n",
        "Cutting out those 27,000 \"extra\" non-Indian recipes seems like such a bummer, though. Wouldn't it be nice if we somehow found another 25,000 Indian recipes to even up our unbalanced training dataset to 27k non-Indian and 27k Indian? It's possible with **oversampling!**\n",
        "\n",
        "Oversampling generates **new datapoints** based on your existing dataset. In this case we're going to use the `RandomOverSampler`, which just fills our dataset with **copies of the less-included class**. We'll have 27k Indian recipes, *but they'll be 25,0000 copies of the original ones*. Can that possibly help?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6deyxiYMlXgi"
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "resampler = RandomOverSampler()\n",
        "X_train_resampled, y_train_resampled = resampler.fit_resample(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkUkQE86lXgi",
        "outputId": "9bf0d7ea-42f1-44c4-c8d8-293b39871e96"
      },
      "source": [
        "y_train_resampled.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    27599\n",
              "0    27599\n",
              "Name: is_indian, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCisLEpblXgi"
      },
      "source": [
        "Looking good, a nice even 27,599 apiece. Let's see how the classifier works out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFD0A2CDlXgj",
        "outputId": "b2376e32-ce8f-47c9-e7d7-17adbca4ce22"
      },
      "source": [
        "# We already split our dataset into train and test data\n",
        "\n",
        "# Train the classifier on the resampled training data\n",
        "clf = LinearSVC()\n",
        "clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Build a confusion matrix with the result\n",
        "y_true = y_test\n",
        "y_pred = clf.predict(X_test)\n",
        "matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "label_names = pd.Series(['not indian', 'indian'])\n",
        "pd.DataFrame(matrix,\n",
        "     columns='Predicted ' + label_names,\n",
        "     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted not indian</th>\n",
              "      <th>Predicted indian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Is not indian</th>\n",
              "      <td>0.969363</td>\n",
              "      <td>0.030637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is indian</th>\n",
              "      <td>0.068653</td>\n",
              "      <td>0.931347</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Predicted not indian  Predicted indian\n",
              "Is not indian              0.969363          0.030637\n",
              "Is indian                  0.068653          0.931347"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-hUag9ClXgk"
      },
      "source": [
        "Also looking pretty good! A little bit better at predicting non-Indian dishes and a little bit worse at predicting Indian dishes, but it more or less evens out with the undersampled example. \n",
        "\n",
        "There are also other oversampling techniques that involve **creating synthetic data,** new datapoints that aren't *copies* of our data, but rather totally new ones. You can read more about them [on the imbalanced-learn page](https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html).\n",
        "\n",
        "#### Review\n",
        "\n",
        "In this section we talked about the problem of **imbalanced classes**, where an uneven split in your labels can cause suboptimal classifier performance. We used the imbalanced-learn library to talk about two methods of solving the issue - undersampling and oversampling - which both boosted performance as compared to the imbalanced dataset.\n",
        "\n",
        "#### Discussion topics\n",
        "\n",
        "What is the difference between oversampling and undersampling? Why might have oversampling done a better job predicting non-Indian recipes?\n",
        "\n",
        "Why did we only resample the training data, and not the test data?\n",
        "\n",
        "While the idea of automatically-generated fake data might sound more attractive than just re-using existing data, [what might be some issues with it](https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html)?\n",
        "\n",
        "Can we think of any times when we might *not* want a balanced dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WF7Ztt1sLxf"
      },
      "source": [
        "You can find more examples with imbalanced datasets in the [example gallery](https://imbalanced-learn.org/stable/auto_examples/index.html#general-examples)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9eQ6oTYu72W"
      },
      "source": [
        "## Bootstrap sampling and sub-sampling\n",
        "\n",
        "Bootstrap sampling and sub-sampling are two methods that are particularly relevant in the context of machine learning and deep learning. \n",
        "\n",
        "[original text from this blog](https://machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/).\n",
        "\n",
        "The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset **with replacement**.\n",
        "\n",
        "It can be used to estimate summary statistics such as the mean or standard deviation. It is used in applied machine learning to estimate the skill of machine learning models when making predictions on data not included in the training data.\n",
        "\n",
        "A desirable property of the results from estimating machine learning model skill is that the estimated skill can be presented with confidence intervals, a feature not readily available with other methods such as cross-validation.\n",
        "\n",
        "### Bootstrap sampling example with word embeddings\n",
        "\n",
        "In this section we will explore the stability of word embeddings using bootstrap sampling. This is a well researched question in NLP research ([Evaluating the Stability of Embedding-based Word Similarities](https://mimno.infosci.cornell.edu/papers/antoniak-stability.pdf)). In the paper, *the authors come to the conclusion that there are several sources of variability\n",
        "in cosine similarities between word embeddings vectors. The size of the corpus, the length of individual documents, and the presence or absence of specific\n",
        "documents can all affect the resulting embeddings. While differences in word association are measurable and are often significant, small differences in cosine similarity are not reliable, especially for small corpora. If the intention of a study is to learn about a specific corpus, they recommend that practitioners test the statistical confidence of similarities based on\n",
        "word embeddings by training on multiple bootstrap samples.*\n",
        "\n",
        "We will conduct a mini-version of this experiment to test how stable word similarities are after training with and without bootrstrap sampling on a smaller text dataset. We will use a dataset we used before, the hobbies dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlyIe6dIx7dg"
      },
      "source": [
        "import gensim"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIwNQY1e7hBy",
        "outputId": "ffd4f30d-3613-49f1-bae2-55bffe0d3c2b"
      },
      "source": [
        "from yellowbrick.datasets import load_hobbies"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9sTDbRs7iu0"
      },
      "source": [
        "from gensim.parsing.preprocessing import preprocess_documents"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5K8rZFd7suG"
      },
      "source": [
        "corpus = load_hobbies()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj3Qa1G77pwV"
      },
      "source": [
        "preprocessed_texts = preprocess_documents(corpus.data)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cD7R4Hu8Y0s",
        "outputId": "30ccfc25-92ff-41a9-be13-e420cbc84e3d"
      },
      "source": [
        "len(preprocessed_texts)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "448"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPTNVcPM72AK"
      },
      "source": [
        "from gensim.models import Word2Vec\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-9qqwOB78nd"
      },
      "source": [
        "w2vmodel_cleaned = Word2Vec(\n",
        "        preprocessed_texts,\n",
        "        size=100,\n",
        "        window=10)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNzddav_8AxG",
        "outputId": "8a61d670-f79e-47a2-f411-6c18ec39ad17"
      },
      "source": [
        "w2vmodel_cleaned.wv.most_similar(\"book\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('read', 0.9998674392700195),\n",
              " ('stori', 0.9998670816421509),\n",
              " ('publish', 0.9998620748519897),\n",
              " ('author', 0.9998588562011719),\n",
              " ('movi', 0.9998471140861511),\n",
              " ('edit', 0.9998458027839661),\n",
              " ('futur', 0.9998434782028198),\n",
              " ('turn', 0.9998424649238586),\n",
              " ('bui', 0.9998399019241333),\n",
              " ('page', 0.9998395442962646)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOAnPibn8wJ8"
      },
      "source": [
        "We will use this as a reference while we try out how stable the model is with two sets of bootstrapped texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPD5-Qf78DW6"
      },
      "source": [
        "from sklearn.utils import resample"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omdSsM4F8ORi"
      },
      "source": [
        "bootstrap_texts = resample(preprocessed_texts)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bhZF74A8UuY",
        "outputId": "9d1b2b72-25f6-4b90-9b03-94c596b6e52b"
      },
      "source": [
        "len(bootstrap_texts)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "448"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmXdgRCn8cCw"
      },
      "source": [
        "w2vmodel_boot = Word2Vec(\n",
        "        bootstrap_texts,\n",
        "        size=100,\n",
        "        window=10)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHH3wI2M8gro",
        "outputId": "820bf815-1edb-48b8-9411-92c35108142e"
      },
      "source": [
        "w2vmodel_boot.wv.most_similar(\"book\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('author', 0.9998607635498047),\n",
              " ('publish', 0.9998499155044556),\n",
              " ('broken', 0.9997895956039429),\n",
              " ('compani', 0.9997869729995728),\n",
              " ('featur', 0.9997847080230713),\n",
              " ('contain', 0.9997791051864624),\n",
              " ('stori', 0.9997789859771729),\n",
              " ('page', 0.9997786283493042),\n",
              " ('futur', 0.9997777938842773),\n",
              " ('novel', 0.9997714757919312)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K5LeO4J8noC"
      },
      "source": [
        "bootstrap_texts_1 = resample(preprocessed_texts)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIAtptJz8kty"
      },
      "source": [
        "w2vmodel_boot_1 = Word2Vec(\n",
        "        bootstrap_texts_1,\n",
        "        size=100,\n",
        "        window=10)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2fVOV2s8rn8",
        "outputId": "c353c293-0e74-4ae4-e6a0-a43e7e3f6f30"
      },
      "source": [
        "w2vmodel_boot_1.wv.most_similar(\"book\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('stori', 0.9997271299362183),\n",
              " ('femal', 0.9997173547744751),\n",
              " ('write', 0.9996861815452576),\n",
              " ('read', 0.9996781349182129),\n",
              " ('actor', 0.9996703863143921),\n",
              " ('compani', 0.9996675252914429),\n",
              " ('batman', 0.9996577501296997),\n",
              " ('present', 0.9996563196182251),\n",
              " ('product', 0.9996486902236938),\n",
              " ('futur', 0.9996473789215088)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQDDDuUB83t3"
      },
      "source": [
        "Smaller corpora are likely to be more variable in their embeddings, and when bootstrapped we see this more clearly. We encourage you to read the paper on stability in word embeddings and to run your own experiments with your datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iznvG1f1eiX"
      },
      "source": [
        "### Sub-sampling\n",
        "\n",
        "Unlike bootstrap sampling, in sub-sampling we sample without replacement, usually in cases when we are dealing with very large data and only want a portion of it. [subsample](https://pypi.org/project/subsample/) is a python package that works as a command-line tool for sub-sampling, and is especially powerful with text based datasets. It includes methods such as reservoir sampling and two pass sampling. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo53ShLuV18u"
      },
      "source": [
        "# Annotation & Reliability\n",
        "\n",
        "Up until this week, we have assumed that the corpus you have used for analysis assignments represented a *meaningful* assemblage of texts from which reasonable inferences could be drawn about the social game, social world and social actors that produced it. \n",
        "\n",
        "This week, we ask you to build a corpus for preliminary analysis and articulate what your sample represents in context of your final project. We begin by exploring how we can get *human* readings of content at scale. We want to gather and utilize human responses for several reasons. First, we may want to use crowdsourced human scores as the primary method of coding, extracting or organizing content (as it was in two of the assigned readings). Second, we may want to validate or tune a computational algorithm we may have developed in terms of how it is associated with human meanings or experience. Finally, we may want to use human coding on a sample of data as the basis for training a model or algorithm to then extrapolate *human-like* annotations to the entire population. Here intelligent sampling is critical to maximize effective maching training. \n",
        "\n",
        "For this notebook we will be using the following packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-h9duIEYwuS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d888157-e090-4ef6-9bb1-f35299d82ee9"
      },
      "source": [
        "! pip install -U git+git://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git\n",
            "  Cloning git://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git to /tmp/pip-req-build-sydlad_r\n",
            "  Running command git clone -q git://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git /tmp/pip-req-build-sydlad_r\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from lucem-illud==8.0.1) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from lucem-illud==8.0.1) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from lucem-illud==8.0.1) (1.1.5)\n",
            "Collecting python-docx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/a0/52729ce4aa026f31b74cc877be1d11e4ddeaa361dc7aebec148171644b33/python-docx-0.8.11.tar.gz (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pillow in /usr/local/lib/python3.7/dist-packages (from lucem-illud==8.0.1) (7.1.2)\n",
            "Collecting pdfminer2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/97/bd2a2de878438c27ffd710b5d6562c7a0230b0f3ca86059ec635ed231eb1/pdfminer2-20151206-py2.py3-none-any.whl (117kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 49.2MB/s \n",
            "\u001b[?25hCollecting GitPython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/da/6f6224fdfc47dab57881fe20c0d1bc3122be290198ba0bf26a953a045d92/GitPython-3.1.17-py3-none-any.whl (166kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 52.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wordcloud in /usr/local/lib/python3.7/dist-packages (from lucem-illud==8.0.1) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from lucem-illud==8.0.1) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: seaborn in /usr/local/lib/python3.7/dist-packages (from lucem-illud==8.0.1) (0.11.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lucem-illud==8.0.1) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from lucem-illud==8.0.1) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: gensim in /usr/local/lib/python3.7/dist-packages (from lucem-illud==8.0.1) (3.6.0)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.7/dist-packages (from lucem-illud==8.0.1) (3.2.2)\n",
            "Collecting pyanno3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/1a/ee2b136ea0283adb2a9302c29594127f84b6e34cb0b02b91c63bed0a534b/pyanno3-2.0.2.tar.gz (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 13.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from lucem-illud==8.0.1) (4.6.3)\n",
            "Requirement already satisfied, skipping upgrade: graphviz in /usr/local/lib/python3.7/dist-packages (from lucem-illud==8.0.1) (0.10.1)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/fe/6156cb9ad11c01728adaaf6eebd855e66ac17d12a3826cb6fa6d3c2b3dab/boto3-1.17.78-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: networkx in /usr/local/lib/python3.7/dist-packages (from lucem-illud==8.0.1) (2.5.1)\n",
            "Collecting pydub\n",
            "  Downloading https://files.pythonhosted.org/packages/a6/53/d78dc063216e62fc55f6b2eebb447f6a4b0a59f55c8406376f76bf959b08/pydub-0.25.1-py2.py3-none-any.whl\n",
            "Collecting speechrecognition\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/e1/7f5678cd94ec1234269d23756dbdaa4c8cfaed973412f88ae8adf7893a50/SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8MB 202kB/s \n",
            "\u001b[?25hCollecting pysoundfile\n",
            "  Downloading https://files.pythonhosted.org/packages/2a/b3/0b871e5fd31b9a8e54b4ee359384e705a1ca1e2870706d2f081dc7cc1693/PySoundFile-0.9.0.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.7/dist-packages (from lucem-illud==8.0.1) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: IPython in /usr/local/lib/python3.7/dist-packages (from lucem-illud==8.0.1) (5.5.0)\n",
            "Requirement already satisfied, skipping upgrade: spacy in /usr/local/lib/python3.7/dist-packages (from lucem-illud==8.0.1) (2.2.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->lucem-illud==8.0.1) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->lucem-illud==8.0.1) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->lucem-illud==8.0.1) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->lucem-illud==8.0.1) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->lucem-illud==8.0.1) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->lucem-illud==8.0.1) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx->lucem-illud==8.0.1) (4.2.6)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from pdfminer2->lucem-illud==8.0.1) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 13.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from GitPython->lucem-illud==8.0.1) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lucem-illud==8.0.1) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->lucem-illud==8.0.1) (5.0.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lucem-illud==8.0.1) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lucem-illud==8.0.1) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lucem-illud==8.0.1) (2.4.7)\n",
            "Collecting traits\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/40/225cc2bba4c0466fdb5d9d67aac164bbe6320428eb11a71084be4338aceb/traits-6.2.0-cp37-cp37m-manylinux2010_x86_64.whl (5.1MB)\n",
            "\u001b[K     |████████████████████████████████| 5.1MB 19.9MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.3MB/s \n",
            "\u001b[?25hCollecting botocore<1.21.0,>=1.20.78\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/9e/b5c2ba127c653d22b5b9a48b173fb3e12f3bafc4f6c52004fa737157e63b/botocore-1.20.78-py2.py3-none-any.whl (7.5MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 35.0MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->lucem-illud==8.0.1) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: cffi>=0.6 in /usr/local/lib/python3.7/dist-packages (from pysoundfile->lucem-illud==8.0.1) (1.14.5)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->lucem-illud==8.0.1) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->lucem-illud==8.0.1) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->lucem-illud==8.0.1) (1.0.18)\n",
            "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->lucem-illud==8.0.1) (0.7.5)\n",
            "Requirement already satisfied, skipping upgrade: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->lucem-illud==8.0.1) (5.0.5)\n",
            "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from IPython->lucem-illud==8.0.1) (4.8.0)\n",
            "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->lucem-illud==8.0.1) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->lucem-illud==8.0.1) (56.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->lucem-illud==8.0.1) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->lucem-illud==8.0.1) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->lucem-illud==8.0.1) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->lucem-illud==8.0.1) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->lucem-illud==8.0.1) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->lucem-illud==8.0.1) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->lucem-illud==8.0.1) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->lucem-illud==8.0.1) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->lucem-illud==8.0.1) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->lucem-illud==8.0.1) (7.4.0)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->lucem-illud==8.0.1) (1.0.0)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=0.6->pysoundfile->lucem-illud==8.0.1) (2.20)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->lucem-illud==8.0.1) (0.2.5)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->IPython->lucem-illud==8.0.1) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->IPython->lucem-illud==8.0.1) (0.7.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->lucem-illud==8.0.1) (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->lucem-illud==8.0.1) (3.4.1)\n",
            "Building wheels for collected packages: lucem-illud, python-docx, pyanno3\n",
            "  Building wheel for lucem-illud (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lucem-illud: filename=lucem_illud-8.0.1-cp37-none-any.whl size=34956 sha256=288546caa261f0ef241d83d00740217b5bf49e5afae67f096ea2f4fe8ab8a3c0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wbc3dr6c/wheels/bd/50/3e/b51b39ce44f42e16944c633246df1a4e3f773545f7caed0796\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-cp37-none-any.whl size=184602 sha256=ccd440b7db8b67590d9ac3947a42c3f946173a52edbd4e924db5a5302c94e238\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/90/f1/a7cb70b38633ae04e7fb963b1c70f63fd6fc01c075b8230adc\n",
            "  Building wheel for pyanno3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyanno3: filename=pyanno3-2.0.2-cp37-none-any.whl size=116994 sha256=4300c809a62552c99f01b177cb538f49b4f41ec0b7e8231c58f68c17df0be037\n",
            "  Stored in directory: /root/.cache/pip/wheels/d8/5d/f6/7c1618b7471ec03ac97f6913baba31ae007003c4fa2bc99855\n",
            "Successfully built lucem-illud python-docx pyanno3\n",
            "\u001b[31mERROR: botocore 1.20.78 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: python-docx, pdfminer2, smmap, gitdb, GitPython, traits, pyanno3, jmespath, botocore, s3transfer, boto3, pydub, speechrecognition, pysoundfile, lucem-illud\n",
            "Successfully installed GitPython-3.1.17 boto3-1.17.78 botocore-1.20.78 gitdb-4.0.7 jmespath-0.10.0 lucem-illud-8.0.1 pdfminer2-20151206 pyanno3-2.0.2 pydub-0.25.1 pysoundfile-0.9.0.post1 python-docx-0.8.11 s3transfer-0.4.2 smmap-4.0.0 speechrecognition-3.8.1 traits-6.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRkwWNqrV182"
      },
      "source": [
        "#This provides access to data and to helper functions from previous weeks\n",
        "#Make sure you update it before starting this notebook\n",
        "import lucem_illud #pip install -U git+git://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git\n",
        "\n",
        "#All these packages need to be installed from pip\n",
        "import numpy as np #For arrays\n",
        "import scipy as sp #For some stats\n",
        "import pandas #Gives us DataFrames\n",
        "import matplotlib.pyplot as plt #For graphics\n",
        "import seaborn #Makes the graphics look nicer\n",
        "import pyanno #On python3 make sure to pip install pyanno3\n",
        "\n",
        "#We need to import these this way due to how pyanno is setup\n",
        "from pyanno.measures import pairwise_matrix, agreement, cohens_kappa, cohens_weighted_kappa, fleiss_kappa, krippendorffs_alpha, pearsons_rho, scotts_pi, spearmans_rho\n",
        "from pyanno.annotations import AnnotationsContainer\n",
        "from pyanno.models import ModelA, ModelBt, ModelB\n",
        "\n",
        "from functools import reduce\n",
        "from itertools import permutations\n",
        "import math\n",
        "\n",
        "\n",
        "#This 'magic' command makes the plots work better\n",
        "#in the notebook, don't use it outside of a notebook.\n",
        "#Also you can ignore the warning\n",
        "%matplotlib inline\n",
        "\n",
        "import os #For looking through files\n",
        "import os.path #For managing file paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIl4xklAV185"
      },
      "source": [
        "## Example Annotation Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOKC5F1TV185"
      },
      "source": [
        "Load Rzhetsky et al (2009)'s sample dataset, which can be found [here](https://github.com/enthought/uchicago-pyanno/tree/master/data). This data is the result of a content analytic / content extraction study in which Andrey Rzhetsky and colleagues from the National Library of Medicine, published [here](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000391) in [PLOS Computational Biology](http://journals.plos.org/ploscompbiol/), gave eight annotators 10,000 sentence chunks from biomedical text in biomedical abstracts and articles, then asked them, in a loop design schematically illustrated below that provided 3 independent codings for each document. The sampling strategy pursued diversity by drawing from PubMed abstracts (1000) and full-text articles (9000: 20% from abstracts, 10% from introductions, 20% from methods, 25% from results, and 25% from discussions.) The dataset extract here involves respondents codes for sentences in terms of their *Evidence*: {0, 1, 2, 3, -1} where 0 is the complete lack of evidence, 3 is direct evidence present within the sentence, and -1 is didn't respond. (They also crowdsourced and analyzed *polarity*, *certainty*, and *number*). For example, consider the following two abutting sentence chunks: *\"Because null mutations in toxR and toxT abolish CT and TcpA expression in the El Tor biotype and also attenuate virulence...\"* [i.e., average certainty = 0], *\"...it is likely that the ToxR regulon has functional similarities between the two biotypes despite the clear differences in the inducing parameters observed in vitro\"* [i.e., average certainty = 1].\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rewnl75ZV186",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5647ad3-c2ec-4e7b-d182-6c727565ab16"
      },
      "source": [
        "%%html\n",
        "<img source=\"loopdesign.png\">"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img source=\"loopdesign.png\">"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hlBwLvwV187"
      },
      "source": [
        "[Click here for loop design](loopdesign.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuXHXPPXV187"
      },
      "source": [
        "x = np.loadtxt(\"/content/pyAnno/testdata_numerical.txt\")\n",
        "anno = AnnotationsContainer.from_array(x, missing_values=[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMRJGqlRV187"
      },
      "source": [
        "Interrogate the AnnotationsContainer object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "586-PZ8mV188"
      },
      "source": [
        "anno.annotations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5QGd9pdV188"
      },
      "source": [
        "anno.labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOIorO7HV188"
      },
      "source": [
        "anno.missing_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4htBijAcV189"
      },
      "source": [
        "## Annotation Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGTNUugEV189"
      },
      "source": [
        "First, we assume categorical codes...that each code is qualitatively distinct from each other. Two measures are primarily used for this: Scott's $\\pi$, Cohen's $\\kappa$, and Krippendorff's $\\alpha$ which each measure the extent of agreement between two annotators, but take into account the possibility of the agreement occurring by chance in slightly different ways. Any agreement measure begins with the frequency of codes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmVLrQVUV189"
      },
      "source": [
        "pyanno.measures.agreement.labels_frequency(anno.annotations,4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti8-scntV18-"
      },
      "source": [
        "Now consider the \"confusion matrix\" or matrix of coded agreements between any two coders:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_s800HZV18-"
      },
      "source": [
        "c = pyanno.measures.agreement.confusion_matrix(anno.annotations[:,0], anno.annotations[:,1],4)\n",
        "print(c)\n",
        "ac = seaborn.heatmap(c)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYU_8VPcV18-"
      },
      "source": [
        "Scott's $\\pi$ is computed as:\n",
        "\n",
        "$\\pi = \\frac{\\text{Pr}(a)-\\text{Pr}(e)}{1-\\text{Pr}(e)}$\n",
        "\n",
        "Where Pr($a$) is relative observed agreement, and Pr($e$) is expected agreement using joint proportions calculated from the confusion matrix or matrix of coded agreements between any two coders:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82vPjoedV18-"
      },
      "source": [
        "scotts_pi(anno.annotations[:,0], anno.annotations[:,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYVDhh1KV18_"
      },
      "source": [
        "The generalization of Scott's $\\pi$ to $n$ coders is Fleiss' $\\kappa$ (Fleiss called it $\\kappa$ because he thought he was generalizing Cohen's $\\kappa$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF7vGAk_V18_"
      },
      "source": [
        "fleiss_kappa(anno.annotations[::])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4XsB8-PV18_"
      },
      "source": [
        "Krippendorff's $\\alpha$ generalizes of Fleiss' $\\kappa$ to $n$ coders and takes into account the fact that annotations here are not categorically different, but ordinal, by adding a weight matrix in which off-diagonal cells contain weights indicating the seriousness of the disagreement between each score. When produced with no arguments, it simply produces an arithmetic distance (e.g., 3-1=2), such that cells one off the diagonal are weighted 1, two off 2, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B_Gsv9hV18_"
      },
      "source": [
        "krippendorffs_alpha(anno.annotations[::])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbb9JDeVV19A"
      },
      "source": [
        "Like Scott's $\\pi$, Cohen's $\\kappa$ also takes into account the possibility of the agreement occurring by chance, but in the following way:\n",
        "\n",
        "$\\kappa = \\frac{p_o-p_e}{1-p_e}=1-\\frac{1-p_o}{p_e}$\n",
        "\n",
        "where $p_o$ is the relative observed agreement among raters, and $p_e$ is the hypothetical probability of chance agreement, using the observed data to calculate the probabilities of each observer randomly saying each category. If the raters are in complete agreement then $\\kappa = 1$. If there is no agreement among the raters other than what would be expected by chance (as given by $p_e$), $\\kappa ≤ 0 $. Here, Cohen's $\\kappa$ statistic for the first two annotators is computed. This is probably the most common metric of agreement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTvcFYQQV19A"
      },
      "source": [
        "cohens_kappa(anno.annotations[:,0], anno.annotations[:,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruqyXSqGV19B"
      },
      "source": [
        "m = pairwise_matrix(cohens_kappa, anno.annotations)\n",
        "print(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4Elw_flV19B"
      },
      "source": [
        "ax = seaborn.heatmap(m)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLUrx-5cV19B"
      },
      "source": [
        "You can see that this 8 by 3 loop design will be less stable than an 8 choose 3 combinatorial design, because each codes with more others. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyzaKu2pV19C"
      },
      "source": [
        "One can also assess the average Cohen's $\\kappa$ for all pairs of coders that have coded against one another:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWrkoTt3V19C"
      },
      "source": [
        "def pairwise_metric_average(metric, array):\n",
        "    \"\"\"Calculate the pairwise metric average for the real elements of metric function run on an array of annotations\"\"\"\n",
        "    p = permutations(range(array[0,:].size),2)\n",
        "    m = [metric(array[:,x[0]], array[:,x[1]]) for x in p]\n",
        "    clean_m = [c for c in m if not math.isnan(c)]\n",
        "    return reduce(lambda a, b: a + b, clean_m)/len(clean_m)    \n",
        " \n",
        "pairwise_metric_average(cohens_kappa, anno.annotations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHrRAHHpV19C"
      },
      "source": [
        "As recognized with Krippendorff's flexible $\\alpha$, our scores are *not* categorical, but rather ordered and her considered metric. Weighted $\\kappa$ allows you to count disagreements differently and is useful when codes are ordered as they are here. Here a weight matrix is added to the calculation, in which off-diagonal cells contain weights indicating the seriousness of the disagreement between each score. When automatically produced, it simply produces an arithmetic distance (e.g., 3-1=2), such that cells one off the diagonal are weighted 1, two off 2, etc. Here\n",
        "\n",
        "$\\kappa = 1-\\frac{\\sum^k_{i=1}\\sum^k_{j=1}w_{ij}x_{ij}}{\\sum^k_{i=1}\\sum^k_{j=1}w_{ij}m_{ij}}$\n",
        "\n",
        "where $\\kappa$ = $n$ codes and $w_{ij}$,$x_{ij}$, and $m_{ij}$ represent elements in the weight, observed, and expected matrices, respectively. (Obviously, when diagonal cells contain weights of 0 and off-diagonal cells weights of 1, this equals $\\kappa$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FZKRwqvV19C"
      },
      "source": [
        "cohens_weighted_kappa(anno.annotations[:,0], anno.annotations[:,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKqarpwlV19D"
      },
      "source": [
        "Or averaged over the total:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCcpcXFZV19D"
      },
      "source": [
        "pairwise_metric_average(cohens_weighted_kappa,anno.annotations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "127jxCIyV19D"
      },
      "source": [
        "Alternatively, if the annontation data can be understood as indicating real values, we can assess not agreement, but rather the correlation of values (Pearson's $\\rho$) or correlation of ranks (Spearman's $\\rho$) for pairs of coders:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QER6H5rV19D"
      },
      "source": [
        "n = pairwise_matrix(pearsons_rho, anno.annotations)\n",
        "m = pairwise_matrix(spearmans_rho, anno.annotations)\n",
        "an = seaborn.heatmap(n)\n",
        "plt.show()\n",
        "am = seaborn.heatmap(m)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwg5klTdV19D"
      },
      "source": [
        "Or averaged over all comparable pairs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQeCJmjyV19E"
      },
      "source": [
        "print(pairwise_metric_average(pearsons_rho,anno.annotations), pairwise_metric_average(spearmans_rho,anno.annotations))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1VrQsJoV19E"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai7OspMyV19E"
      },
      "source": [
        "However, what if some coders are better than others. The prior measures all rely on the assumption that all coders are equally good. What if some are worse than others? Now we use Rzhetsky et al (2009) and Dawid & Skene's models to make inference about true label classes by downweighting bad or deviant coders. Pyanno provides three relevant models: ModelA, ModelB, and ModelBt. Model A can only be currently run on a balanced 8-coder design, but assesses accuracy purely based on agreement. Model B with $\\theta$s models the relationship between each coder and code. Model B is the Dawid & Skene model from the reading. The following image schematically suggests the relationship between the models. <img src=\"../data/models.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pGdYBZ0V19E"
      },
      "source": [
        "The models should provide similar results. To estimate the parameters for any models, we first need to create a new model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0O59GAqV19F"
      },
      "source": [
        "# create a new instance of model A, with 4 label classes\n",
        "model = ModelB.create_initial_state(4, 8)\n",
        "# other model parameters are initialized from the model prior\n",
        "print(model.theta)\n",
        "print(model.log_likelihood(anno.annotations))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7ntdVMnV19F"
      },
      "source": [
        "samples = model.sample_posterior_over_accuracy(anno.annotations, 200, burn_in_samples=100, thin_samples=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYhQTm94V19F"
      },
      "source": [
        "Pyanno allows one to use either MLE (maximum likelihood estimation) or MAP (maximum a posteriori estimation) to estimate model parameters. Note that the parameters here correspond to our estimation of the accuracy of each annotator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H161ME1zV19F"
      },
      "source": [
        "model.map(anno.annotations)\n",
        "print(model.theta)\n",
        "print(model.log_likelihood(anno.annotations))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A704-twbV19F"
      },
      "source": [
        "model = ModelB.create_initial_state(4, 8)\n",
        "model.map(anno.annotations)\n",
        "print(model.theta)\n",
        "print(model.log_likelihood(anno.annotations))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXxbbfIcV19G"
      },
      "source": [
        "Once we have model parameters estimated, we can now make inferences about the true label classes. We can calculate the posterior distribution over the true label classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxkQDgqwV19G"
      },
      "source": [
        "posterior = model.infer_labels(anno.annotations)\n",
        "print(posterior)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejdVGi6tV19G"
      },
      "source": [
        "Let's turn the posterior of the first 100 samples into a heatmap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9CWX6N-V19G"
      },
      "source": [
        "votes = []\n",
        "for r in anno.annotations:\n",
        "    v = [0] * len(anno.labels)\n",
        "    votes.append(v)\n",
        "    for a in r:\n",
        "        if a > -1:\n",
        "            v[a] += 1\n",
        "votes_array = np.array(votes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWZVo7NJV19H"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize = (15, 10), sharey=True)\n",
        "num_questions = 20\n",
        "\n",
        "seaborn.heatmap(votes_array[:num_questions], annot = True, ax=ax2)\n",
        "seaborn.heatmap(posterior[:num_questions], annot=True, ax =ax1)\n",
        "ax1.set_title(\"Model\")\n",
        "ax2.set_title(\"Votes\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F31p6Ku1V19H"
      },
      "source": [
        "This differs markedly from taking annotator scores at face value (Add comparison of average values)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mYpyDcWV19H"
      },
      "source": [
        "samples = model.sample_posterior_over_accuracy(anno.annotations, 200, burn_in_samples=100, thin_samples=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lQqA4V0V19H"
      },
      "source": [
        "print(samples[0].mean(axis=0))\n",
        "print(samples[0].std(axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAwFGIeV19H"
      },
      "source": [
        "Let's try everything again with ModelBt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtMy40DfV19I"
      },
      "source": [
        "# create a new instance of model B, with 4 label classes and 8 annotators.\n",
        "model = ModelBt.create_initial_state(4, 8)\n",
        "print(model.theta)\n",
        "print(model.log_likelihood(anno.annotations))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlJJ6koiV19I"
      },
      "source": [
        "model.map(anno.annotations)\n",
        "print(model.theta)\n",
        "print(model.log_likelihood(anno.annotations))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WWzHGDHV19I"
      },
      "source": [
        "posterior = model.infer_labels(anno.annotations)\n",
        "print(posterior)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5tKU8zYV19I"
      },
      "source": [
        "Let's visualize the posterior of the first 10 samples according to ModelBt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqVb3V4tV19I"
      },
      "source": [
        "ax = seaborn.heatmap(posterior[:10,])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS79Bo5SV19J"
      },
      "source": [
        "The property of these scores is that they enable us to identify the most likely code assuming coders of unequal quality, which also allows us to break ties when we know coder identity. For some analyses, we may simply use the posterior themselves rather than the most probably code outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHWOCrXLV19J"
      },
      "source": [
        "## Generating Annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc_54qfZV19J"
      },
      "source": [
        "Pyanno also allows one to generate artificial data from a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQvjA0mOV19J"
      },
      "source": [
        "model = ModelBt.create_initial_state(4, 3, theta=[0.99,0.75,0.25])\n",
        "#randome generate annotations with 4 label classes and 3 annotators. The accuracy of the three annotators are 0.99, 0.75, and 0.25 respectively.\n",
        "model.generate_annotations(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eykUoqBZV19J"
      },
      "source": [
        "## Visualizing coder accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyAKsh1VV19K"
      },
      "source": [
        "Pyanno provides a [graphical user interface](http://docs.enthought.com/uchicago-pyanno/user_guide.html) for making plots. However, it is not compatible with ipython notebooks. Nevertheless, nothing prevents us from making plots using matplotlib. Let's make a plot of the accuracy of each annotator inferred from ModelA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0SF2QvsV19K"
      },
      "source": [
        "model = ModelBt.create_initial_state(4, 8)\n",
        "model.mle(anno.annotations)\n",
        "samples = model.sample_posterior_over_accuracy(anno.annotations, 200, burn_in_samples=100, thin_samples=3)\n",
        "y =  samples.mean(axis=0)#.mean(axis = 1).mean(axis = 1)\n",
        "y_ci = samples.std(axis=0)#.mean(axis = 1).mean(axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH79yOaWV19K"
      },
      "source": [
        "plt.figure()\n",
        "plt.errorbar(range(8),y, yerr = y_ci)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1-sNTW0V19K"
      },
      "source": [
        "## Example with articles that use the General Social Survey"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPpLLg_2V19K"
      },
      "source": [
        "I performed a recent study in which the variables from thousands of articles were associated with those used in the General Social Survey, a widely used population sample, in order to interrogate how social science analyses are performed. Each article was reread and coded by a balanced set of three student coders using a 6 choose 3 design, such that all possible 3-coder-subsets (20) coded an equal number of articles. Coding was performed through a website that allowed students access to the digital article. To evaluate the validity of the student codes, we also recruited a sample of authors associated with 97 of our published articles to fill out the same online survey. \n",
        "\n",
        "Because not all coders coded items with equal accuracy, and because “don’t know” was an optional answer, leading to potential ties, we used a generative, probabilistic model to estimate the maximum a posteriori probability (MAP) prediction that an item’s code is true, which integrates over the estimated accuracy of coders, assuming only that the entire population of coders is slightly more often right than wrong. The model (“Model B”) is based on a simple underlying generation process that directly accounts for the probability that coded values are correct (Rzhetsky et al. 2009). For each coded value j, a set of parameters, denoted γj, represents the probability that each coded value is correct. For the ith coder (i = 1, 2, …, 6), we introduce a matrix of probabilities, denoted λ(i)x|y, that defines the probability that she assigns code x (e.g., Dependent variable) to a GSS variable with correct annotation y. For a perfect coder, the matrix λ(i)x|y would equal the identity matrix and her vote would count most toward the total. For a coder that always codes incorrectly—a “troll”—her matrix λ(i)x|y will have all its value off the diagonal and will only minimally influence the posterior. We co-authored the open source pyanno software that implements this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REXYehoMV19L"
      },
      "source": [
        "Getting the data for each content analysis survey regarding how GSS variables were used in a large population of social science articles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykNTFn9NV19L"
      },
      "source": [
        "#anno_vdep = AnnotationsContainer.from_file(missing_values=[-1], filename=\"GSSvariable_testSdependent.csv\")\n",
        "dev = np.loadtxt(fname=\"/content/dataforgssstudy/n7GSSvariable_testSdependent.csv\", dtype=int, delimiter=\",\")\n",
        "anno_dv = AnnotationsContainer.from_array(dev)\n",
        "\n",
        "ind = np.loadtxt(fname=\"/content/dataforgssstudy/n7GSSvariable_testSindependent.csv\", dtype=int, delimiter=\",\")\n",
        "anno_iv = AnnotationsContainer.from_array(ind)\n",
        "\n",
        "cent = np.loadtxt(fname=\"/content/dataforgssstudy/n7GSSvariable_testScentral.csv\", dtype=int, delimiter=\",\")\n",
        "anno_cv = AnnotationsContainer.from_array(cent)\n",
        "\n",
        "cont = np.loadtxt(fname=\"/content/dataforgssstudy/n7GSSvariable_testScontrol.csv\", dtype=int, delimiter=\",\")\n",
        "anno_ctv = AnnotationsContainer.from_array(cont)\n",
        "\n",
        "test = np.loadtxt(fname=\"/content/dataforgssstudy/testH.csv\", dtype=int, delimiter=\",\")\n",
        "anno_test = AnnotationsContainer.from_array(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujJLFwp_V19L"
      },
      "source": [
        "Let's examine the data structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rve3E6KWV19M"
      },
      "source": [
        "dev.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anEzFAUlV19M"
      },
      "source": [
        "anno_dv.labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td7UJTVgV19M"
      },
      "source": [
        "anno_dv.missing_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbXn8-uMV19M"
      },
      "source": [
        "anno_dv.annotations.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGEd31uGV19N"
      },
      "source": [
        "First, let's use Cohen's $\\kappa$ to measure agreement between coders..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_1p9YwfV19N"
      },
      "source": [
        "m = pairwise_matrix(cohens_kappa, anno_dv.annotations)\n",
        "print(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QJb1YwoV19N"
      },
      "source": [
        "Let's visualize that..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SSq9NgPV19N"
      },
      "source": [
        "ax = seaborn.heatmap(m)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofn7SzXXV19N"
      },
      "source": [
        "pairwise_metric_average(cohens_kappa, anno_dv.annotations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB1UPSHRV19O"
      },
      "source": [
        "Let's compute the statistics on each of the datasets and with Pearson's $\\rho$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLzY8eT4V19O"
      },
      "source": [
        "datasets = [anno_dv.annotations, anno_iv.annotations, anno_cv.annotations, anno_ctv.annotations]\n",
        "ck = [pairwise_matrix(cohens_kappa, anno) for anno in datasets]\n",
        "pr = [pairwise_matrix(pearsons_rho, anno) for anno in datasets]\n",
        "titles = ['DV', 'IV', 'Central Variable', \"Control Variable\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcryQ-IUV19O"
      },
      "source": [
        "fig, axs = plt.subplots(2,4)\n",
        "fig.set_size_inches(18, 7)\n",
        "for k, ax, title in zip(ck,axs[0], titles):\n",
        "    seaborn.heatmap(k, ax = ax)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "for r, ax in zip(pr,axs[1]):\n",
        "    seaborn.heatmap(r, ax = ax)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfw_Ve0EV19O"
      },
      "source": [
        "Now we will compare the student coders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaB6iFr2V19O"
      },
      "source": [
        "nondiag = (np.eye(6)-np.ones(6))*-1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJz9cZMgV19P"
      },
      "source": [
        "xdevck = pairwise_matrix(cohens_kappa, anno_dv.annotations)\n",
        "xdevpr = pairwise_matrix(pearsons_rho, anno_dv.annotations)\n",
        "\n",
        "xindck = pairwise_matrix(cohens_kappa, anno_iv.annotations)\n",
        "xindpr = pairwise_matrix(pearsons_rho, anno_iv.annotations)\n",
        "\n",
        "xcenck = pairwise_matrix(cohens_kappa, anno_cv.annotations)\n",
        "xcenpr = pairwise_matrix(pearsons_rho, anno_cv.annotations)\n",
        "\n",
        "xconck = pairwise_matrix(cohens_kappa, anno_ctv.annotations)\n",
        "xconpr = pairwise_matrix(pearsons_rho, anno_ctv.annotations)\n",
        "\n",
        "print(np.average(xdevck, weights=nondiag))\n",
        "print(np.average(xdevpr, weights=nondiag))\n",
        "print(np.average(xindck, weights=nondiag))\n",
        "print(np.average(xindpr, weights=nondiag))\n",
        "print(np.average(xcenck, weights=nondiag))\n",
        "print(np.average(xcenpr, weights=nondiag))\n",
        "print(np.average(xconck, weights=nondiag))\n",
        "print(np.average(xconpr, weights=nondiag))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzTMrpSgV19P"
      },
      "source": [
        "Now we are going to bring in \"gold standard\" data. In this case, this is where we asked authors of the articles to code their own article's variables and compare with our student coders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ge0-xeaV19P"
      },
      "source": [
        "mergedata = np.loadtxt(fname=\"/content/dataforgssstudy/gss_mergedataC.txt\", dtype=int, delimiter=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH2l3nL-V19P"
      },
      "source": [
        "anno_merge_dep = AnnotationsContainer.from_array(mergedata[:,0:2])\n",
        "anno_merge_ind = AnnotationsContainer.from_array(mergedata[:,2:4])\n",
        "anno_merge_cen = AnnotationsContainer.from_array(mergedata[:,4:6])\n",
        "anno_merge_con = AnnotationsContainer.from_array(mergedata[:,6:8])\n",
        "anno_merge_dkn = AnnotationsContainer.from_array(mergedata[:,8:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szyM6zO-V19P"
      },
      "source": [
        "print(\"\"\"Dependent variable -- kappa & rho\"\"\")\n",
        "print(cohens_kappa(anno_merge_dep.annotations[:,0], anno_merge_dep.annotations[:,1]))\n",
        "print(pearsons_rho(anno_merge_dep.annotations[:,0], anno_merge_dep.annotations[:,1]))\n",
        "\n",
        "print(\"\\nIndependent variable\")\n",
        "print(cohens_kappa(anno_merge_ind.annotations[:,0], anno_merge_ind.annotations[:,1]))\n",
        "print(pearsons_rho(anno_merge_ind.annotations[:,0], anno_merge_ind.annotations[:,1]))\n",
        "\n",
        "print(\"\\nCentral variable\")\n",
        "print(cohens_kappa(anno_merge_cen.annotations[:,0], anno_merge_cen.annotations[:,1]))\n",
        "print(pearsons_rho(anno_merge_cen.annotations[:,0], anno_merge_cen.annotations[:,1]))\n",
        "\n",
        "print(\"\\nControl variable\")\n",
        "print(cohens_kappa(anno_merge_con.annotations[:,0], anno_merge_con.annotations[:,1]))\n",
        "print(pearsons_rho(anno_merge_con.annotations[:,0], anno_merge_con.annotations[:,1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg_Fd_kkV19Q"
      },
      "source": [
        "Whoah! Student coders and authors viewed articles that were \"central\" or critical to the published argument as fundamentally different (exhibiting negative agreement and correlation). Why? Likely because that researchers recalled what they had _intended_ as their central variables before analysis, but those that _worked out_ became central in the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsGr1VyKV19Q"
      },
      "source": [
        "Now for the assessment of the relative values of authors, then student coders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HET5rhSkV19Q"
      },
      "source": [
        "print(\"Dependent\")\n",
        "print(np.average(anno_merge_dep.annotations[:,0]))\n",
        "print(np.average(anno_merge_dep.annotations[:,1]))\n",
        "\n",
        "print(\"\\nIndependent\")\n",
        "print(np.average(anno_merge_ind.annotations[:,0]))\n",
        "print(np.average(anno_merge_ind.annotations[:,1]))\n",
        "\n",
        "print(\"\\nCentral\")\n",
        "print(np.average(anno_merge_cen.annotations[:,0]))\n",
        "print(np.average(anno_merge_cen.annotations[:,1]))\n",
        "\n",
        "print(\"\\nControl\")\n",
        "print(np.average(anno_merge_con.annotations[:,0]))\n",
        "print(np.average(anno_merge_con.annotations[:,1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kzptpvcV19Q"
      },
      "source": [
        "## Now we are going to use models to predict the correct annotations\n",
        "\n",
        "Recall that Model A is built for 8 coders, but we have 6. We're going to *hack* it by adding two blank columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cknvyc-fV19Q"
      },
      "source": [
        "dev.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mRmhgc_V19R"
      },
      "source": [
        "negs2 = np.ones((21461, 2), dtype=np.int)*(-1)\n",
        "devA = np.concatenate((dev, negs2), axis=1)\n",
        "devA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyBJoztQV19R"
      },
      "source": [
        "anno_dvA = AnnotationsContainer.from_array(devA)\n",
        "model_devA = ModelA.create_initial_state(2)\n",
        "model_devA.theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnCUfJ1AV19R"
      },
      "source": [
        "model_dvB = ModelB.create_initial_state(2, 6)\n",
        "print(model_dvB.pi)\n",
        "print(model_dvB.log_likelihood(anno_dv.annotations))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5hwgo9AV19R"
      },
      "source": [
        "model_dvB.map(anno_dv.annotations)\n",
        "print(model_dvB.pi)\n",
        "print(model_dvB.log_likelihood(anno_dv.annotations))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgNOov2UV19S"
      },
      "source": [
        "# compute the posterior distribution over true annotations\n",
        "posterior_dvB = model_dvB.infer_labels(anno_dv.annotations)\n",
        "# each row show the probability of each label class for the\n",
        "# corresponding item\n",
        "print(posterior)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmnEV8hxV19S"
      },
      "source": [
        "samples_dvB = model_dvB.sample_posterior_over_accuracy(anno_dv.annotations, 200, burn_in_samples=100, thin_samples=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF10F9TiV19S"
      },
      "source": [
        "# we can then compute a credible interval for the parameters:\n",
        "ci_dv_mean = samples_dvB[0].mean(axis=0)\n",
        "print(\"Mean\")\n",
        "print(ci_dv_mean)\n",
        "\n",
        "ci_dv_stdev = samples_dvB[0].std(axis=0)\n",
        "print(\"\\nSTD\")\n",
        "print(ci_dv_stdev)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaGHPto_V19S"
      },
      "source": [
        "We will use Model B estimates for other variable assessments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tgcP3YQV19S"
      },
      "source": [
        "#test\n",
        "model_testB = ModelB.create_initial_state(2, 6)\n",
        "print(model_testB.log_likelihood(anno_test.annotations))\n",
        "model_testB.map(anno_test.annotations)\n",
        "print(model_testB.pi)\n",
        "print(model_testB.log_likelihood(anno_test.annotations))\n",
        "print(anno_test.annotations.shape)\n",
        "posterior_testB = model_testB.infer_labels(anno_test.annotations)\n",
        "print(posterior_testB.shape)\n",
        "samples_testB = model_testB.sample_posterior_over_accuracy(anno_test.annotations, 200, burn_in_samples=100, thin_samples=3)\n",
        "ci_test_mean = samples_testB[0].mean(axis=0)\n",
        "print(ci_test_mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3YcH9i9V19T"
      },
      "source": [
        "#indepedent variables\n",
        "model_ivB = ModelB.create_initial_state(2, 6)\n",
        "print(model_ivB.log_likelihood(anno_iv.annotations))\n",
        "model_ivB.map(anno_iv.annotations)\n",
        "print(model_ivB.pi)\n",
        "print(model_ivB.log_likelihood(anno_iv.annotations))\n",
        "print(anno_iv.annotations.shape)\n",
        "posterior_ivB = model_ivB.infer_labels(anno_iv.annotations)\n",
        "print(posterior_ivB.shape)\n",
        "samples_ivB = model_ivB.sample_posterior_over_accuracy(anno_iv.annotations, 200, burn_in_samples=100, thin_samples=3)\n",
        "ci_iv_mean = samples_ivB[0].mean(axis=0)\n",
        "print(ci_iv_mean)\n",
        "\n",
        "#central variables\n",
        "model_cvB = ModelB.create_initial_state(2, 6)\n",
        "print(model_cvB.log_likelihood(anno_cv.annotations))\n",
        "model_cvB.map(anno_cv.annotations)\n",
        "print(model_cvB.pi)\n",
        "print(model_cvB.log_likelihood(anno_cv.annotations))\n",
        "print(anno_cv.annotations.shape)\n",
        "posterior_cvB = model_cvB.infer_labels(anno_cv.annotations)\n",
        "print(posterior_cvB.shape)\n",
        "samples_cvB = model_cvB.sample_posterior_over_accuracy(anno_cv.annotations, 200, burn_in_samples=100, thin_samples=3)\n",
        "ci_cv_mean = samples_cvB[0].mean(axis=0)\n",
        "print(ci_cv_mean)\n",
        "\n",
        "#control variables\n",
        "model_ctvB = ModelB.create_initial_state(2, 6)\n",
        "print(model_ctvB.log_likelihood(anno_ctv.annotations))\n",
        "model_ctvB.map(anno_ctv.annotations)\n",
        "print(model_ctvB.pi)\n",
        "print(model_ctvB.log_likelihood(anno_ctv.annotations))\n",
        "print(anno_ctv.annotations.shape)\n",
        "posterior_ctvB = model_ctvB.infer_labels(anno_ctv.annotations)\n",
        "print(posterior_ctvB.shape)\n",
        "samples_ctvB = model_ctvB.sample_posterior_over_accuracy(anno_iv.annotations, 200, burn_in_samples=100, thin_samples=3)\n",
        "ci_ctv_mean = samples_ctvB[0].mean(axis=0)\n",
        "print(ci_ctv_mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-ZBzVl7V19T"
      },
      "source": [
        "Now we will package up the predicted data into a format we can use for other, subsequent analysis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7kacD-JV19T"
      },
      "source": [
        "print(posterior_dvB.shape)\n",
        "print(posterior_ivB.shape)\n",
        "print(posterior_cvB.shape)\n",
        "print(posterior_ctvB.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baz0MZSUV19T"
      },
      "source": [
        "predicted_annotations = np.concatenate((posterior_dvB, posterior_ivB, posterior_cvB, posterior_ctvB), axis=1) # posterior_dvBt, posterior_ivBt, posterior_cvBt, posterior_ctvBt), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkr9qMrzV19T"
      },
      "source": [
        "predicted_annotations.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "385kWOt0V19U"
      },
      "source": [
        "These annotations allowed us to uncover the degree to which social scientists alter their models to achieve a better fit...undocumented data mining. The answer was that social scientists did mine their data, but that it likely improved their analysis because change in the social world was the result of greater distortion than undocumented data mining."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEyYlhHlV19U"
      },
      "source": [
        "## Another example analysis looks at a different data set of Hotel Reviews by a variety of patrons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0YIMkStV19U"
      },
      "source": [
        "df_hotels = pandas.read_csv('/content/hot_Reviews.csv', index_col=0)\n",
        "df_hotels[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qLr3TlnV19U"
      },
      "source": [
        "Here a rank of 0 is a missing value and to simplify things more we will convert from a 1-10 scale to a 1-5 scale, with 0 as missing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Jng_BIMV19U"
      },
      "source": [
        "df_hotels = df_hotels.apply(lambda x: x // 2) #integer divide by 2 rounds all values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFieIIYrV19V"
      },
      "source": [
        "And we can visualize all the reviews as a heatmap with the missing values greyed out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOXd9xZHV19V"
      },
      "source": [
        "fig, ax = plt.subplots(figsize = (20,20))\n",
        "seaborn.heatmap(df_hotels, cmap='rainbow', ax = ax)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mORkEopOV19V"
      },
      "source": [
        "To give the dataframe to pyanno we need to convert to np array and change the nans to intergers, lets use -1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFt1Y3xKV19V"
      },
      "source": [
        "hot_mat = np.array(df_hotels.fillna(-1).as_matrix())\n",
        "anno_hot = AnnotationsContainer.from_array(hot_mat, missing_values=[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "911mzelcV19V"
      },
      "source": [
        "anno_hot.annotations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeZCWq0EV19W"
      },
      "source": [
        "anno_hot.labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOG5dn_-V19W"
      },
      "source": [
        "anno_hot.missing_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJwDgpgqV19W"
      },
      "source": [
        "Look at coder agreement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vnqPuNdV19W"
      },
      "source": [
        "pyanno.measures.agreement.labels_frequency(anno_hot.annotations, 6)#6 possible catagories"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIUq3s9XV19W"
      },
      "source": [
        "c = pyanno.measures.agreement.confusion_matrix(anno_hot.annotations[:,0], anno_hot.annotations[:,1], 6) #6 possible catagories\n",
        "print(c)\n",
        "ac = seaborn.heatmap(c)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzFTsrUfV19X"
      },
      "source": [
        "Most agreement is on 2 i.e. an average hotel and there's little agreement as rating go higher, likely due to scarcity in the sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpByJLYLV19X"
      },
      "source": [
        "scotts_pi(anno_hot.annotations[:,0], anno_hot.annotations[:,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyGrxGGGV19X"
      },
      "source": [
        "krippendorffs_alpha(anno_hot.annotations[::])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VfYRNzeV19X"
      },
      "source": [
        "cohens_kappa(anno_hot.annotations[:,0], anno_hot.annotations[:,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wPQs29sV19X"
      },
      "source": [
        "m = pairwise_matrix(cohens_kappa, anno_hot.annotations)\n",
        "fig, ax = plt.subplots(figsize = (15, 15))\n",
        "seaborn.heatmap(m, ax =ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udiZkNFBV19Y"
      },
      "source": [
        "model_hot = ModelBt.create_initial_state(6, 49)\n",
        "model_hot.mle(anno_hot.annotations)\n",
        "print(model.theta)\n",
        "print(model_hot.log_likelihood(anno_hot.annotations))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D05y2X6wV19Y"
      },
      "source": [
        "def makeQuestionComparison(model, anno_target, num_questions = 20):\n",
        "    votes = []\n",
        "    for r in anno_target.annotations:\n",
        "        v = [0] * len(anno_target.labels)\n",
        "        votes.append(v)\n",
        "        for a in r:\n",
        "            if a > -1:\n",
        "                v[a] += 1\n",
        "    votes_array = np.array(votes)\n",
        "    posterior = model.infer_labels(anno_target.annotations)\n",
        "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize = (15, 10), sharey=True)\n",
        "\n",
        "    seaborn.heatmap(votes_array[:num_questions], annot = True, ax=ax2)\n",
        "    seaborn.heatmap(np.nan_to_num(posterior,0)[:num_questions], annot=True, ax =ax1)\n",
        "    ax1.set_title(\"Model\")\n",
        "    ax2.set_title(\"Votes\")\n",
        "    return fig, (ax1, ax2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxea9X-iV19Y"
      },
      "source": [
        "makeQuestionComparison(model_hot, anno_hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLWH1-JYYalC"
      },
      "source": [
        "# Homework\n",
        "\n",
        "In this notebook we explored a variety of sampling methods, as well as its applications to machine learning and deep learning. We then explored annotations of your data (these are what power our models!), and how to identify reliable annotations. These concepts will be crucial as we discover active learning in the next tutorial. \n",
        "\n",
        "In this notebook's exercises, you will be using these concepts in the context of your social scientific data and experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYMeFeNw_OHC"
      },
      "source": [
        "**1)** Run 3 probabilistic sampling methods and 2 non-probabilistic methods and explore the samples returned. How would sampling help with your data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e2KuerfAOVT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bjpVT5G_ixZ"
      },
      "source": [
        "sampling_help = 'value' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol7uyuUX_jQ5"
      },
      "source": [
        "**2)** Find an imbalanced dataset and build a classifier to predict the label causing the imbalance. Explore undersampling and oversampling solutions to your dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQzty-sp_2lu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "208OUAVt_40f"
      },
      "source": [
        "**3)** Use bootstrap sampling to test the stability of a word, sentence, or graph embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdHlotohAAPC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-jvSKsYV19Y"
      },
      "source": [
        "**4)** Perform a content annotation survey of some kind in which at least 3 people evaluate and code each piece of content, using Amazon Mechanical Turk as described in the link below, or by hand with friends.  With the resulting data, calculate, visualize and discuss inter-coder agreement or covariation with appropriate metrics. What does this means for the reliability of human assessments regarding content in your domain?\n",
        "\n",
        "Run your own survey and annotations: [MTurk tutorial](https://canvas.uchicago.edu/courses/33672/files/4767031/download).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJG0nAQV_HvS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJa-RipyASBd"
      },
      "source": [
        "reliability = 'value' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UctWeGKc-0sC"
      },
      "source": [
        "\n",
        "**5)** In the cells immediately following, use the results of your content annotation survey to predict high and low-quality analysts, then predict MAP estimates for your codes in question. What do these estimates suggest about the distribution of skill among your coders? How different are these estimates from a majority vote?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yykso2L9Axbl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FANuSqmcAVBI"
      },
      "source": [
        "reliability = 'value' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}