{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "tutorial_2_advanced_deep_learning_for_text.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5xk9k0xgoTjE",
        "koRIOArfoTjM",
        "qq6IR_LgoTjP",
        "slMtEO_FoTjR",
        "r8wE-4jioTjR",
        "-u_pJpTKoTjS",
        "4TLHkNqzoTjS",
        "_xyZikOT-ZMx",
        "-H5fFtGt-ZM0",
        "B_XYjM4p7ZtP",
        "6xd8NOQB7ZtQ",
        "hhCeAYWN7ZtR"
      ],
      "toc_visible": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7eb35e89b40d4160be485628ead4aebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6d1fe792c2734dd9894723b493b230eb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_df2e14a318d649b4b932419f30c2b762",
              "IPY_MODEL_7d4a3a09605b4fed8627177ebf5f37c6"
            ]
          }
        },
        "6d1fe792c2734dd9894723b493b230eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "df2e14a318d649b4b932419f30c2b762": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f7407246c9ed4cc098b953b45c03a553",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 244715968,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 244715968,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c6a792a3b276421aa49ebed2d325d567"
          }
        },
        "7d4a3a09605b4fed8627177ebf5f37c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ec004cc52bc548dab857bf1632d4f8a9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 245M/245M [00:17&lt;00:00, 13.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f183bf7ec0814c0898ac0a1a2afc06db"
          }
        },
        "f7407246c9ed4cc098b953b45c03a553": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c6a792a3b276421aa49ebed2d325d567": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ec004cc52bc548dab857bf1632d4f8a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f183bf7ec0814c0898ac0a1a2afc06db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VENUSp4hdmSO"
      },
      "source": [
        "# Thinking with Deep Learning: Week 4 Part 2\n",
        "# Advanced Deep Learning for Text\n",
        "\n",
        "__Instructor:__ James Evans\n",
        "\n",
        "__Teaching Assistants & Content Creators/Organisers:__ Bhargav Srinivasa Desikan, Likun Cao\n",
        "\n",
        "In this notebook, we will be exploring different state of the art deep learning models. Most of these methods are based on powerful encoder-decoder systems called Transformer, and a certain alignment trick called Attention. \n",
        "\n",
        "Throughout this tutorial, we will link to endorsed resources and blog posts which we highly recommend reading before diving into the code. The purpose of this notebook is to accustom you to the power of these large, pre-trained language models. We will explore most of the standard tasks which one can do using these Transformers, as well as visualisations to better understand what makes a language model tick.\n",
        "\n",
        "NOTE: the first two sections of this tutorial introduce sequence to sequence models, with and without attention. We use machine language translation between french and english as the example, but note that we can use any sequence to sequence pair (Question-Answer, Human-Chatbot, etc).\n",
        "\n",
        "NOTE: As usual, we advice you to check the HW first - we focus on testing your use of the transformers models, with an optional question (for extra credit) on sequence to sequence tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o90p-q8JNEMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51eb1542-8efb-474d-9384-bb020a30566d"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 23.4MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 42.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDTkQEaHNDe9"
      },
      "source": [
        "# Encoders, Decoders, Seq2Seq and Attention\n",
        "\n",
        "Encoder-Decoder models are a powerful way to deal with sequence to sequence problems - this kind of setup is also called seq2seq.\n",
        "\n",
        "When we use this kind of sequence to sequence alignment and also map certain parts of the sequences with different values (a process called Attention), the sequence to sequence prediction task performance increases. In this section and the next we will focus on Machine Translation as the domain to try Encoder and Decoder tasks, without and with attention. \n",
        "\n",
        "**We highly recommend using this [visual blog](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) which explains seq2seq and attention based models**\n",
        "\n",
        "We will be using the code from a Keras tutorial and PyTorch tutorial to demonstrate these models.\n",
        "\n",
        "NOTE: these sections can be skimmed through - you can go through these in detail if you decide to do the HW question on sequence to sequence pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9UQ-9budmSS"
      },
      "source": [
        "# empty cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIuje1-Ve-67"
      },
      "source": [
        "## Character-level recurrent sequence-to-sequence model\n",
        "\n",
        "This code is adapted from the [Keras official tutorial](https://keras.io/examples/nlp/lstm_seq2seq/) on seq2seq using LSTM, authored by [fchollet](https://twitter.com/fchollet). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW7DdSj0e-7J"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "This example demonstrates how to implement a basic character-level\n",
        "recurrent sequence-to-sequence model. We apply it to translating\n",
        "short English sentences into short French sentences,\n",
        "character-by-character. Note that it is fairly unusual to\n",
        "do character-level machine translation, as word-level\n",
        "models are more common in this domain.\n",
        "\n",
        "**Summary of the algorithm**\n",
        "\n",
        "- We start with input sequences from a domain (e.g. English sentences)\n",
        "    and corresponding target sequences from another domain\n",
        "    (e.g. French sentences).\n",
        "- An encoder LSTM turns input sequences to 2 state vectors\n",
        "    (we keep the last LSTM state and discard the outputs).\n",
        "- A decoder LSTM is trained to turn the target sequences into\n",
        "    the same sequence but offset by one timestep in the future,\n",
        "    a training process called \"teacher forcing\" in this context.\n",
        "    It uses as initial state the state vectors from the encoder.\n",
        "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
        "    given `targets[...t]`, conditioned on the input sequence.\n",
        "- In inference mode, when we want to decode unknown input sequences, we:\n",
        "    - Encode the input sequence into state vectors\n",
        "    - Start with a target sequence of size 1\n",
        "        (just the start-of-sequence character)\n",
        "    - Feed the state vectors and 1-char target sequence\n",
        "        to the decoder to produce predictions for the next character\n",
        "    - Sample the next character using these predictions\n",
        "        (we simply use argmax).\n",
        "    - Append the sampled character to the target sequence\n",
        "    - Repeat until we generate the end-of-sequence character or we\n",
        "        hit the character limit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk8__4jEe-7L"
      },
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjnD7Gtle-7M"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9axnF-ie-7O"
      },
      "source": [
        "### Download the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik7TrAiMe-7P"
      },
      "source": [
        "!!curl -O http://www.manythings.org/anki/fra-eng.zip\n",
        "!!unzip fra-eng.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzpQ-1sre-7T"
      },
      "source": [
        "### Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14m6qbZVe-7T"
      },
      "source": [
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = \"fra.txt\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ1NzQs6e-7U"
      },
      "source": [
        "### Prepare the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VXgK_P4e-7V"
      },
      "source": [
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmT-s5TGe-7X"
      },
      "source": [
        "### Build the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1emHhx2e-7Y"
      },
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KVvUL8re-7b"
      },
      "source": [
        "### Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIXh70Mte-7c"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2,\n",
        ")\n",
        "# Save model\n",
        "model.save(\"s2s\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HgC-DTCe-7d"
      },
      "source": [
        "### Run inference (sampling)\n",
        "\n",
        "1. encode input and retrieve initial decoder state\n",
        "2. run one step of decoder with this initial state\n",
        "and a \"start of sequence\" token as target.\n",
        "Output will be the next target token.\n",
        "3. Repeat with the current target token and current states\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JSbU7pse-7f"
      },
      "source": [
        "# Define sampling models\n",
        "# Restore the model and construct the encoder and decoder.\n",
        "model = keras.models.load_model(\"s2s\")\n",
        "\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"input_3\")\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_4\")\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8K7BU-fe-7i"
      },
      "source": [
        "You can now generate decoded sentences as such:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHNiUUmPe-7j"
      },
      "source": [
        "for seq_index in range(20):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(\"-\")\n",
        "    print(\"Input sentence:\", input_texts[seq_index])\n",
        "    print(\"Decoded sentence:\", decoded_sentence)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTFsrjOgoTjB"
      },
      "source": [
        "\n",
        "##  Translation with a Sequence to Sequence Network and Attention\n",
        "\n",
        "This code is adapted from the [PyTorch official tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html), authored by [Sean Robertson](https://github.com/spro/practical-pytorch).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03fSZ2pmv02A"
      },
      "source": [
        "### Introduction\n",
        "In this project we will be teaching a neural network to translate from\n",
        "French to English.\n",
        "\n",
        "\n",
        "    [KEY: > input, = target, < output]\n",
        "\n",
        "    > il est en train de peindre un tableau .\n",
        "    = he is painting a picture .\n",
        "    < he is painting a picture .\n",
        "\n",
        "    > pourquoi ne pas essayer ce vin delicieux ?\n",
        "    = why not try that delicious wine ?\n",
        "    < why not try that delicious wine ?\n",
        "\n",
        "    > elle n est pas poete mais romanciere .\n",
        "    = she is not a poet but a novelist .\n",
        "    < she not not a poet but a novelist .\n",
        "\n",
        "    > vous etes trop maigre .\n",
        "    = you re too skinny .\n",
        "    < you re all alone .\n",
        "\n",
        "... to varying degrees of success.\n",
        "\n",
        "This is made possible by the simple but powerful idea of the [sequence\n",
        "to sequence network](https://arxiv.org/abs/1409.3215), in which two\n",
        "recurrent neural networks work together to transform one sequence to\n",
        "another. An encoder network condenses an input sequence into a vector,\n",
        "and a decoder network unfolds that vector into a new sequence.\n",
        "\n",
        "\n",
        "To improve upon this model we'll use an [attention\n",
        "mechanism](https://arxiv.org/abs/1409.0473), which lets the decoder\n",
        "learn to focus over a specific range of the input sequence.\n",
        "\n",
        "### imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIHXqUQvoTjD"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xk9k0xgoTjE"
      },
      "source": [
        "### Loading data files\n",
        "\n",
        "\n",
        "The data for this project is a set of many thousands of English to\n",
        "French translation pairs.\n",
        "\n",
        "`This question on Open Data Stack\n",
        "Exchange <https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages>`__\n",
        "pointed me to the open translation site https://tatoeba.org/ which has\n",
        "downloads available at https://tatoeba.org/eng/downloads - and better\n",
        "yet, someone did the extra work of splitting language pairs into\n",
        "individual text files here: https://www.manythings.org/anki/\n",
        "\n",
        "The English to French pairs are too big to include in the repo, so\n",
        "download to ``data/eng-fra.txt`` before continuing. The file is a tab\n",
        "separated list of translation pairs:\n",
        "\n",
        "::\n",
        "\n",
        "    I am cold.    J'ai froid.\n",
        "\n",
        ".. Note::\n",
        "   Download the data from\n",
        "   `here <https://download.pytorch.org/tutorial/data.zip>`_\n",
        "   and extract it to the current directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8VYnzaHoTjE"
      },
      "source": [
        "Similar to the character encoding used in the character-level RNN\n",
        "tutorials, we will be representing each word in a language as a one-hot\n",
        "vector, or giant vector of zeros except for a single one (at the index\n",
        "of the word). Compared to the dozens of characters that might exist in a\n",
        "language, there are many many more words, so the encoding vector is much\n",
        "larger. We will however cheat a bit and trim the data to only use a few\n",
        "thousand words per language.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/word-encoding.png\n",
        "   :alt:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUB-q_f8oTjG"
      },
      "source": [
        "We'll need a unique index per word to use as the inputs and targets of\n",
        "the networks later. To keep track of all this we will use a helper class\n",
        "called ``Lang`` which has word → index (``word2index``) and index → word\n",
        "(``index2word``) dictionaries, as well as a count of each word\n",
        "``word2count`` to use to later replace rare words.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yulUXQbioTjG"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBwz8kK8oTjH"
      },
      "source": [
        "The files are all in Unicode, to simplify we will turn Unicode\n",
        "characters to ASCII, make everything lowercase, and trim most\n",
        "punctuation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiyZRpvFoTjH"
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2iz97nYoTjI"
      },
      "source": [
        "To read the data file we will split the file into lines, and then split\n",
        "lines into pairs. The files are all English → Other Language, so if we\n",
        "want to translate from Other Language → English I added the ``reverse``\n",
        "flag to reverse the pairs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8gLtSLyoTjI"
      },
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90NyCjp7oTjJ"
      },
      "source": [
        "Since there are a *lot* of example sentences and we want to train\n",
        "something quickly, we'll trim the data set to only relatively short and\n",
        "simple sentences. Here the maximum length is 10 words (that includes\n",
        "ending punctuation) and we're filtering to sentences that translate to\n",
        "the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced\n",
        "earlier).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hC5odnfoTjJ"
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFJFSQrvoTjK"
      },
      "source": [
        "The full process for preparing the data is:\n",
        "\n",
        "-  Read text file and split into lines, split lines into pairs\n",
        "-  Normalize text, filter by length and content\n",
        "-  Make word lists from sentences in pairs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zbqOWRVoTjK"
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KElEyvZOoTjL"
      },
      "source": [
        "### The Seq2Seq Model\n",
        "\n",
        "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
        "sequence and uses its own output as input for subsequent steps.\n",
        "\n",
        "A `Sequence to Sequence network <https://arxiv.org/abs/1409.3215>`__, or\n",
        "seq2seq network, or `Encoder Decoder\n",
        "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
        "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
        "an input sequence and outputs a single vector, and the decoder reads\n",
        "that vector to produce an output sequence.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
        "   :alt:\n",
        "\n",
        "Unlike sequence prediction with a single RNN, where every input\n",
        "corresponds to an output, the seq2seq model frees us from sequence\n",
        "length and order, which makes it ideal for translation between two\n",
        "languages.\n",
        "\n",
        "Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the\n",
        "black cat\". Most of the words in the input sentence have a direct\n",
        "translation in the output sentence, but are in slightly different\n",
        "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
        "construction there is also one more word in the input sentence. It would\n",
        "be difficult to produce a correct translation directly from the sequence\n",
        "of input words.\n",
        "\n",
        "With a seq2seq model the encoder creates a single vector which, in the\n",
        "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
        "vector — a single point in some N dimensional space of sentences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRgxRkttoTjM"
      },
      "source": [
        "### The Encoder\n",
        "\n",
        "The encoder of a seq2seq network is a RNN that outputs some value for\n",
        "every word from the input sentence. For every input word the encoder\n",
        "outputs a vector and a hidden state, and uses the hidden state for the\n",
        "next input word.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/encoder-network.png\n",
        "   :alt:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcpRP9DKoTjM"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koRIOArfoTjM"
      },
      "source": [
        "### The Decoder\n",
        "\n",
        "The decoder is another RNN that takes the encoder output vector(s) and\n",
        "outputs a sequence of words to create the translation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUT9JS-poTjN"
      },
      "source": [
        "\n",
        "\n",
        "In the simplest seq2seq decoder we use only last output of the encoder.\n",
        "This last output is sometimes called the *context vector* as it encodes\n",
        "context from the entire sequence. This context vector is used as the\n",
        "initial hidden state of the decoder.\n",
        "\n",
        "At every step of decoding, the decoder is given an input token and\n",
        "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
        "token, and the first hidden state is the context vector (the encoder's\n",
        "last hidden state).\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/decoder-network.png\n",
        "   :alt:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJoM8Mn4oTjN"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLkleT6hoTjO"
      },
      "source": [
        "I encourage you to train and observe the results of this model, but to\n",
        "save space we'll be going straight for the gold and introducing the\n",
        "Attention Mechanism.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCR7DceXoTjO"
      },
      "source": [
        "Attention Decoder\n",
        "^^^^^^^^^^^^^^^^^\n",
        "\n",
        "If only the context vector is passed betweeen the encoder and decoder,\n",
        "that single vector carries the burden of encoding the entire sentence.\n",
        "\n",
        "Attention allows the decoder network to \"focus\" on a different part of\n",
        "the encoder's outputs for every step of the decoder's own outputs. First\n",
        "we calculate a set of *attention weights*. These will be multiplied by\n",
        "the encoder output vectors to create a weighted combination. The result\n",
        "(called ``attn_applied`` in the code) should contain information about\n",
        "that specific part of the input sequence, and thus help the decoder\n",
        "choose the right output words.\n",
        "\n",
        ".. figure:: https://i.imgur.com/1152PYf.png\n",
        "   :alt:\n",
        "\n",
        "Calculating the attention weights is done with another feed-forward\n",
        "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
        "Because there are sentences of all sizes in the training data, to\n",
        "actually create and train this layer we have to choose a maximum\n",
        "sentence length (input length, for encoder outputs) that it can apply\n",
        "to. Sentences of the maximum length will use all the attention weights,\n",
        "while shorter sentences will only use the first few.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/attention-decoder-network.png\n",
        "   :alt:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-tZt8K8oTjO"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrQR6uRVoTjP"
      },
      "source": [
        "\n",
        "\n",
        "### Training\n",
        "\n",
        "Preparing Training Data\n",
        "\n",
        "To train, for each pair we will need an input tensor (indexes of the\n",
        "words in the input sentence) and target tensor (indexes of the words in\n",
        "the target sentence). While creating these vectors we will append the\n",
        "EOS token to both sequences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPDyE3u6oTjP"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq6IR_LgoTjP"
      },
      "source": [
        "### Training the Model\n",
        "\n",
        "To train we run the input sentence through the encoder, and keep track\n",
        "of every output and the latest hidden state. Then the decoder is given\n",
        "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
        "encoder as its first hidden state.\n",
        "\n",
        "\"Teacher forcing\" is the concept of using the real target outputs as\n",
        "each next input, instead of using the decoder's guess as the next input.\n",
        "Using teacher forcing causes it to converge faster but `when the trained\n",
        "network is exploited, it may exhibit\n",
        "instability <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf>`__.\n",
        "\n",
        "You can observe outputs of teacher-forced networks that read with\n",
        "coherent grammar but wander far from the correct translation -\n",
        "intuitively it has learned to represent the output grammar and can \"pick\n",
        "up\" the meaning once the teacher tells it the first few words, but it\n",
        "has not properly learned how to create the sentence from the translation\n",
        "in the first place.\n",
        "\n",
        "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
        "choose to use teacher forcing or not with a simple if statement. Turn\n",
        "``teacher_forcing_ratio`` up to use more of it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X76FoDFoTjP"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhBofIiGoTjQ"
      },
      "source": [
        "This is a helper function to print time elapsed and estimated time\n",
        "remaining given the current time and progress %.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cbdMCvNoTjQ"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mklP0f8VoTjQ"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "-  Start a timer\n",
        "-  Initialize optimizers and criterion\n",
        "-  Create set of training pairs\n",
        "-  Start empty losses array for plotting\n",
        "\n",
        "Then we call ``train`` many times and occasionally print the progress (%\n",
        "of examples, time so far, estimated time) and average loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFA8uyDBoTjQ"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slMtEO_FoTjR"
      },
      "source": [
        "### Plotting results\n",
        "\n",
        "Plotting is done with matplotlib, using the array of loss values\n",
        "``plot_losses`` saved while training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-hQtgY9oTjR"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8wE-4jioTjR"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Evaluation is mostly the same as training, but there are no targets so\n",
        "we simply feed the decoder's predictions back to itself for each step.\n",
        "Every time it predicts a word we add it to the output string, and if it\n",
        "predicts the EOS token we stop there. We also store the decoder's\n",
        "attention outputs for display later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_khje6GoTjR"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV5vpawIoTjR"
      },
      "source": [
        "We can evaluate random sentences from the training set and print out the\n",
        "input, target, and output to make some subjective quality judgements:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLDbHlxmoTjS"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u_pJpTKoTjS"
      },
      "source": [
        "### Training and Evaluating\n",
        "\n",
        "With all these helper functions in place (it looks like extra work, but\n",
        "it makes it easier to run multiple experiments) we can actually\n",
        "initialize a network and start training.\n",
        "\n",
        "Remember that the input sentences were heavily filtered. For this small\n",
        "dataset we can use relatively small networks of 256 hidden nodes and a\n",
        "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
        "reasonable results.\n",
        "\n",
        ".. Note::\n",
        "   If you run this notebook you can train, interrupt the kernel,\n",
        "   evaluate, and continue training later. Comment out the lines where the\n",
        "   encoder and decoder are initialized and run ``trainIters`` again.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9HTlKHcoTjS"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUMOAFIHoTjS"
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TLHkNqzoTjS"
      },
      "source": [
        "### Visualizing Attention\n",
        "\n",
        "\n",
        "A useful property of the attention mechanism is its highly interpretable\n",
        "outputs. Because it is used to weight specific encoder outputs of the\n",
        "input sequence, we can imagine looking where the network is focused most\n",
        "at each time step.\n",
        "\n",
        "You could simply run ``plt.matshow(attentions)`` to see attention output\n",
        "displayed as a matrix, with the columns being input steps and rows being\n",
        "output steps:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGhWjlvXoTjT"
      },
      "source": [
        "output_words, attentions = evaluate(\n",
        "    encoder1, attn_decoder1, \"je suis trop froid .\")\n",
        "plt.matshow(attentions.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzJbc42eoTjT"
      },
      "source": [
        "For a better viewing experience we will do the extra work of adding axes\n",
        "and labels:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZlEiTYdoTjT"
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
        "\n",
        "evaluateAndShowAttention(\"elle est trop petit .\")\n",
        "\n",
        "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
        "\n",
        "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElOPfN-CjiK5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOXhlS5ANBDx"
      },
      "source": [
        "# Transformers\n",
        "\n",
        "Transformers are a neural architecture comprised of encoder component and a decoder component, which uses self attention to reach high levels of performance for sequence to sequence tasks (and drops all RNN components!).\n",
        "\n",
        "**We highly recommend reading this [visual blog](http://jalammar.github.io/illustrated-transformer/) on the Transformer before proceeding.**\n",
        "\n",
        "\n",
        "## Models\n",
        "\n",
        "We usually see two families of models - the Generative Pre-Trained Transformer models (GPT-n), and the BERT family of models, which featured Bi-directional Transformers as language models.\n",
        "\n",
        "### GPT and similar models\n",
        "\n",
        "These are auto-regressive language models created by [OpenAI](https://openai.com/). GPT3 has 175 billion parameters and is one of the largest, most powerful generative language model available today.\n",
        "\n",
        "**We highly recommend reading this [visual blog](http://jalammar.github.io/illustrated-gpt2/) on the GPT-2 before proceeding.**\n",
        "\n",
        "\n",
        "### BERT and similar models\n",
        "\n",
        "The BERT model, first presented by Google Research, uses bi directional transformers and two tasks - Masked Language Modelling task, and Next Sentence Prediction task, to train the model. These large transformers based models are trained on lots of data (Wikipedia + books). BERT has since spun off many models, include language specific ones (SpanBERT), domain specific models (SciBERT), and different level of sequences (CharacterBERT). \n",
        "\n",
        "**We highly recommend reading this [visual blog](http://jalammar.github.io/illustrated-bert/) on BERT and similar models before proceeding**.\n",
        "\n",
        "Since BERT, the state of the art has constatly been beat by models such as XL-Net. We recommend this [blog post](https://medium.com/@phylypo/a-survey-of-the-state-of-the-art-language-models-up-to-early-2020-aba824302c6) which describes various language models as of 2020 (right before GPT-3 burst on the scene). \n",
        "\n",
        "We also highly recommend these papers for a more critical view on such language models:\n",
        "\n",
        "[On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜](https://dl.acm.org/doi/10.1145/3442188.3445922) - Bender et al, 2021\n",
        "\n",
        "[Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data](https://www.aclweb.org/anthology/2020.acl-main.463/) - Bender and Koller, 2020\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX2nO2EaAdHf"
      },
      "source": [
        "# empty cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cJ-SkJpAdhp"
      },
      "source": [
        "## Popular Tasks using Transformers \n",
        "\n",
        "We will now use the popular Transformers package default 'pipelines' feature to explore common NLP tasks.\n",
        "\n",
        "[Transformers Documentation](https://huggingface.co/transformers/)\n",
        "\n",
        "[Transformers GitHub](https://github.com/huggingface/transformers)\n",
        "\n",
        "The following sections of code are taken from the [Summary of Tasks](https://huggingface.co/transformers/task_summary.html) page in the Transformers documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqTAhBf3-ZMi"
      },
      "source": [
        "### Sequence Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNsErheY-ZMi"
      },
      "source": [
        "Sequence classification is the task of classifying sequences according to a given number of classes. An example of\n",
        "sequence classification is the GLUE dataset, which is entirely based on that task. If you would like to fine-tune a\n",
        "model on a GLUE sequence classification task, you may leverage the [run_glue.py](https://github.com/huggingface/transformers/tree/master/examples/text-classification/run_glue.py) and\n",
        "[run_pl_glue.py](https://github.com/huggingface/transformers/tree/master/examples/text-classification/run_pl_glue.py) or\n",
        "[run_tf_glue.py](https://github.com/huggingface/transformers/tree/master/examples/text-classification/run_tf_glue.py) scripts.\n",
        "\n",
        "Here is an example of using pipelines to do sentiment analysis: identifying if a sequence is positive or negative. It\n",
        "leverages a fine-tuned model on sst2, which is a GLUE task.\n",
        "\n",
        "This returns a label (\"POSITIVE\" or \"NEGATIVE\") alongside a score, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5eXH5lA-ZMi",
        "outputId": "d0abdeb0-c986-4e08-a1f3-df27146fe8f8"
      },
      "source": [
        "from transformers import pipeline\n",
        "nlp = pipeline(\"sentiment-analysis\")\n",
        "result = nlp(\"I hate you\")[0]\n",
        "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n",
        "result = nlp(\"I love you\")[0]\n",
        "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label: NEGATIVE, with score: 0.9991\n",
              "label: POSITIVE, with score: 0.9999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dYX9haN-ZMj"
      },
      "source": [
        "Here is an example of doing a sequence classification using a model to determine if two sequences are paraphrases of\n",
        "each other. The process is the following:\n",
        "\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it\n",
        "   with the weights stored in the checkpoint.\n",
        "2. Build a sequence from the two sentences, with the correct model-specific separators token type ids and attention\n",
        "   masks (`PreTrainedTokenizer.encode` and `PreTrainedTokenizer.__call__` take\n",
        "   care of this).\n",
        "3. Pass this sequence through the model so that it is classified in one of the two available classes: 0 (not a\n",
        "   paraphrase) and 1 (is a paraphrase).\n",
        "4. Compute the softmax of the result to get probabilities over the classes.\n",
        "5. Print the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmUMMNsM-ZMk",
        "outputId": "02b1a7d3-11d0-43f7-dfbc-4da417f20036"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
        "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
        "sequence_1 = \"Apples are especially bad for your health\"\n",
        "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
        "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
        "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
        "paraphrase_classification_logits = model(**paraphrase).logits\n",
        "not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
        "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
        "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
        "# Should be paraphrase\n",
        "for i in range(len(classes)):\n",
        "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
        "# Should not be paraphrase\n",
        "for i in range(len(classes)):\n",
        "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "not paraphrase: 10%\n",
              "is paraphrase: 90%\n",
              "not paraphrase: 94%\n",
              "is paraphrase: 6%"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2oChnp1-ZMm"
      },
      "source": [
        "### Extractive Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12uRPuEq-ZMm"
      },
      "source": [
        "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
        "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune a\n",
        "model on a SQuAD task, you may leverage the [run_squad.py](https://github.com/huggingface/transformers/tree/master/examples/question-answering/run_squad.py) and\n",
        "[run_tf_squad.py](https://github.com/huggingface/transformers/tree/master/examples/question-answering/run_tf_squad.py) scripts.\n",
        "\n",
        "\n",
        "Here is an example of using pipelines to do question answering: extracting an answer from a text given a question. It\n",
        "leverages a fine-tuned model on SQuAD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ast1vLPT-ZMm"
      },
      "source": [
        "from transformers import pipeline\n",
        "nlp = pipeline(\"question-answering\")\n",
        "context = r\"\"\"\n",
        "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
        "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
        "a model on a SQuAD task, you may leverage the examples/question-answering/run_squad.py script.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5fq8iB0-ZMn"
      },
      "source": [
        "This returns an answer extracted from the text, a confidence score, alongside \"start\" and \"end\" values, which are the\n",
        "positions of the extracted answer in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMSCEtkU-ZMn",
        "outputId": "9d33658c-34b5-4afc-fd85-61723551eef4"
      },
      "source": [
        "result = nlp(question=\"What is extractive question answering?\", context=context)\n",
        "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
        "result = nlp(question=\"What is a good example of a question answering dataset?\", context=context)\n",
        "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Answer: 'the task of extracting an answer from a text given a question.', score: 0.6226, start: 34, end: 96\n",
              "Answer: 'SQuAD dataset,', score: 0.5053, start: 147, end: 161"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t3RxO8D-ZMn"
      },
      "source": [
        "Here is an example of question answering using a model and a tokenizer. The process is the following:\n",
        "\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it\n",
        "   with the weights stored in the checkpoint.\n",
        "2. Define a text and a few questions.\n",
        "3. Iterate over the questions and build a sequence from the text and the current question, with the correct\n",
        "   model-specific separators token type ids and attention masks.\n",
        "4. Pass this sequence through the model. This outputs a range of scores across the entire sequence tokens (question and\n",
        "   text), for both the start and end positions.\n",
        "5. Compute the softmax of the result to get probabilities over the tokens.\n",
        "6. Fetch the tokens from the identified start and stop values, convert those tokens to a string.\n",
        "7. Print the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbvf9egg-ZMo",
        "outputId": "90a0c3c9-fe5f-4f63-8d4b-efa360ac7575"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "text = r\"\"\"\n",
        "🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
        "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
        "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
        "TensorFlow 2.0 and PyTorch.\n",
        "\"\"\"\n",
        "questions = [\n",
        "    \"How many pretrained models are available in 🤗 Transformers?\",\n",
        "    \"What does 🤗 Transformers provide?\",\n",
        "    \"🤗 Transformers provides interoperability between which frameworks?\",\n",
        "]\n",
        "for question in questions:\n",
        "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    outputs = model(**inputs)\n",
        "    answer_start_scores = outputs.start_logits\n",
        "    answer_end_scores = outputs.end_logits\n",
        "\n",
        "    answer_start = torch.argmax(\n",
        "        answer_start_scores\n",
        "    )  # Get the most likely beginning of answer with the argmax of the score\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Question: How many pretrained models are available in 🤗 Transformers?\n",
              "Answer: over 32 +\n",
              "Question: What does 🤗 Transformers provide?\n",
              "Answer: general - purpose architectures\n",
              "Question: 🤗 Transformers provides interoperability between which frameworks?\n",
              "Answer: tensorflow 2 . 0 and pytorch"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOrLpn_G-ZMp"
      },
      "source": [
        "### Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AEzL3nu-ZMp"
      },
      "source": [
        "Language modeling is the task of fitting a model to a corpus, which can be domain specific. All popular\n",
        "transformer-based models are trained using a variant of language modeling, e.g. BERT with masked language modeling,\n",
        "GPT-2 with causal language modeling.\n",
        "\n",
        "Language modeling can be useful outside of pretraining as well, for example to shift the model distribution to be\n",
        "domain-specific: using a language model trained over a very large corpus, and then fine-tuning it to a news dataset or\n",
        "on scientific papers e.g. [LysandreJik/arxiv-nlp](https://huggingface.co/lysandre/arxiv-nlp)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zwgOcdZ-ZMp"
      },
      "source": [
        "#### Masked Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r5bR8hM-ZMp"
      },
      "source": [
        "Masked language modeling is the task of masking tokens in a sequence with a masking token, and prompting the model to\n",
        "fill that mask with an appropriate token. This allows the model to attend to both the right context (tokens on the\n",
        "right of the mask) and the left context (tokens on the left of the mask). Such a training creates a strong basis for\n",
        "downstream tasks requiring bi-directional context, such as SQuAD (question answering, see [Lewis, Lui, Goyal et al.](https://arxiv.org/abs/1910.13461), part 4.2).\n",
        "\n",
        "Here is an example of using pipelines to replace a mask from a sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBTkRCoE-ZMq"
      },
      "source": [
        "from transformers import pipeline\n",
        "nlp = pipeline(\"fill-mask\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDiURWsT-ZMq"
      },
      "source": [
        "This outputs the sequences with the mask filled, the confidence score, and the token id in the tokenizer vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayEq2LE2-ZMq",
        "outputId": "4f3ddaf8-e050-4710-f204-d8c27931720b"
      },
      "source": [
        "from pprint import pprint\n",
        "pprint(nlp(f\"HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.1792745739221573,\n",
              "  'sequence': '<s>HuggingFace is creating a tool that the community uses to '\n",
              "              'solve NLP tasks.</s>',\n",
              "  'token': 3944,\n",
              "  'token_str': 'Ġtool'},\n",
              " {'score': 0.11349421739578247,\n",
              "  'sequence': '<s>HuggingFace is creating a framework that the community uses '\n",
              "              'to solve NLP tasks.</s>',\n",
              "  'token': 7208,\n",
              "  'token_str': 'Ġframework'},\n",
              " {'score': 0.05243554711341858,\n",
              "  'sequence': '<s>HuggingFace is creating a library that the community uses to '\n",
              "              'solve NLP tasks.</s>',\n",
              "  'token': 5560,\n",
              "  'token_str': 'Ġlibrary'},\n",
              " {'score': 0.03493533283472061,\n",
              "  'sequence': '<s>HuggingFace is creating a database that the community uses '\n",
              "              'to solve NLP tasks.</s>',\n",
              "  'token': 8503,\n",
              "  'token_str': 'Ġdatabase'},\n",
              " {'score': 0.02860250137746334,\n",
              "  'sequence': '<s>HuggingFace is creating a prototype that the community uses '\n",
              "              'to solve NLP tasks.</s>',\n",
              "  'token': 17715,\n",
              "  'token_str': 'Ġprototype'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUN9TACu-ZMq"
      },
      "source": [
        "Here is an example of doing masked language modeling using a model and a tokenizer. The process is the following:\n",
        "\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a DistilBERT model and\n",
        "   loads it with the weights stored in the checkpoint.\n",
        "2. Define a sequence with a masked token, placing the `tokenizer.mask_token` instead of a word.\n",
        "3. Encode that sequence into a list of IDs and find the position of the masked token in that list.\n",
        "4. Retrieve the predictions at the index of the mask token: this tensor has the same size as the vocabulary, and the\n",
        "   values are the scores attributed to each token. The model gives higher score to tokens it deems probable in that\n",
        "   context.\n",
        "5. Retrieve the top 5 tokens using the PyTorch `topk` or TensorFlow `top_k` methods.\n",
        "6. Replace the mask token by the tokens and print the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihYScqSt-ZMq"
      },
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "import torch\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"distilbert-base-cased\")\n",
        "sequence = f\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
        "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
        "token_logits = model(input).logits\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVBAG5Al-ZMr"
      },
      "source": [
        "This prints five sequences, with the top 5 tokens predicted by the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyBSdXGp-ZMr",
        "outputId": "c2813939-393b-43d7-eee5-27681e8b617d"
      },
      "source": [
        "for token in top_5_tokens:\n",
        "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.\n",
              "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.\n",
              "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.\n",
              "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.\n",
              "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxGiKa1h-ZMr"
      },
      "source": [
        "#### Causal Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt7FighC-ZMr"
      },
      "source": [
        "Causal language modeling is the task of predicting the token following a sequence of tokens. In this situation, the\n",
        "model only attends to the left context (tokens on the left of the mask). Such a training is particularly interesting\n",
        "for generation tasks.\n",
        "\n",
        "Usually, the next token is predicted by sampling from the logits of the last hidden state the model produces from the\n",
        "input sequence.\n",
        "\n",
        "Here is an example of using the tokenizer and model and leveraging the\n",
        "`PreTrainedModel.top_k_top_p_filtering` method to sample the next token following an input sequence\n",
        "of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8mfXD4L-ZMs"
      },
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer, top_k_top_p_filtering\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n",
        "sequence = f\"Hugging Face is based in DUMBO, New York City, and \"\n",
        "input_ids = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "# get logits of last hidden state\n",
        "next_token_logits = model(input_ids).logits[:, -1, :]\n",
        "# filter\n",
        "filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n",
        "# sample\n",
        "probs = F.softmax(filtered_next_token_logits, dim=-1)\n",
        "next_token = torch.multinomial(probs, num_samples=1)\n",
        "generated = torch.cat([input_ids, next_token], dim=-1)\n",
        "resulting_string = tokenizer.decode(generated.tolist()[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpULOLx0-ZMs"
      },
      "source": [
        "This outputs a (hopefully) coherent next token following the original sequence, which in our case is the word **has**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHzm0TFJ-ZMs",
        "outputId": "328d3181-2c0c-466b-ca06-2ae15ebc382f"
      },
      "source": [
        "print(resulting_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Hugging Face is based in DUMBO, New York City, and has"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQFoV5oM-ZMu"
      },
      "source": [
        "In the next section, we show how this functionality is leveraged in `PreTrainedModel.generate` to\n",
        "generate multiple tokens up to a user-defined length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZRvqHBm-ZMu"
      },
      "source": [
        "### Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ3yetUs-ZMu"
      },
      "source": [
        "In text generation (**a.k.a** **open-ended text generation**) the goal is to create a coherent portion of text that is a\n",
        "continuation from the given context. The following example shows how **GPT-2** can be used in pipelines to generate text.\n",
        "As a default all models apply **Top-K** sampling when used in pipelines, as configured in their respective configurations\n",
        "(see [gpt-2 config](https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json) for example)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emadcB78-ZMv",
        "outputId": "f97574ef-d2c2-441d-9aec-168a3d88974b"
      },
      "source": [
        "from transformers import pipeline\n",
        "text_generator = pipeline(\"text-generation\")\n",
        "print(text_generator(\"As far as I am concerned, I will\", max_length=50, do_sample=False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'As far as I am concerned, I will be the first to admit that I am not a fan of the idea of a \"free market.\" I think that the idea of a free market is a bit of a stretch. I think that the idea'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxjVQiU3-ZMw"
      },
      "source": [
        "Here, the model generates a random text with a total maximal length of **50** tokens from context **\"As far as I am\n",
        "concerned, I will\"**. The default arguments of `PreTrainedModel.generate()` can be directly overridden in the\n",
        "pipeline, as is shown above for the argument `max_length`.\n",
        "\n",
        "Here is an example of text generation using `XLNet` and its tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHqrRI7R-ZMw"
      },
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "model = AutoModelWithLMHead.from_pretrained(\"xlnet-base-cased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "# Padding text helps XLNet with short prompts - proposed by Aman Rusia in https://github.com/rusiaaman/XLNet-gen#methodology\n",
        "PADDING_TEXT = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
        "(except for Alexei and Maria) are discovered.\n",
        "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
        "remainder of the story. 1883 Western Siberia,\n",
        "a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
        "Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
        "father initially slaps him for making such an accusation, Rasputin watches as the\n",
        "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
        "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
        "with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\"\n",
        "prompt = \"Today the weather is really nice and I am planning on \"\n",
        "inputs = tokenizer.encode(PADDING_TEXT + prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "prompt_length = len(tokenizer.decode(inputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
        "outputs = model.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60)\n",
        "generated = prompt + tokenizer.decode(outputs[0])[prompt_length:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks3Ycx_s-ZMw",
        "outputId": "deab959b-0163-4268-8e73-ccd7436cbb26"
      },
      "source": [
        "print(generated)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Today the weather is really nice and I am planning on anning on taking a nice...... of a great time!<eop>..............."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms4-QUv9-ZMx"
      },
      "source": [
        "Text generation is currently possible with **GPT-2**, **OpenAi-GPT**, **CTRL**, **XLNet**, **Transfo-XL** and **Reformer** in\n",
        "PyTorch and for most models in Tensorflow as well. As can be seen in the example above **XLNet** and **Transfo-XL** often\n",
        "need to be padded to work well. GPT-2 is usually a good choice for **open-ended text generation** because it was trained\n",
        "on millions of webpages with a causal language modeling objective.\n",
        "\n",
        "For more information on how to apply different decoding strategies for text generation, please also refer to our text\n",
        "generation blog post [here](https://huggingface.co/blog/how-to-generate)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGBuenUnS7uR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xyZikOT-ZMx"
      },
      "source": [
        "### Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3naHHJ6E-ZMx"
      },
      "source": [
        "Named Entity Recognition (NER) is the task of classifying tokens according to a class, for example, identifying a token\n",
        "as a person, an organisation or a location. An example of a named entity recognition dataset is the CoNLL-2003 dataset,\n",
        "which is entirely based on that task. If you would like to fine-tune a model on an NER task, you may leverage the\n",
        "[run_ner.py](https://github.com/huggingface/transformers/tree/master/examples/token-classification/run_ner.py)\n",
        "(PyTorch), [run_pl_ner.py](https://github.com/huggingface/transformers/tree/master/examples/token-classification/run_pl_ner.py) (leveraging\n",
        "pytorch-lightning) or the [run_tf_ner.py](https://github.com/huggingface/transformers/tree/master/examples/token-classification/run_tf_ner.py) (TensorFlow)\n",
        "scripts.\n",
        "\n",
        "Here is an example of using pipelines to do named entity recognition, specifically, trying to identify tokens as\n",
        "belonging to one of 9 classes:\n",
        "\n",
        "- O, Outside of a named entity\n",
        "- B-MIS, Beginning of a miscellaneous entity right after another miscellaneous entity\n",
        "- I-MIS, Miscellaneous entity\n",
        "- B-PER, Beginning of a person's name right after another person's name\n",
        "- I-PER, Person's name\n",
        "- B-ORG, Beginning of an organisation right after another organisation\n",
        "- I-ORG, Organisation\n",
        "- B-LOC, Beginning of a location right after another location\n",
        "- I-LOC, Location\n",
        "\n",
        "It leverages a fine-tuned model on CoNLL-2003, fine-tuned by [@stefan-it](https://github.com/stefan-it) from [dbmdz](https://github.com/dbmdz)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJciNeGO-ZMy"
      },
      "source": [
        "from transformers import pipeline\n",
        "nlp = pipeline(\"ner\")\n",
        "sequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very\"\n",
        "           \"close to the Manhattan Bridge which is visible from the window.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU0UdGTt-ZMy"
      },
      "source": [
        "This outputs a list of all words that have been identified as one of the entities from the 9 classes defined above.\n",
        "Here are the expected results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZnjywhw-ZMy",
        "outputId": "c65839e0-7bcb-4822-e234-1981fa68706b"
      },
      "source": [
        "print(nlp(sequence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\n",
              "    {'word': 'Hu', 'score': 0.9995632767677307, 'entity': 'I-ORG'},\n",
              "    {'word': '##gging', 'score': 0.9915938973426819, 'entity': 'I-ORG'},\n",
              "    {'word': 'Face', 'score': 0.9982671737670898, 'entity': 'I-ORG'},\n",
              "    {'word': 'Inc', 'score': 0.9994403719902039, 'entity': 'I-ORG'},\n",
              "    {'word': 'New', 'score': 0.9994346499443054, 'entity': 'I-LOC'},\n",
              "    {'word': 'York', 'score': 0.9993270635604858, 'entity': 'I-LOC'},\n",
              "    {'word': 'City', 'score': 0.9993864893913269, 'entity': 'I-LOC'},\n",
              "    {'word': 'D', 'score': 0.9825621843338013, 'entity': 'I-LOC'},\n",
              "    {'word': '##UM', 'score': 0.936983048915863, 'entity': 'I-LOC'},\n",
              "    {'word': '##BO', 'score': 0.8987102508544922, 'entity': 'I-LOC'},\n",
              "    {'word': 'Manhattan', 'score': 0.9758241176605225, 'entity': 'I-LOC'},\n",
              "    {'word': 'Bridge', 'score': 0.990249514579773, 'entity': 'I-LOC'}\n",
              "]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AHQ2Sqn-ZMy"
      },
      "source": [
        "Note how the tokens of the sequence \"Hugging Face\" have been identified as an organisation, and \"New York City\",\n",
        "\"DUMBO\" and \"Manhattan Bridge\" have been identified as locations.\n",
        "\n",
        "Here is an example of doing named entity recognition, using a model and a tokenizer. The process is the following:\n",
        "\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it\n",
        "   with the weights stored in the checkpoint.\n",
        "2. Define the label list with which the model was trained on.\n",
        "3. Define a sequence with known entities, such as \"Hugging Face\" as an organisation and \"New York City\" as a location.\n",
        "4. Split words into tokens so that they can be mapped to predictions. We use a small hack by, first, completely\n",
        "   encoding and decoding the sequence, so that we're left with a string that contains the special tokens.\n",
        "5. Encode that sequence into IDs (special tokens are added automatically).\n",
        "6. Retrieve the predictions by passing the input to the model and getting the first output. This results in a\n",
        "   distribution over the 9 possible classes for each token. We take the argmax to retrieve the most likely class for\n",
        "   each token.\n",
        "7. Zip together each token with its prediction and print it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7le9YPL9-ZMz"
      },
      "source": [
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "import torch\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "label_list = [\n",
        "    \"O\",       # Outside of a named entity\n",
        "    \"B-MISC\",  # Beginning of a miscellaneous entity right after another miscellaneous entity\n",
        "    \"I-MISC\",  # Miscellaneous entity\n",
        "    \"B-PER\",   # Beginning of a person's name right after another person's name\n",
        "    \"I-PER\",   # Person's name\n",
        "    \"B-ORG\",   # Beginning of an organisation right after another organisation\n",
        "    \"I-ORG\",   # Organisation\n",
        "    \"B-LOC\",   # Beginning of a location right after another location\n",
        "    \"I-LOC\"    # Location\n",
        "]\n",
        "sequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very\" \\\n",
        "           \"close to the Manhattan Bridge.\"\n",
        "# Bit of a hack to get the tokens with the special tokens\n",
        "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
        "inputs = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "outputs = model(inputs).logits\n",
        "predictions = torch.argmax(outputs, dim=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRmyFjWv-ZMz"
      },
      "source": [
        "This outputs a list of each token mapped to its corresponding prediction. Differently from the pipeline, here every\n",
        "token has a prediction as we didn't remove the \"0\"th class, which means that no particular entity was found on that\n",
        "token. The following array should be the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVGGzja3-ZMz",
        "outputId": "26ab0f2f-739b-410f-a5bb-d0ad73746878"
      },
      "source": [
        "print([(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].numpy())])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('[CLS]', 'O'), ('Hu', 'I-ORG'), ('##gging', 'I-ORG'), ('Face', 'I-ORG'), ('Inc', 'I-ORG'), ('.', 'O'), ('is', 'O'), ('a', 'O'), ('company', 'O'), ('based', 'O'), ('in', 'O'), ('New', 'I-LOC'), ('York', 'I-LOC'), ('City', 'I-LOC'), ('.', 'O'), ('Its', 'O'), ('headquarters', 'O'), ('are', 'O'), ('in', 'O'), ('D', 'I-LOC'), ('##UM', 'I-LOC'), ('##BO', 'I-LOC'), (',', 'O'), ('therefore', 'O'), ('very', 'O'), ('##c', 'O'), ('##lose', 'O'), ('to', 'O'), ('the', 'O'), ('Manhattan', 'I-LOC'), ('Bridge', 'I-LOC'), ('.', 'O'), ('[SEP]', 'O')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H5fFtGt-ZM0"
      },
      "source": [
        "### Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE9OrGAJ-ZM0"
      },
      "source": [
        "Summarization is the task of summarizing a document or an article into a shorter text.\n",
        "\n",
        "An example of a summarization dataset is the CNN / Daily Mail dataset, which consists of long news articles and was\n",
        "created for the task of summarization. If you would like to fine-tune a model on a summarization task, various\n",
        "approaches are described in this [document](https://github.com/huggingface/transformers/blob/master/examples/seq2seq/README.md).\n",
        "\n",
        "Here is an example of using the pipelines to do summarization. It leverages a Bart model that was fine-tuned on the CNN\n",
        "/ Daily Mail data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBmfYCc6-ZM0"
      },
      "source": [
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
        "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
        "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
        "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
        "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
        "2010 marriage license application, according to court documents.\n",
        "Prosecutors said the marriages were part of an immigration scam.\n",
        "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
        "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
        "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
        "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
        "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
        "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
        "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
        "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
        "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
        "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNppiVo2-ZM0"
      },
      "source": [
        "Because the summarization pipeline depends on the `PreTrainedModel.generate()` method, we can override the default\n",
        "arguments of `PreTrainedModel.generate()` directly in the pipeline for `max_length` and `min_length` as shown\n",
        "below. This outputs the following summary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6GAHEEl-ZM0",
        "outputId": "de1cbda4-ce1c-4236-8db9-d256f0623ee3"
      },
      "source": [
        "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn9S6Rmn-ZM1"
      },
      "source": [
        "Here is an example of doing summarization using a model and a tokenizer. The process is the following:\n",
        "\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. Summarization is usually done using an encoder-decoder\n",
        "   model, such as `Bart` or `T5`.\n",
        "2. Define the article that should be summarized.\n",
        "3. Add the T5 specific prefix \"summarize: \".\n",
        "4. Use the `PreTrainedModel.generate()` method to generate the summary.\n",
        "\n",
        "In this example we use Google`s T5 model. Even though it was pre-trained only on a multi-task mixed dataset (including\n",
        "CNN / Daily Mail), it yields very good results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rfuam6GI-ZM1"
      },
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "# T5 uses a max_length of 512 so we cut the article to 512 tokens.\n",
        "inputs = tokenizer.encode(\"summarize: \" + ARTICLE, return_tensors=\"pt\", max_length=512)\n",
        "outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8gF8TN1-ZM1"
      },
      "source": [
        "### Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBB0eYEv-ZM1"
      },
      "source": [
        "We have already seen Translation examples early in this notebook. We will now use a different case with BERT.\n",
        "\n",
        "An example of a translation dataset is the WMT English to German dataset, which has sentences in English as the input\n",
        "data and the corresponding sentences in German as the target data. If you would like to fine-tune a model on a\n",
        "translation task, various approaches are described in this [document](https://github.com/huggingface/transformers/blob/master/examples/seq2seq/README.md).\n",
        "\n",
        "Here is an example of using the pipelines to do translation. It leverages a T5 model that was only pre-trained on a\n",
        "multi-task mixture dataset (including WMT), yet, yielding impressive translation results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnKOrsbD-ZM1",
        "outputId": "d9b08453-c0e7-45c4-b8c1-d78f87f3c394"
      },
      "source": [
        "from transformers import pipeline\n",
        "translator = pipeline(\"translation_en_to_de\")\n",
        "print(translator(\"Hugging Face is a technology company based in New York and Paris\", max_length=40))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'translation_text': 'Hugging Face ist ein Technologieunternehmen mit Sitz in New York und Paris.'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esSGSUHh-ZM2"
      },
      "source": [
        "Because the translation pipeline depends on the `PreTrainedModel.generate()` method, we can override the default\n",
        "arguments of `PreTrainedModel.generate()` directly in the pipeline as is shown for `max_length` above.\n",
        "\n",
        "Here is an example of doing translation using a model and a tokenizer. The process is the following:\n",
        "\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. Summarization is usually done using an encoder-decoder\n",
        "   model, such as `Bart` or `T5`.\n",
        "2. Define the article that should be summarized.\n",
        "3. Add the T5 specific prefix \"translate English to German: \"\n",
        "4. Use the `PreTrainedModel.generate()` method to perform the translation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKIy-6ba-ZM2"
      },
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "inputs = tokenizer.encode(\"translate English to German: Hugging Face is a technology company based in New York and Paris\", return_tensors=\"pt\")\n",
        "outputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PMky27P-ZM2"
      },
      "source": [
        "As with the pipeline example, we get the same translation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfsUBnVb-ZM2",
        "outputId": "e86af3a1-9159-410b-8c1f-688592521a3e"
      },
      "source": [
        "print(tokenizer.decode(outputs[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Hugging Face ist ein Technologieunternehmen mit Sitz in New York und Paris."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47yVwspLAvZx"
      },
      "source": [
        "## Fine tuning models  \n",
        "\n",
        "(section in progress - need to include classification task, language generation, and language modelling - code ready, need to plug in)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0GYy34n1zmB"
      },
      "source": [
        "# BERT Word and Sentence Embeddings\n",
        "\n",
        "In this section we will explore the many ways we can use BERT to generate word and sentence embeddings.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIKFzr2s7ZtK"
      },
      "source": [
        "### Embeddings, Context Words\n",
        "\n",
        "We saw how a bootstrapped BERT model performed so much better than a model trained from scatch. Because BERT's method of capturing context is bidirectional, meaning that words can now have different word embedding values based on their location within a sentence. Let us use the same BERT model to capture sentence and word embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "sFlXkaFf7ZtK"
      },
      "source": [
        "from transformers import BertModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crmHvLsp7ZtK"
      },
      "source": [
        "Let's go through the sentence format for the BERT model, as well as how our vocabulary looks like. Note that you have to use the BERT tokenizer to use the BERT model because of the similar vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "o4goTAyK7ZtK"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnCAUot67ZtK",
        "outputId": "521aa138-4bac-43fe-ace0-581ecf2f94be"
      },
      "source": [
        "text = \"Here is the sentence I want embeddings for.\"\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Print out the tokens.\n",
        "print (tokenized_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzcdpcIK7ZtL"
      },
      "source": [
        "BERTS model uses a WordPiece technique to do its tokenizing, as described in the paper. That's why the word embedding is split up the way it is.\n",
        "A quick peek at what the voabulary looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hVi4xx57ZtL",
        "outputId": "aaaef24c-120d-46a9-a0c4-2105d633457e"
      },
      "source": [
        "list(tokenizer.vocab.keys())[6000:6030]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['peninsula',\n",
              " 'adults',\n",
              " 'novels',\n",
              " 'emerged',\n",
              " 'vienna',\n",
              " 'metro',\n",
              " 'debuted',\n",
              " 'shoes',\n",
              " 'tamil',\n",
              " 'songwriter',\n",
              " 'meets',\n",
              " 'prove',\n",
              " 'beating',\n",
              " 'instance',\n",
              " 'heaven',\n",
              " 'scared',\n",
              " 'sending',\n",
              " 'marks',\n",
              " 'artistic',\n",
              " 'passage',\n",
              " 'superior',\n",
              " '03',\n",
              " 'significantly',\n",
              " 'shopping',\n",
              " '##tive',\n",
              " 'retained',\n",
              " '##izing',\n",
              " 'malaysia',\n",
              " 'technique',\n",
              " 'cheeks']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHBVq2rk7ZtL",
        "outputId": "f8893ed6-bbe8-4d73-a31d-b5384e7ed11e"
      },
      "source": [
        "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
        "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
        "\n",
        "# Add the special tokens.\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Split the sentence into tokens.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Map the token strings to their vocabulary indeces.\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# Display the words with their indices.\n",
        "for tup in zip(tokenized_text, indexed_tokens):\n",
        "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS]           101\n",
            "after         2,044\n",
            "stealing     11,065\n",
            "money         2,769\n",
            "from          2,013\n",
            "the           1,996\n",
            "bank          2,924\n",
            "vault        11,632\n",
            ",             1,010\n",
            "the           1,996\n",
            "bank          2,924\n",
            "robber       27,307\n",
            "was           2,001\n",
            "seen          2,464\n",
            "fishing       5,645\n",
            "on            2,006\n",
            "the           1,996\n",
            "mississippi   5,900\n",
            "river         2,314\n",
            "bank          2,924\n",
            ".             1,012\n",
            "[SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uURNyYKD7ZtL"
      },
      "source": [
        "#### Segment ID\n",
        "\n",
        "BERT is trained on and expects sentence pairs, using 1s and 0s to distinguish between the two sentences. That is, for each token in “tokenized_text,” we must specify which sentence it belongs to: sentence 0 (a series of 0s) or sentence 1 (a series of 1s). For our purposes, single-sentence inputs only require a series of 1s, so we will create a vector of 1s for each token in our input sentence.\n",
        "\n",
        "If you want to process two sentences, assign each word in the first sentence plus the ‘[SEP]’ token a 0, and all tokens of the second sentence a 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWF4CpzR7ZtM",
        "outputId": "8ac2f2db-2051-4ee6-b250-00c513e7eb98"
      },
      "source": [
        "segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "print (segments_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oY0Goj27ZtN"
      },
      "source": [
        "Like we did for classification, we now convert these segments to tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8Bt1vwX7ZtN"
      },
      "source": [
        "The embedding layer is the hidden state layer, and this is what we pick up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "HkgVruQM7ZtN"
      },
      "source": [
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkYDOGeg7ZtO",
        "outputId": "2b8cac46-1700-491b-bdc1-4ec6187e5c54"
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model_embedding = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model_embedding.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "1uEUCKHD7ZtO"
      },
      "source": [
        "output = model_embedding(tokens_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "S5rDTvJR7ZtO"
      },
      "source": [
        "# output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwa_CRL47ZtO",
        "outputId": "cfb00ebb-6100-4c41-e4a4-6ea8a92168bb"
      },
      "source": [
        "len(output[0][0][0]), len(output[1][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_XYjM4p7ZtP"
      },
      "source": [
        "#### Understanding the Output\n",
        "\n",
        "This kind of forward pass returns us the last layer of the net, which we will use to make our vectors. The first object returned contains the batch number, followed by each of the tokens and their vector values. The second object contains a vector value, which I suspect is the sentence vector of the tokens. \n",
        "\n",
        "The first index is the batch size, and our batch size is 1, so we just choose the 0th index and work with that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "tXlaU_EO7ZtP"
      },
      "source": [
        "word_embeddings, sentence_embedding = output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr1Fe4IZ7ZtP",
        "outputId": "fcd79afa-d70f-4c80-8a66-f4bc1bdc06b7"
      },
      "source": [
        "len(word_embeddings[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m91OJMw17ZtP",
        "outputId": "125c5c35-59ab-4fa1-8ff8-a4a02c7a50b3"
      },
      "source": [
        "word_embeddings[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4964, -0.1831, -0.5231,  ..., -0.1902,  0.3738,  0.3964],\n",
              "        [-0.1323, -0.2762, -0.3495,  ..., -0.4567,  0.3786, -0.1096],\n",
              "        [-0.3626, -0.4002,  0.0676,  ..., -0.3207, -0.2709, -0.3004],\n",
              "        ...,\n",
              "        [ 0.2961, -0.2856, -0.0382,  ..., -0.6056, -0.5163,  0.2005],\n",
              "        [ 0.4878, -0.0909, -0.2358,  ..., -0.0017, -0.5945, -0.2431],\n",
              "        [-0.2517, -0.3519, -0.4688,  ...,  0.2500,  0.0336, -0.2627]],\n",
              "       grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J25Lkeb7ZtQ"
      },
      "source": [
        "Let’s take a quick look at the range of values for a given layer and token.\n",
        "\n",
        "You’ll find that the range is fairly similar for all layers and tokens, with the majority of values falling between [-2, 2], and a small smattering of values around -10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D6zo2IJ7ZtQ",
        "outputId": "7277d479-de6f-4551-976e-711b7b505712"
      },
      "source": [
        "vec = word_embeddings[0][0]\n",
        "vec = vec.detach().numpy()\n",
        "# Plot the values as a histogram to show their distribution.\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.hist(vec, bins=200)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t16xoZYq7ZtQ"
      },
      "source": [
        "These values are grouped by layer - we can use the permute function to make it grouped by each individual token instead. Let us look at what the later looks like:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xd8NOQB7ZtQ"
      },
      "source": [
        "#### Word Vectors\n",
        "\n",
        "So each of those tokens have embedding values - let us try and compare them with each other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "aJO51oGQ7ZtR"
      },
      "source": [
        "token_vecs = []\n",
        "# For each token in the sentence...\n",
        "for embedding in word_embeddings[0]:\n",
        "    cat_vec = embedding.detach().numpy()\n",
        "    # Use `cat_vec` to represent `token`.\n",
        "    token_vecs.append(cat_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oZR3FKN7ZtR",
        "outputId": "5a837796-fdcb-44fd-a6dc-eac113e645eb"
      },
      "source": [
        "len(token_vecs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5KWAimA7ZtR"
      },
      "source": [
        "Another method to create the vectors is to sum the last four layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhCeAYWN7ZtR"
      },
      "source": [
        "#### Sentence Vector\n",
        "\n",
        "To get a single vector for our entire sentence we have multiple application-dependent strategies - we could just average all the tokens in our sentence. We can also use this oppurtunity to see if the second vector returned is a sentence vector too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "UlahGmRm7ZtS"
      },
      "source": [
        "sentence_embedding_0 = sentence_embedding.detach().numpy()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Nyv_QXsk7ZtS"
      },
      "source": [
        "sentence_embedding_1 = np.mean(token_vecs, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikFTzpVp7ZtS",
        "outputId": "6a806494-961e-46de-c4b2-ca14e30fe76d"
      },
      "source": [
        "len(sentence_embedding_0), len(sentence_embedding_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAB1xXXf7ZtT"
      },
      "source": [
        "Remember that the power of these vectors is how they are context dependant - our sentence had multiple uses of the word bank. Let us see the index and the word of the sentence and check the context accordingly. We'll then print the simlarity values for the similar and different meanings and see how it turns out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr0_tT2g7ZtT",
        "outputId": "8b430dca-6438-48c5-a008-d5c05e830495"
      },
      "source": [
        "for i, token_str in enumerate(tokenized_text):\n",
        "    print(i, token_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [CLS]\n",
            "1 after\n",
            "2 stealing\n",
            "3 money\n",
            "4 from\n",
            "5 the\n",
            "6 bank\n",
            "7 vault\n",
            "8 ,\n",
            "9 the\n",
            "10 bank\n",
            "11 robber\n",
            "12 was\n",
            "13 seen\n",
            "14 fishing\n",
            "15 on\n",
            "16 the\n",
            "17 mississippi\n",
            "18 river\n",
            "19 bank\n",
            "20 .\n",
            "21 [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SLTbo087ZtT",
        "outputId": "bf8a2e4e-fadb-49e3-e384-e2baca0fb709"
      },
      "source": [
        "print('First 5 vector values for each instance of \"bank\".')\n",
        "print('')\n",
        "print(\"bank vault   \", str(token_vecs[6][:5]))\n",
        "print(\"bank robber  \", str(token_vecs[10][:5]))\n",
        "print(\"river bank   \", str(token_vecs[19][:5]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 5 vector values for each instance of \"bank\".\n",
            "\n",
            "bank vault    [ 0.9001056  -0.53804165 -0.16690847  0.22416186  0.6896585 ]\n",
            "bank robber   [ 0.7977126  -0.52172744 -0.1983698   0.18898535  0.59409326]\n",
            "river bank    [ 0.29608926 -0.28563383 -0.03818326  0.16736214  0.7712624 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "um9VIP-D7ZtT"
      },
      "source": [
        "from scipy.spatial.distance import cosine\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYJAau6T7ZtT",
        "outputId": "074f8808-2ffc-4a67-edd8-6311a27e3c6c"
      },
      "source": [
        "\n",
        "# Calculate the cosine similarity between the word bank \n",
        "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
        "diff_bank = 1 - cosine(token_vecs[10], token_vecs[19])\n",
        "\n",
        "# Calculate the cosine similarity between the word bank\n",
        "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
        "same_bank = 1 - cosine(token_vecs[10], token_vecs[6])\n",
        "\n",
        "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
        "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vector similarity for  *similar*  meanings:  0.95\n",
            "Vector similarity for *different* meanings:  0.70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BesrJbg7ZtU"
      },
      "source": [
        "This makes sense! Let us see if the mean value of all the tokens and what we think is the sentence vector is the same thing, by checking their cosine distance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPRi2ErU7ZtU",
        "outputId": "0ef22658-fe1c-4471-c144-abd05c4678e3"
      },
      "source": [
        "1 - cosine(sentence_embedding_0, sentence_embedding_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.008313187398016453"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7xbAiqb7ZtU"
      },
      "source": [
        "That is good - it seems it is indeed the sentence vector, so we can now write two functions which calculate the word and sentence vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ZioE7gS77ZtU"
      },
      "source": [
        "def word_vector(text, word_id, model, tokenizer):\n",
        "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    word_embeddings, sentence_embeddings = model(tokens_tensor)   \n",
        "    vector = word_embeddings[0][word_id].detach().numpy()\n",
        "    return vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "wsl46pjQ7ZtU"
      },
      "source": [
        "word_10 = word_vector(text, 6, model_embedding, tokenizer)\n",
        "word_6 = word_vector(text, 10, model_embedding, tokenizer)\n",
        "word_19 = word_vector(text, 19, model_embedding, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "YYIlUpJt7ZtV"
      },
      "source": [
        "def sentence_vector(text, model, tokenizer, method=\"average\"):\n",
        "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    word_embeddings, sentence_embeddings = model(tokens_tensor)\n",
        "    token_vecs = []\n",
        "    \n",
        "    for embedding in word_embeddings[0]:\n",
        "        cat_vec = embedding.detach().numpy()\n",
        "        token_vecs.append(cat_vec)\n",
        "        \n",
        "    if method == \"average\":\n",
        "        sentence_embedding = np.mean(token_vecs, axis=0)\n",
        "    if method == \"model\":\n",
        "        sentence_embedding = sentence_embeddings\n",
        "    # do something\n",
        "    return sentence_embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "lqAidK2r7ZtV"
      },
      "source": [
        "sen_vec_0 = sentence_vector(text, model_embedding, tokenizer)\n",
        "sen_vec_1 = sentence_vector(text, model_embedding, tokenizer, method=\"model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GsjHA-o7ZtV"
      },
      "source": [
        "#### Similarity metrics\n",
        "It is worth noting that word-level similarity comparisons are not appropriate with BERT embeddings because these embeddings are contextually dependent, meaning that the word vector changes depending on the sentence it appears in. This enables direct sensitivity to polysemy so that, e.g., your representation encodes river “bank” and not a financial institution “bank”. Nevertheless, it makes direct word-to-word similarity comparisons less valuable. For sentence embeddings, however, similarity comparison is still valid such that one can query, for example, a single sentence against a dataset of other sentences in order to find the most similar. Depending on the similarity metric used, the resulting similarity values will be less informative than the relative ranking of similarity outputs as some similarity metrics make assumptions about the vector space (equally-weighted dimensions, for example) that do not hold for our 768-dimensional vector space.\n",
        "\n",
        "### Using the Vectors\n",
        "\n",
        "Without fine-tuning, BERT features may be less useful than plain GloVe or word2vec.\n",
        "They start to be interesting when you fine-tune a classifier on top of BERT. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zpo1ZO-12KB"
      },
      "source": [
        "### Contextual Sentence Embeddings\n",
        "\n",
        "Standard BERT sentence embeddings don't perform as well as special architectures built for sentence embeddings, such as BERT siamese networks. \n",
        "\n",
        "We will use the [UKPLab package \"sentence transformers\"](https://github.com/UKPLab/sentence-transformers\n",
        "), which implement such methods.\n",
        "\n",
        "- Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (EMNLP 2019)\n",
        "- Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation (EMNLP 2020)\n",
        "- Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks (NAACL 2021)\n",
        "- The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes (arXiv 2020)\n",
        "\n",
        "Documentation for [SBERT](https://www.sbert.net/examples/applications/computing-embeddings/README.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTm6r2TRHw1C",
        "outputId": "8d128313-ce71-4704-f285-a7ad7b0294cd"
      },
      "source": [
        "!pip install sentence-transformers"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/aa/f672ce489063c4ee7a566ebac1b723c53ac0cea19d9e36599cc241d8ed56/sentence-transformers-1.0.4.tar.gz (74kB)\n",
            "\r\u001b[K     |████▍                           | 10kB 15.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 20kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 30kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 40kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 51kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 61kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 71kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-1.0.4-cp37-none-any.whl size=114307 sha256=b427cc6a7d0e8f153402a465b37a734214353f478ccdd98d3f9968fda4a04484\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/ea/89/d0d2e013d951b6d23270aa9ca4018b82632ab7cd933c331316\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-1.0.4 sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "7eb35e89b40d4160be485628ead4aebb",
            "6d1fe792c2734dd9894723b493b230eb",
            "df2e14a318d649b4b932419f30c2b762",
            "7d4a3a09605b4fed8627177ebf5f37c6",
            "f7407246c9ed4cc098b953b45c03a553",
            "c6a792a3b276421aa49ebed2d325d567",
            "ec004cc52bc548dab857bf1632d4f8a9",
            "f183bf7ec0814c0898ac0a1a2afc06db"
          ]
        },
        "id": "K432TawSH0mV",
        "outputId": "ebda2adc-202e-4f20-820d-d623e2da8c5f"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7eb35e89b40d4160be485628ead4aebb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=244715968.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtnxsyhoH1nk"
      },
      "source": [
        "\n",
        "#Our sentences we like to encode\n",
        "sentences = ['This framework generates embeddings for each input sentence',\n",
        "    'Sentences are passed as a list of string.', \n",
        "    'The quick brown fox jumps over the lazy dog.']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jspJw3pH24w"
      },
      "source": [
        "#Sentences are encoded by calling model.encode()\n",
        "embeddings = model.encode(sentences)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDkn9wcaZ1mD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe7ea75-c5bc-4cd3-d9ff-ed2824ae3863"
      },
      "source": [
        "#Print the embeddings\n",
        "for sentence, embedding in zip(sentences, embeddings):\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Embedding:\", embedding)\n",
        "    print(\"\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: This framework generates embeddings for each input sentence\n",
            "Embedding: [-2.14861646e-01  3.95723283e-01  4.69087005e-01  1.36177391e-01\n",
            "  6.91134529e-03  4.29976672e-01  1.03049493e+00 -8.20221826e-02\n",
            "  2.20890775e-01 -8.78799498e-01 -1.11538105e-01 -1.70814916e-01\n",
            " -4.95070279e-01 -3.25745046e-01 -1.68513298e-01 -7.61576772e-01\n",
            " -3.08237225e-01  1.28200486e-01  2.28901401e-01 -8.63927901e-01\n",
            "  8.01117897e-01 -5.13606787e-01 -8.88547152e-02  8.67859423e-01\n",
            "  8.12777877e-01 -2.80298680e-01  1.01604247e+00  5.12090147e-01\n",
            "  5.16793191e-01 -1.59421980e-01  8.63558426e-02 -7.04056799e-01\n",
            " -4.15288240e-01 -1.02645140e-02  1.39428526e-01 -7.40843220e-03\n",
            " -2.75418386e-02 -1.06805253e+00 -1.14779401e+00 -4.22716290e-01\n",
            "  2.93512702e-01 -3.92946452e-01  3.17559600e-01 -1.97931565e-02\n",
            " -5.53470194e-01  4.99457214e-03 -5.46877012e-02  5.82734048e-01\n",
            " -1.38893008e+00 -4.44052726e-01 -8.77214193e-01 -1.82058096e-01\n",
            "  4.69049871e-01 -6.04763150e-01 -3.85194331e-01 -1.42893810e-02\n",
            " -4.25226748e-01 -1.15834557e-01  9.40649867e-01 -1.18831202e-01\n",
            " -8.38251114e-01 -1.86480731e-01  1.80723533e-01 -5.74890316e-01\n",
            "  1.09698683e-01 -8.01009461e-02  2.35777318e-01  3.24258357e-01\n",
            " -2.56726950e-01 -1.03238904e+00 -3.13236892e-01 -7.26499617e-01\n",
            " -1.05913281e+00 -7.37700999e-01 -6.37596428e-01 -7.13506863e-02\n",
            " -9.52580348e-02  2.07689837e-01  5.54521263e-01  5.02925575e-01\n",
            " -1.71785796e+00 -2.97019452e-01 -3.23002301e-02  5.47169566e-01\n",
            "  8.03761423e-01 -4.23405439e-01  3.23710561e-01 -1.60900444e-01\n",
            " -6.40348196e-01  9.59192038e-01  4.04997677e-01 -7.39458680e-01\n",
            "  3.10167298e-02  3.12072158e-01 -1.36234090e-01 -4.47537214e-01\n",
            "  2.17739031e-01 -3.04858685e-01 -5.95458411e-02  6.49897158e-01\n",
            " -8.15244079e-01 -5.28618872e-01  8.22357476e-01  6.20349884e-01\n",
            "  1.25000134e-01  1.10236990e+00  1.66929796e-01 -8.03726315e-01\n",
            " -3.41515034e-01 -3.34514886e-01 -2.91581452e-01 -1.45557547e+00\n",
            " -4.90796387e-01  1.26138791e-01  3.22210819e-01 -2.49505848e-01\n",
            " -2.41237566e-01  4.74305339e-02 -6.96381152e-01 -2.26158112e-01\n",
            " -1.38959372e+00  1.45546007e+00  1.18826404e-01  3.26671600e-01\n",
            "  4.06463325e-01 -6.91176355e-01 -1.68644041e-01  1.18514991e+00\n",
            "  7.01420456e-02 -6.17584229e-01 -5.57674542e-02 -3.14199805e-01\n",
            " -3.26177478e-03 -7.64269292e-01  3.26422662e-01 -8.40867639e-01\n",
            " -6.80994034e-01  4.96408910e-01 -5.42567968e-01  2.35481352e-01\n",
            " -1.07771449e-01  5.03979206e-01  3.81111085e-01 -5.52189767e-01\n",
            "  1.02989089e+00 -2.70375758e-01  7.87821531e-01 -4.43495750e-01\n",
            " -2.73340702e-01 -2.84596950e-01 -7.65576005e-01  1.85731396e-01\n",
            "  6.52226061e-02 -2.59277880e-01  2.01147392e-01 -9.51705500e-02\n",
            " -2.80552596e-01 -1.91266924e-01 -2.96029150e-01 -8.89366716e-02\n",
            " -6.67773113e-02  9.02513117e-02  4.53255385e-01  2.35963330e-01\n",
            " -1.02095914e+00  1.10106325e+00 -3.85880113e-01  3.05349939e-02\n",
            " -3.15052688e-01  5.43550313e-01  7.09786952e-01  1.81038007e-01\n",
            "  1.53462380e-01 -9.47926864e-02 -1.23032248e+00 -5.97304761e-01\n",
            "  6.59966707e-01 -7.63570130e-01 -1.17271408e-01  1.92770753e-02\n",
            "  2.58255824e-02 -4.45586681e-01  1.00884482e-01 -1.32846490e-01\n",
            "  4.12283063e-01  5.06297529e-01  6.48799300e-01  2.31071159e-01\n",
            "  4.65693653e-01 -1.84467465e-01  1.74038783e-01  6.62421346e-01\n",
            " -1.05500937e-01  8.82795379e-02  9.01203215e-01  1.77087098e-01\n",
            " -9.68292952e-01  8.09800744e-01  7.22857475e-01  2.31930360e-01\n",
            "  5.56458056e-01 -1.98873013e-01 -1.70700699e-01 -2.57590562e-01\n",
            "  3.78198981e-01 -6.28216565e-02  5.00691980e-02  5.90931833e-01\n",
            " -7.46897161e-01 -2.43003935e-01  7.83916265e-02 -2.54881889e-01\n",
            " -1.68795928e-01 -8.83960187e-01 -4.51894969e-01  3.60423297e-01\n",
            "  1.04601696e-01  5.05949736e-01  6.97647393e-01 -4.89396781e-01\n",
            "  1.80793560e+00 -8.84189069e-01  4.40685749e-01 -7.89011121e-01\n",
            " -1.58614898e+00 -7.57925808e-02  4.97111827e-01  5.33538342e-01\n",
            "  1.66082788e+00  6.55650049e-02  2.62312323e-01  2.71409284e-02\n",
            " -2.17011958e-01 -4.86777835e-02 -1.83057383e-01 -7.71511942e-02\n",
            " -6.92980215e-02  4.21109349e-02 -2.45435014e-01 -4.55564708e-01\n",
            "  7.94221222e-01  7.10082591e-01 -7.66061723e-01 -6.63944781e-01\n",
            "  2.02442750e-01 -5.29783182e-02 -6.51861608e-01  4.20901000e-01\n",
            "  8.00044775e-01 -1.99938387e-01  7.74166226e-01 -1.05542827e+00\n",
            " -7.72870183e-01 -1.24074370e-01  3.96244317e-01  2.05330141e-02\n",
            " -4.59689736e-01 -4.60713536e-01 -2.73005292e-02  1.04890394e+00\n",
            " -3.72773796e-01 -6.71275139e-01  1.92536458e-01  9.12053943e-01\n",
            "  2.45730415e-01 -1.05865240e+00 -1.19349092e-01 -1.65290147e-01\n",
            " -2.20465110e-04  2.24491194e-01  9.26604390e-01  1.38912439e-01\n",
            "  3.15128177e-01  3.49052101e-02  2.80817509e-01 -5.85850030e-02\n",
            "  3.64543378e-01 -1.19164079e-01 -2.78652012e-01 -2.61051416e-01\n",
            " -8.20971847e-01 -5.31148612e-01  6.50792778e-01 -1.80739433e-01\n",
            "  9.74659383e-01 -7.19640672e-01  7.93504864e-02 -1.62516725e+00\n",
            " -2.39554681e-02 -7.93494359e-02  8.92077386e-01  7.92281866e-01\n",
            " -2.70498753e-01  4.28890586e-01  6.16606653e-01  2.35147066e-02\n",
            "  3.75258029e-01  6.23110175e-01 -5.88211179e-01 -4.53590527e-02\n",
            "  1.75673425e-01 -3.91468070e-02 -7.61111856e-01  5.07026672e-01\n",
            " -5.65473318e-01  7.81477690e-01  5.30629084e-02 -3.91099393e-01\n",
            " -1.07537258e+00  1.96339995e-01  2.39876583e-01  1.94732666e-01\n",
            "  2.11872399e-01 -2.56307781e-01  6.05002046e-02  5.69243655e-02\n",
            "  6.13818109e-01 -7.52121329e-01 -8.74090612e-01 -6.55297875e-01\n",
            " -3.64038646e-01  8.27897608e-01 -4.21353489e-01  4.42054309e-02\n",
            " -1.91348088e+00  2.02103220e-02  6.40182793e-01  6.45967722e-01\n",
            "  2.77148206e-02 -1.15632319e+00  2.42441803e-01  1.75996155e-01\n",
            "  1.25014201e-01 -1.09560132e+00 -8.04940879e-01  6.76403701e-01\n",
            " -6.30171359e-01 -2.56114066e-01 -5.53488851e-01 -9.26134944e-01\n",
            "  4.25729826e-02  2.14995220e-01  7.44043738e-02  2.15905439e-02\n",
            "  7.48094440e-01 -5.37216961e-01 -6.78459704e-01 -1.66759706e+00\n",
            "  5.32293320e-01  1.66086704e-01 -1.03965349e-01 -7.35412896e-01\n",
            " -1.00405417e-01 -1.72141828e-02  3.41768116e-01 -6.44865274e-01\n",
            " -7.36157410e-03 -2.29583848e-02  6.52531311e-02  9.39297557e-01\n",
            "  3.39741260e-01  7.10197389e-01  5.60059957e-02 -1.57409117e-01\n",
            "  3.81532520e-01 -4.59956229e-01 -7.61767507e-01  4.13135350e-01\n",
            "  8.20452034e-01 -7.27131546e-01  4.91590679e-01 -6.41027749e-01\n",
            "  2.73763062e-03  4.60016122e-03 -9.97587204e-01  1.39735907e-01\n",
            " -6.73142135e-01  8.12663138e-01  5.35585463e-01  2.01656640e-01\n",
            " -1.27372727e-01 -3.75034213e-01  5.53800464e-01 -5.78014433e-01\n",
            "  8.20998311e-01 -4.47678983e-01  6.85085177e-01 -7.58787692e-01\n",
            " -3.03430736e-01 -2.60477364e-01  4.73110601e-02  3.20054859e-01\n",
            "  1.26069021e+00 -2.70864546e-01 -3.56528938e-01 -3.50517929e-01\n",
            " -1.05006528e+00 -2.73964435e-01 -2.20990911e-01 -6.39563501e-01\n",
            " -2.05288187e-01  3.47976714e-01 -2.83570111e-01  2.52523214e-01\n",
            "  4.74942625e-01 -2.39981472e-01 -2.59468198e-01 -3.73993665e-01\n",
            "  1.11898017e+00 -5.97785152e-02 -3.17755193e-01  2.83937566e-02\n",
            "  6.12017870e-01  2.88132966e-01 -7.36221552e-01 -4.83369231e-01\n",
            "  9.62792695e-01  4.25665379e-01 -8.37563515e-01  6.20009422e-01\n",
            " -3.51198405e-01  5.40451258e-02 -5.17962098e-01  3.51877332e-01\n",
            " -7.45414376e-01 -1.91230737e-02 -6.88773215e-01 -1.65345352e-02\n",
            "  3.91551346e-01  1.26307666e-01  5.43735564e-01 -1.12217665e-03\n",
            " -1.32171929e+00 -3.79308730e-01  9.44518983e-01 -5.17166674e-01\n",
            "  3.94988656e-01 -5.72174072e-01 -7.56195068e-01 -4.94098932e-01\n",
            " -9.59618807e-01 -3.56306702e-01  3.47858220e-01  1.92040190e-01\n",
            " -7.59966850e-01  9.11783934e-01 -2.66923457e-01 -6.59035072e-02\n",
            "  1.34563696e+00  2.53031790e-01  6.06151581e-01 -6.44796908e-01\n",
            "  1.46288186e-01 -7.33342022e-02 -1.09505013e-01  4.42775220e-01\n",
            " -1.14623082e+00  2.09505945e-01 -3.31450522e-01  6.83462560e-01\n",
            " -5.90674460e-01 -5.59607387e-01 -3.34143102e-01 -4.14540395e-02\n",
            " -6.25860631e-01  1.34002373e-01  8.23193789e-01  9.74457115e-02\n",
            "  6.53405547e-01  3.33940625e-01  5.15485704e-01  3.01085055e-01\n",
            "  2.75440902e-01  4.59566474e-01 -2.64265478e-01  7.68748820e-01\n",
            " -5.22121966e-01  3.07117254e-01  3.39837521e-01 -1.72995552e-01\n",
            " -2.31123716e-01 -2.11408138e-01 -3.21941018e-01 -5.59845567e-01\n",
            "  5.35828650e-01  1.27442867e-01  6.54728860e-02  3.07592571e-01\n",
            "  5.78120887e-01 -3.99432540e-01  3.41288358e-01  3.32933217e-01\n",
            " -5.27723134e-01  1.40525103e+00  2.02652421e-02  6.89634860e-01\n",
            "  2.37005785e-01 -4.19819862e-01 -5.62872231e-01  2.67245144e-01\n",
            "  3.95165592e-01 -8.38230669e-01 -1.02827382e+00 -6.18217587e-01\n",
            "  3.33494961e-01  1.11911163e-01 -4.35440212e-01 -6.61202490e-01\n",
            " -5.05944848e-01 -2.89843351e-01 -6.94819167e-02 -3.26595724e-01\n",
            "  3.70959461e-01 -8.73412013e-01  6.57922387e-01 -3.56096685e-01\n",
            "  3.55288446e-01  9.53735411e-02  9.21535552e-01 -4.35438186e-01\n",
            " -3.31038296e-01  7.16769814e-01  3.48410100e-01  2.68658012e-01\n",
            "  2.09747717e-01  2.30887622e-01  1.24244086e-01 -2.58268476e-01\n",
            " -1.26489013e-01 -5.04594803e-01  1.12179971e+00  1.57293469e-01\n",
            " -5.76923132e-01  8.48137140e-01  5.13889492e-01  2.97580659e-01\n",
            " -5.28442621e-01 -5.61792813e-02  1.31085384e+00  3.40419650e-01\n",
            "  5.28906360e-02  1.80996016e-01 -7.56171107e-01 -3.30783464e-02\n",
            " -1.11227834e+00 -1.49555838e+00 -4.25673366e-01  5.29351771e-01\n",
            " -7.37268806e-01  7.72298947e-02 -4.31842327e-01 -3.81445616e-01\n",
            " -1.00274837e+00  9.82387587e-02 -3.35934818e-01  2.86342114e-01\n",
            " -3.95706773e-01 -3.59958142e-01 -1.28869042e-01 -1.02426484e-01\n",
            " -8.03231955e-01  7.17405379e-01  5.53769648e-01  1.42497897e+00\n",
            " -3.76650225e-03 -5.81878603e-01  1.74958557e-01 -8.15998733e-01\n",
            "  5.34404814e-01 -1.05510168e-01 -1.11391917e-01  4.20671135e-01\n",
            "  1.76866904e-01  2.44068876e-01 -4.96632636e-01  3.20558697e-01\n",
            "  3.70842904e-01 -4.57167953e-01 -2.89158285e-01 -3.65325958e-02\n",
            " -4.99849319e-01  1.52022183e-01  8.43055397e-02 -6.10918403e-01\n",
            "  2.54775614e-01  5.97720265e-01  4.87281114e-01 -6.25879094e-02\n",
            " -5.28183997e-01  6.35388047e-02  5.78197092e-02 -1.75121322e-01\n",
            "  4.67425361e-02  6.24961853e-01 -1.32456541e-01 -5.35473406e-01\n",
            "  7.86534429e-01  9.04671624e-02 -4.74243283e-01  3.34745347e-01\n",
            " -3.04574549e-01 -4.77599412e-01 -6.86327338e-01  2.25269109e-01\n",
            "  1.92438498e-01 -8.47835183e-01  9.28517282e-01  3.99917424e-01\n",
            "  5.00401616e-01  1.11668289e+00  2.81863391e-01 -6.58473253e-01\n",
            "  1.61275283e-01  5.24361849e-01  7.12566435e-01 -1.59863651e-01\n",
            " -3.81524831e-01 -5.28492257e-02  1.56984404e-01 -2.92216957e-01\n",
            " -1.45679876e-01 -3.67437214e-01 -1.71809524e-01 -3.31042647e-01\n",
            "  7.17509508e-01 -7.96705186e-01  1.74565017e-01  1.57078907e-01\n",
            " -2.65925765e-01  9.69792068e-01  2.61141509e-01 -5.69139898e-01\n",
            " -3.86578113e-01  1.67456484e+00 -8.78100634e-01 -9.50442076e-01\n",
            " -3.61159474e-01 -2.44712815e-01 -2.85401046e-01 -6.60277724e-01\n",
            "  3.31613898e-01 -5.31171322e-01  2.66039968e-01 -1.36696804e+00\n",
            " -4.40184087e-01 -1.35701835e-01 -4.98269111e-01  2.83413023e-01\n",
            "  2.46833637e-01  3.88351738e-01 -3.97594333e-01 -1.08555786e-01\n",
            "  1.63776100e-01 -5.25445938e-02 -9.34023559e-01  2.02774748e-01\n",
            "  8.68575811e-01  1.19385934e+00  1.02536082e+00  3.35175812e-01\n",
            "  1.85766682e-01  5.57820320e-01 -7.93145716e-01 -7.11507201e-01\n",
            "  5.68578005e-01  7.87335098e-01  2.44152963e-01  6.92314386e-01\n",
            " -3.56337816e-01  7.36616075e-01  3.43624473e-01  4.37910229e-01\n",
            " -5.80578268e-01  1.35115415e-01  1.68259725e-01  1.03812528e+00\n",
            "  8.60627472e-01  2.64944762e-01  5.12469932e-02  5.78465581e-01\n",
            "  3.83680426e-02  4.76748109e-01 -3.58209759e-01 -2.83857971e-01\n",
            " -5.94629347e-01 -2.94512838e-01 -2.85188019e-01 -3.81574035e-01\n",
            "  4.68358733e-02 -3.29064965e-01 -3.85556042e-01  6.10779047e-01\n",
            "  4.31492150e-01 -5.74178219e-01 -2.88198113e-01  2.54072964e-01\n",
            "  6.39703333e-01  6.01766050e-01 -5.93520105e-01  4.01052356e-01\n",
            "  3.55871975e-01 -3.15500855e-01  4.17918324e-01 -7.73398459e-01\n",
            " -7.86552310e-01  5.09269595e-01  3.42436694e-02  6.58748329e-01\n",
            "  4.10032153e-01 -5.67274988e-01  7.57191420e-01  2.20869079e-01\n",
            " -1.43134490e-01  3.98438871e-01 -7.75016397e-02  7.12158978e-02\n",
            " -3.44103165e-02  5.10026217e-01  9.36940491e-01  2.42051914e-01\n",
            " -6.09050035e-01 -4.19722140e-01  6.51433885e-01  5.78778088e-01\n",
            "  7.11937621e-02  6.12331808e-01 -1.62587106e-01  7.40011595e-03\n",
            "  4.54712868e-01  4.47467774e-01 -2.30808318e-01  1.35016724e-01\n",
            "  3.34864795e-01  4.02488559e-01  6.04867578e-01 -3.47295254e-01\n",
            " -5.64018071e-01  3.36212516e-01  8.68791163e-01  7.28187442e-01\n",
            "  1.91784233e-01  8.06428611e-01 -1.38425544e-01  1.06466508e+00\n",
            " -4.57161367e-01 -3.15823376e-01 -4.53250140e-01 -2.00414479e-01\n",
            " -4.46756065e-01 -3.75528596e-02 -3.77895802e-01 -1.24473572e-01\n",
            " -2.96884716e-01 -1.94234028e-02 -1.51052568e-02  2.08454743e-01\n",
            " -1.20482147e+00 -6.48239851e-01 -2.59301424e-01 -8.11939910e-02\n",
            "  1.30036891e-01  3.42436522e-01 -6.97177351e-01  8.72299001e-02\n",
            "  5.00145674e-01 -2.31190175e-01 -4.95791316e-01  4.23663616e-01]\n",
            "\n",
            "Sentence: Sentences are passed as a list of string.\n",
            "Embedding: [-4.40017134e-01 -2.84884959e-01  2.33638510e-01  5.53782821e-01\n",
            "  2.07572833e-01 -7.39506930e-02  1.70518860e-01 -7.60900259e-01\n",
            " -1.07309198e+00  4.83318061e-01 -3.14346105e-01  4.28550124e-01\n",
            " -5.69232106e-01 -1.82863623e-01  5.44858538e-03 -4.08723712e-01\n",
            "  4.65504766e-01 -1.20832369e-01  3.29284877e-01 -5.02809107e-01\n",
            "  1.10677707e+00  5.52272022e-01 -4.90783662e-01  3.51586848e-01\n",
            "  1.71085689e-02  4.03091162e-02  1.53281048e-01  2.30490595e-01\n",
            "  1.53231874e-01  2.53775954e-01  4.44305599e-01  2.06441507e-01\n",
            "  1.20944574e-01  2.65265465e-01 -4.61455345e-01 -1.20260194e-01\n",
            "  4.32184488e-01 -4.82548803e-01  9.69821036e-01 -1.40900815e+00\n",
            "  3.85373324e-01  5.75201094e-01 -3.25323269e-02 -3.44612718e-01\n",
            "  1.27496481e-01  1.03654720e-01 -1.52405083e-01 -5.66331744e-02\n",
            " -3.13613743e-01 -4.68685180e-02 -1.11476123e+00 -5.41421510e-02\n",
            "  6.68747604e-01 -1.05076325e+00 -1.47516817e-01 -6.93011224e-01\n",
            "  3.11113447e-01 -1.44578233e-01  8.92945886e-01 -2.32680500e-01\n",
            " -5.26881993e-01  5.13305426e-01  6.90059245e-01  8.23274851e-01\n",
            " -1.13103712e+00 -7.42039740e-01 -2.62375146e-01  5.92755497e-01\n",
            " -2.24428803e-01 -1.09858483e-01  6.15413904e-01 -2.76201665e-02\n",
            " -4.43948656e-01 -8.30336988e-01  1.09716487e+00 -4.85061258e-01\n",
            " -7.65187442e-01 -3.75072837e-01  1.52688846e-01 -2.22706258e-01\n",
            "  1.09239362e-01 -1.10419899e-01  1.44566715e-01 -2.04712927e-01\n",
            " -2.01697960e-01 -2.15952676e-02  1.29964009e-01  8.80947709e-03\n",
            " -2.19200745e-01  1.06773722e+00  8.35934952e-02  2.70221114e-01\n",
            " -7.81135380e-01 -4.44701701e-01 -3.96696955e-01 -2.74475366e-01\n",
            " -1.30059227e-01 -4.44923431e-01 -1.22892067e-01  1.38204545e-01\n",
            " -4.23903197e-01 -5.01870096e-01  3.26208472e-02 -1.42148352e+00\n",
            " -6.77387595e-01  2.28037775e-01 -9.74988997e-01  5.36884964e-01\n",
            " -1.80362239e-01 -3.05750877e-01 -7.88950145e-01 -6.02815151e-01\n",
            " -1.53121755e-01  7.60674059e-01  1.31929293e-01 -9.15828466e-01\n",
            "  5.80597371e-02 -1.23834461e-01  9.81520116e-02 -5.77724397e-01\n",
            " -4.74469304e-01  5.84090710e-01  9.41553831e-01 -8.12918425e-01\n",
            " -2.31585637e-01  1.41616508e-01 -1.23332836e-01  6.40414894e-01\n",
            "  7.26718366e-01  4.46607023e-01  1.28211752e-01  1.29415810e-01\n",
            " -3.12790513e-01 -4.45507884e-01 -5.85905731e-01  4.47842211e-01\n",
            "  2.03334272e-01 -1.54535964e-01 -5.85365854e-02 -5.34547210e-01\n",
            "  1.19479135e-01  6.37012839e-01  1.06430364e+00  1.04601808e-01\n",
            "  4.45207715e-01  3.43263209e-01 -7.05613911e-01  1.65450901e-01\n",
            " -7.81165719e-01  5.23608178e-02 -5.09805024e-01  2.24250883e-01\n",
            " -7.88429439e-01 -2.93876510e-02  2.53932625e-01 -1.63439542e-01\n",
            "  7.72962987e-01 -2.30505332e-01  3.60254854e-01  6.12941682e-01\n",
            " -1.45346895e-01 -1.05369322e-01  4.20218199e-01 -9.49986696e-01\n",
            " -8.99415314e-02  3.19952875e-01 -5.22348762e-01 -5.70940018e-01\n",
            " -8.09693515e-01  3.45272124e-01  3.25269699e-01  6.22943461e-01\n",
            " -5.64311482e-02 -6.00425303e-01 -3.25385481e-02 -2.93749660e-01\n",
            " -2.00259462e-01 -6.06869817e-01 -2.20919073e-01 -1.58137009e-01\n",
            "  2.54182350e-02  1.29874274e-01  1.78833097e-01  6.87419534e-01\n",
            " -4.12932009e-01 -3.67394350e-02  2.13502452e-01  3.87533098e-01\n",
            " -2.21345901e-01  3.81755494e-02 -6.22368790e-02  6.01429105e-01\n",
            " -3.17371815e-01  1.29106149e-01  6.95839405e-01  9.02815044e-01\n",
            "  4.00428772e-01  5.23002863e-01 -2.61606723e-01 -5.10836542e-01\n",
            "  3.01280290e-01  3.19185466e-01 -2.90963575e-02 -3.16983342e-01\n",
            " -1.19804256e-01  6.11035287e-01  6.00695074e-01  3.95938993e-01\n",
            " -1.16418588e+00  2.70280123e-01  1.06449950e+00  2.87109017e-01\n",
            " -5.84386110e-01  2.37016246e-01  1.75099328e-01  9.25505102e-01\n",
            " -3.81286480e-02 -1.45975396e-01  8.03709984e-01  1.45437988e-02\n",
            "  5.01940310e-01 -4.71675277e-01 -1.64046269e-02 -9.18503761e-01\n",
            " -5.23944616e-01  5.26829600e-01  8.52761090e-01  3.28267843e-01\n",
            " -3.23196054e-02  1.95678860e-01 -6.08473539e-01 -4.14968044e-01\n",
            " -6.93102837e-01  1.19079733e+00  5.27310632e-02  6.11897051e-01\n",
            " -2.47889385e-01  1.12811022e-01 -2.61843145e-01 -1.05515873e+00\n",
            "  8.22751224e-01  3.12641114e-01 -6.21825576e-01  1.46477759e-01\n",
            " -5.95024824e-01 -5.64856976e-02  1.87146187e-01  9.11254785e-04\n",
            " -5.26550949e-01 -4.85001832e-01 -3.96768361e-01 -2.37110198e-01\n",
            "  2.00679690e-01  8.32857668e-01  7.89025903e-01  5.91370314e-02\n",
            " -1.11143184e+00  2.52465129e-01  5.10859728e-01  1.80581346e-01\n",
            "  5.75816095e-01 -3.58632535e-01  5.28063238e-01 -3.23501408e-01\n",
            "  2.35279471e-01 -6.04159951e-01 -3.95276546e-01 -5.20812869e-01\n",
            "  7.00173736e-01  2.05440134e-01 -7.43569732e-01 -7.59639978e-01\n",
            "  1.06317592e+00 -1.63733363e-02 -1.24163938e+00 -7.58125558e-02\n",
            "  6.22728765e-01  4.82201308e-01 -5.49638808e-01 -6.29955292e-01\n",
            " -4.37203228e-01  6.84665143e-02  5.79669893e-01 -5.30378699e-01\n",
            " -4.70879935e-02  3.67447019e-01  6.52118921e-01  1.25725809e-02\n",
            " -4.72899437e-01 -1.53780282e-01  4.50964957e-01  5.57306349e-01\n",
            "  3.47304940e-01  4.73301202e-01 -1.53923884e-01  1.01995468e+00\n",
            "  2.46373594e-01  1.25905621e+00  4.75635618e-01 -4.48532969e-01\n",
            " -4.30897921e-02  2.29951560e-01 -1.08892620e-02  2.60592271e-02\n",
            " -5.77153146e-01  3.38368028e-01 -1.38464011e-02 -5.44774951e-03\n",
            " -2.65946293e+00  2.76121926e-02 -4.00862724e-01 -1.34992182e-01\n",
            "  8.10801461e-02 -3.52399386e-02  3.14213634e-01  1.40533507e-01\n",
            "  7.71944746e-02  6.06404006e-01  3.01883012e-01  7.98982084e-02\n",
            " -9.19120431e-01  1.01726741e-01 -3.51239949e-01  4.10551816e-01\n",
            " -1.38207054e+00  1.11113286e+00  1.52424246e-01 -1.87042095e-02\n",
            "  2.23530997e-02  5.53124607e-01  7.45245695e-01 -5.71291387e-01\n",
            " -8.08896840e-01  2.17040583e-01 -3.37358326e-01  5.08549750e-01\n",
            " -6.67074203e-01  3.08221459e-01 -5.63719928e-01 -3.75239253e-01\n",
            "  1.11730397e+00 -2.76233315e-01 -5.18304259e-02  4.52806056e-03\n",
            "  4.95214343e-01  2.17807556e-05 -3.72285634e-01  6.67186797e-01\n",
            " -1.35214061e-01  2.89043307e-01 -7.78487563e-01 -2.73780942e-01\n",
            " -3.00374508e-01  2.12187335e-01  6.47209227e-01 -3.80581580e-02\n",
            "  6.55279607e-02 -2.62586474e-01 -5.48949957e-01 -1.33400306e-01\n",
            "  4.76610482e-01  5.99132665e-02  5.79678953e-01 -2.79747456e-01\n",
            " -9.20380414e-01  6.27771437e-01 -3.39029104e-01  3.54434758e-01\n",
            "  2.07702816e-01 -4.15103436e-01  5.75526595e-01  2.76495188e-01\n",
            "  1.02350287e-01 -3.88684422e-02 -1.23644449e-01  1.25053751e+00\n",
            "  6.55772567e-01  4.92051095e-01  4.96730357e-01 -1.64913267e-01\n",
            "  4.01901215e-01 -9.01940882e-01  7.51318514e-01 -9.72825468e-01\n",
            " -1.58395991e-01 -1.25368989e+00  5.61255276e-01 -3.63966554e-01\n",
            " -1.13970779e-01 -4.49824721e-01 -2.76768893e-01  5.17000794e-01\n",
            "  7.38127708e-01 -2.63726771e-01 -3.83594126e-01 -7.10320175e-01\n",
            " -5.58564484e-01 -9.51528549e-01 -3.52462947e-01 -1.38925576e+00\n",
            " -2.85450757e-01  3.91953766e-01  1.58312346e-03  4.38420415e-01\n",
            " -2.86319368e-02 -3.84162143e-02 -4.76916917e-02  2.28311881e-01\n",
            "  8.75046194e-01  2.55445808e-01 -2.66780645e-01 -2.03528628e-01\n",
            " -5.29955745e-01  6.28330350e-01 -9.16983366e-01  7.18333721e-01\n",
            "  2.54868299e-01 -4.76503581e-01 -2.66316891e-01  2.33554348e-01\n",
            " -1.95304170e-01 -4.79752243e-01 -1.10132955e-01 -1.96765482e-01\n",
            " -6.45912588e-01 -8.03692460e-01 -3.73636693e-01 -4.09304529e-01\n",
            "  2.07311027e-02  5.14454901e-01  3.10879588e-01 -5.58766901e-01\n",
            " -1.47257507e+00 -6.63475320e-02  7.99677614e-03 -5.36755800e-01\n",
            "  5.60536802e-01 -8.53897095e-01 -6.34944379e-01 -1.04687937e-01\n",
            " -4.35424507e-01 -2.88165122e-01 -7.39032030e-01  4.68728513e-01\n",
            " -6.47536516e-01  9.58027244e-01  6.89822912e-01 -6.67972505e-01\n",
            " -4.08220947e-01 -3.64771962e-01 -1.69010326e-01  2.59147257e-01\n",
            "  1.72882318e-01  8.99743065e-02  4.81658161e-01 -3.78803387e-02\n",
            " -1.04160833e+00  9.64829922e-02  1.77643359e-01 -6.16742522e-02\n",
            "  3.39036249e-02 -6.96582675e-01 -3.28319609e-01 -9.47255731e-01\n",
            " -2.02866923e-02  4.28760707e-01  3.50636274e-01  4.81449589e-02\n",
            "  4.10232514e-01 -1.97211832e-01 -4.17469293e-01 -3.62728268e-01\n",
            "  5.68773031e-01 -2.78062463e-01 -7.12668002e-01  2.97149688e-01\n",
            " -4.24919039e-01 -5.85203648e-01 -5.55694044e-01 -4.89563972e-01\n",
            "  1.29525006e-01  1.49548367e-01  3.12134624e-01 -3.95024329e-01\n",
            "  4.41022128e-01 -3.99735838e-01 -3.95860851e-01  6.97120965e-01\n",
            "  9.79854837e-02  7.38335624e-02 -4.34458911e-01  4.98539470e-02\n",
            "  5.04526675e-01  7.74308383e-01  3.71799290e-01  6.87007308e-01\n",
            " -5.73139548e-01  2.83806235e-01  4.48777497e-01 -5.81046581e-01\n",
            "  4.79530156e-01  1.79407209e-01 -1.07184120e-01 -3.20084959e-01\n",
            " -1.21254943e-01 -5.49037695e-01 -6.64649978e-02 -2.82175094e-01\n",
            "  8.01550329e-01  1.96934894e-01 -2.91563392e-01  6.00290783e-02\n",
            "  5.03851324e-02  4.99179542e-01 -8.64054859e-02 -7.93622077e-01\n",
            "  7.36667275e-01 -4.78626937e-01  8.52814853e-01 -9.48741853e-01\n",
            "  4.24509555e-01  7.11933851e-01  8.64870429e-01 -4.20697689e-01\n",
            " -1.55480161e-01 -1.56967103e-01  3.06147058e-02 -3.59066933e-01\n",
            "  2.96715349e-01 -2.81565875e-01 -9.81616322e-03 -3.35093468e-01\n",
            " -2.37925947e-01  4.69260514e-01 -6.93621576e-01 -5.24261832e-01\n",
            "  8.86022508e-01  2.16426238e-01  4.99443889e-01 -2.08746627e-01\n",
            "  4.18607682e-01  5.50143301e-01 -9.89394009e-01  1.06479771e-01\n",
            " -5.15580118e-01 -2.13083357e-01 -1.86489150e-01 -2.40663990e-01\n",
            " -4.22476262e-01  8.04680645e-01 -6.70400083e-01 -3.26337308e-01\n",
            " -1.09264994e+00  2.38120064e-01 -4.56477612e-01  1.52825966e-01\n",
            "  3.02347153e-01 -3.37798566e-01  4.88633901e-01  9.69139695e-01\n",
            "  2.99832597e-02  8.53575647e-01  8.65576714e-02  1.25522292e+00\n",
            "  3.46680701e-01 -7.20542610e-01  4.79428679e-01 -1.17434472e-01\n",
            "  7.89934218e-01  1.12460911e+00  4.80012536e-01  1.66808084e-01\n",
            "  1.54325441e-01  9.80944455e-01 -5.36561549e-01 -2.14820534e-01\n",
            "  8.49563658e-01  5.32826960e-01 -1.16748917e+00 -9.40788537e-02\n",
            "  2.89867491e-01  5.91095507e-01  1.54401407e-01  7.35262334e-02\n",
            " -7.74748981e-01  6.82723284e-01 -5.57256639e-01  5.54339945e-01\n",
            " -1.24259286e-01 -9.88852680e-02 -9.50529277e-01  1.53206646e-01\n",
            " -1.51521057e-01  8.96038860e-02 -8.46993476e-02  2.15617102e-03\n",
            "  3.89236957e-01  6.83528483e-01 -3.73609066e-01 -4.47204858e-02\n",
            " -1.12119623e-01 -3.30056190e-01  2.99813956e-01 -5.72184622e-01\n",
            " -8.44940782e-01 -2.58192360e-01  3.99944305e-01  4.71565902e-01\n",
            " -2.43610442e-02 -2.35340875e-02  1.01297326e-01 -6.19897783e-01\n",
            "  2.07537934e-02 -1.75916806e-01  2.80878991e-01 -3.69535238e-01\n",
            "  2.36966312e-01  6.32968724e-01  6.09983265e-01  6.25208318e-01\n",
            " -1.48856668e-02 -4.44871932e-01 -2.82222871e-02 -4.93530929e-01\n",
            "  2.50871360e-01  3.98867786e-01 -2.96253383e-01  1.07541215e+00\n",
            " -3.21016423e-02  5.20453192e-02 -4.81425375e-02 -1.41968980e-01\n",
            " -1.00596035e+00  1.00166023e+00  8.82846788e-02 -8.92296374e-01\n",
            " -2.20024198e-01  3.80993158e-01  2.68496275e-01  7.80829862e-02\n",
            " -2.40454655e-02 -6.09100819e-01  2.18366131e-01 -4.32378203e-01\n",
            "  7.49807537e-01  1.37371361e-01  4.17296410e-01 -8.30028132e-02\n",
            "  6.15673363e-01 -2.36463562e-01 -3.78117830e-01  8.06368515e-03\n",
            " -1.44690681e+00 -1.15392447e+00 -2.93127418e-01  2.42077932e-01\n",
            "  2.46310458e-01  1.91406265e-01  6.10854864e-01  3.83835018e-01\n",
            "  9.08993110e-02  2.50380412e-02 -2.23882332e-01 -1.35085136e-01\n",
            "  8.22251081e-01 -1.92420393e-01 -6.12409592e-01 -4.38487232e-01\n",
            "  5.43268621e-01  3.37916374e-01 -1.58891967e-03  7.65429437e-01\n",
            "  6.36404276e-01 -2.08643213e-01  3.68577302e-01  6.22268677e-01\n",
            "  5.60125969e-02  5.40592611e-01 -5.13922215e-01  3.59055027e-02\n",
            "  2.02022895e-01  8.27891171e-01 -1.99202910e-01  4.84478861e-01\n",
            " -3.62360895e-01  6.35802075e-02  3.05197667e-02  5.13026893e-01\n",
            "  2.42138803e-01 -3.06489557e-01  3.38959724e-01 -4.26248550e-01\n",
            " -2.11788844e-02  3.37312579e-01 -1.09215811e-01  2.59222239e-01\n",
            "  1.25092879e-01 -2.24619433e-01 -6.56369627e-01  3.16836208e-01\n",
            " -9.18568894e-02 -3.39484178e-02 -1.08259805e-01 -8.43419611e-01\n",
            "  2.23729074e-01  1.40167892e+00 -4.03364182e-01  9.60666299e-01\n",
            " -6.62590504e-01 -3.20948921e-02  1.42535999e-01  3.07920635e-01\n",
            " -6.18174493e-01  4.04296577e-01 -3.63852859e-01  1.02347143e-01\n",
            " -1.20922096e-01  3.18051010e-01  3.55131119e-01 -1.92813724e-01\n",
            "  1.60111457e-01 -3.62052530e-01  3.80323529e-01 -8.99687707e-02\n",
            " -3.28807458e-02 -2.91947275e-01 -1.02342002e-01 -2.15260778e-02\n",
            " -2.79609531e-01  4.03045475e-01 -3.45460624e-01 -3.12103152e-01\n",
            "  2.13273838e-02  5.64227641e-01 -8.76291513e-01  3.36275429e-01\n",
            " -6.11466408e-01 -4.88990575e-01  5.14010608e-01 -6.86256766e-01\n",
            " -3.56986642e-01 -1.30945534e-01 -3.03228408e-01  3.82269591e-01\n",
            " -2.45480791e-01 -2.16496468e-01 -3.60788703e-01  2.10056841e-01\n",
            " -4.08748031e-01  3.13907057e-01  3.43504459e-01 -4.26855832e-01\n",
            " -5.97641170e-01 -3.53916436e-01 -3.25845107e-02  1.03349045e-01\n",
            " -5.79411209e-01 -8.75781238e-01 -8.31382930e-01 -7.05234468e-01\n",
            "  4.08002764e-01  8.36048186e-01  5.49910009e-01 -7.75864273e-02\n",
            "  1.11258292e+00  1.19560957e-01 -1.65302753e-01 -8.62516239e-02]\n",
            "\n",
            "Sentence: The quick brown fox jumps over the lazy dog.\n",
            "Embedding: [-0.29504794 -0.24928914 -0.02407108 -0.07039132 -0.29811585  0.7069692\n",
            " -0.74239236 -0.78407073  0.34614015  0.16320167  0.44130778  0.1785438\n",
            "  0.7044552  -0.32664093 -0.06338923 -1.50885     0.9770508  -0.4435397\n",
            " -0.06490178 -0.5104955  -0.6221504  -0.23174827 -0.70270747  0.11203331\n",
            " -1.0565065   0.3978076  -0.32861668  0.60808456 -0.24572016 -0.234693\n",
            "  0.1076289  -0.37076628 -0.10497571  0.22151278 -0.30099067  0.53019285\n",
            "  0.18546484 -0.4248958   0.16138566 -0.11732457 -0.01459106  0.16344242\n",
            "  0.67555827 -0.10893945 -0.5598627  -0.13556401 -0.3252569   0.15245835\n",
            "  0.83148605 -0.21577953  0.26040456 -0.3226602  -0.3134236  -0.35402384\n",
            " -0.31731495  0.10011521  0.20965423 -0.15452269  0.27400306  0.8664179\n",
            "  0.27700448 -0.9204981   0.67404866  0.3638459  -1.4256974  -0.10490566\n",
            " -0.6037206   0.90621513  0.26761165  0.23261839  0.17301382 -0.98498225\n",
            "  0.50503856  0.8658075  -0.55125815 -0.07355981 -0.29351503  0.5462753\n",
            " -0.28894007  0.08116314  0.4572246   0.24111028  0.01339274  0.01332026\n",
            " -0.34126627  0.73993444  0.13049276 -0.59660596 -1.1802906  -0.55255073\n",
            " -0.43480393 -0.60113895 -0.17777957  0.29826593  0.22368687 -0.01720447\n",
            " -0.1382127  -0.7140136   0.39468732 -0.4206185  -0.23356299  1.1361217\n",
            " -0.40446147  0.25767264 -0.20678087 -0.5273859  -0.59089166 -1.0604788\n",
            " -0.55018944 -0.537878    0.27958152  0.37908366  0.3938615   0.05499643\n",
            " -1.1983274  -0.6382739   0.27745977  0.4288037  -0.02487334  0.16069166\n",
            " -0.12971611 -0.7353137   0.18810469  0.21423347 -0.38056254 -0.9690685\n",
            " -0.46432337 -0.17678575  1.2227833   0.26847893  0.7708507   1.1772894\n",
            " -0.32336026 -1.0285031   1.3187659   0.9046375   0.04127354 -0.23759575\n",
            " -0.43307742 -0.09457078 -0.35488424 -0.3783747  -0.7005672  -0.9525502\n",
            " -0.41183898  0.07510395 -0.24765342  0.12819643 -0.39978203 -0.03361531\n",
            " -0.5250222   0.41771388  0.24121891 -0.21282206 -0.09242686 -0.06434336\n",
            " -0.24293339 -0.392988    0.18717778 -0.24792051  0.5082302   0.08582565\n",
            " -0.06178427 -0.7278519  -0.1931402  -0.37551156  0.56459767 -0.45926365\n",
            " -1.2088906   0.5397176  -0.09189409  0.18234986  0.17915224  0.10827307\n",
            "  0.24968588  0.42642164  0.6591812  -1.4397154   0.22103089 -0.10491574\n",
            " -0.7281296  -0.7793967   0.5351962  -0.19566214 -0.28173038 -0.16930328\n",
            "  0.28938544  0.23645915  0.32397768  0.15126728  0.32550132 -0.48286235\n",
            " -0.8546327  -0.709657    0.3462186  -0.3556379  -0.7438788  -0.5553653\n",
            "  0.9786248  -0.05753456 -0.89071965 -1.398013    0.00865554 -0.05987268\n",
            "  0.21584575 -0.22459495 -0.3186688   0.31709683  0.22313517 -0.618149\n",
            "  0.8401666  -0.11141708  0.1863649   0.5244079  -0.9283433  -0.10998376\n",
            " -0.5072048  -0.39576408  0.02102489  0.31876433 -0.5823613   0.8012175\n",
            " -0.13942759  0.791409    0.665021    0.5655318   0.45561084 -1.7672113\n",
            "  0.42538503 -0.6448706  -0.04998599 -0.10018301 -0.15981983  0.8828122\n",
            "  0.08231169 -0.28332368 -0.10496634  0.1133201   0.8480248   0.3034649\n",
            " -0.07900678 -0.35272196  0.68118626 -0.3378141   0.23424971 -0.51116365\n",
            "  0.29190975  0.2206161  -0.70146185 -0.9339573  -0.38025832  0.06421011\n",
            "  0.8802175  -0.81505394  0.89409065 -0.27956793 -0.52322775  0.0081519\n",
            "  0.10105822 -0.22178368  0.17672293 -0.44436446  0.9574506  -0.57532823\n",
            "  0.05374284  0.5480588  -0.56619114 -0.06628729 -0.2448933  -0.28566265\n",
            "  0.19311757 -0.48329702 -0.03926613 -0.25660953  0.26937523 -0.11622784\n",
            "  0.31608412  0.339368    0.01813727 -0.80236316 -0.0470774   0.7807606\n",
            " -0.15943934 -0.93117905 -0.52061087  0.10505978  0.23645882 -0.84167606\n",
            " -0.42988417  0.26685023  0.09688685  0.0844897   0.22840917  0.1748086\n",
            " -0.05814681  0.46619484  0.39566183 -0.36072192  0.6241333   0.4679517\n",
            " -0.33907905 -1.0587549  -0.11634321 -0.8130789   0.10006281 -0.21486326\n",
            "  0.14546843  0.28032705 -1.184585   -0.5417935  -0.0323888  -0.8145897\n",
            " -0.4547323  -0.12876976 -0.47351912  0.26073316 -0.5298447   0.08050846\n",
            " -0.7219734  -1.9170643   1.0178503   0.02739568 -0.19760406 -0.55603015\n",
            "  0.31861427 -0.18285213 -0.8437875   0.4449898  -0.07462999  1.479908\n",
            " -0.305886    0.8552275   0.50381446 -0.97984505 -0.52269405  0.19828618\n",
            "  0.1801138  -0.04705004 -0.7062983   0.7738586   0.74643415 -0.12369321\n",
            "  1.334396    0.07319343  0.5295145   0.17404312  0.13214695  0.2014035\n",
            "  0.9603085  -0.15788525 -0.35888422 -0.44871035  0.56051964 -0.16564229\n",
            " -0.05350338 -0.34440327  0.55484086 -0.15947552  0.13032727  0.49269402\n",
            " -0.759737    0.6087388   0.09435388 -0.5345014   0.082265    0.3357614\n",
            "  0.00287668 -0.3563355   0.06594834  0.6304723   0.1522599   0.07160143\n",
            "  0.53866816  0.8423209  -0.0387472   0.07968555  0.61756855  0.13109915\n",
            "  0.2821713  -0.42084506 -0.3746804  -0.73284316  0.51400304 -0.52475995\n",
            " -0.40360883 -0.3528385  -0.7362979   0.06346614  0.6006439   0.28381506\n",
            "  0.26397774 -0.8403681  -0.4249082  -0.37517855 -0.69716907  0.17636304\n",
            "  0.7278238  -0.48440775  0.72277313 -0.8333511  -1.2145156   0.00566106\n",
            " -0.29923052  0.9932377  -0.44600365 -0.7705367  -0.16503829  0.5795059\n",
            "  0.750134    0.15441935 -0.5403457   0.36663023  0.3597289   0.13203011\n",
            " -0.06501791 -0.7562084   0.05972052 -0.05343279 -0.8717952  -0.6507389\n",
            "  0.20121448  0.26946452  0.46197262 -0.03474287 -0.27133352 -0.85557276\n",
            " -1.0319226   1.542151    0.03179884 -0.01259902  1.0165514   0.23240435\n",
            "  0.61442715  0.24847853 -0.5638409   1.0902945   1.2404324  -0.3361013\n",
            " -0.15447398  0.3174623   0.1662096   0.66436034  0.32516453  0.7571814\n",
            "  0.43693042  0.02617342  0.58352786 -0.4497247  -0.42067036  1.2717215\n",
            "  0.619818    0.09513715  0.99953675  0.16964221  0.57333946  0.03280015\n",
            " -0.52312857  0.3145954   0.31922293 -0.15563308 -0.19858462 -0.4016218\n",
            " -0.65326774 -0.02513359  0.19594876  0.22884654 -0.1877774   0.43887985\n",
            "  0.07788306 -0.02008974 -0.49797073  0.4834849   0.12745221 -0.6160864\n",
            " -0.0812486  -0.5065673  -0.62386423  0.28166875 -0.27924109 -0.6982607\n",
            " -0.19035207 -0.03756554  0.36926594 -0.5209532   0.40065587  0.42559624\n",
            " -1.0909194   0.7374079  -0.5786045  -0.1697333  -0.50460714  0.5367044\n",
            "  0.92325115 -0.11388322  0.770533   -0.4756622   0.00394904  0.62468356\n",
            "  0.44725356  0.6697478   0.5900599   1.4067311   0.01771343 -0.23772721\n",
            "  0.533703    0.7343835   0.3404131  -0.3704796  -0.3141146   0.43266782\n",
            " -0.21224649 -0.27238455  0.91076684 -0.40004    -0.33146867 -0.07111714\n",
            "  1.0325636   0.13733935  0.8095737  -0.40879104 -0.84060866 -0.04757345\n",
            "  0.26968074  0.66213566  0.06746114  0.78130364 -0.40080228 -0.0878948\n",
            "  0.27243423  0.13973977 -0.9014991  -0.787437   -0.5380019  -0.5783102\n",
            " -0.7879668   0.37334776  0.29370365 -0.55647177  0.5750766  -0.473961\n",
            " -0.3078017  -0.16893768 -0.18534215 -0.34943458 -0.3891635  -0.08740187\n",
            "  0.62453336 -0.27022037  0.76970786  1.3697737   0.64353424 -0.16518863\n",
            "  0.1250806  -0.01604673 -0.14783573  0.12581465 -0.36538598  0.85302764\n",
            " -0.03830759  0.83079034  0.6103301   0.23708336 -0.16622484  0.08115871\n",
            "  0.594327    0.11284275 -0.12878503 -0.04780911 -0.14178552 -0.37859246\n",
            " -1.5032073   0.11368108  0.24221407 -0.01273945 -0.8036621  -0.61911815\n",
            "  0.02401471 -0.93213844 -0.74975425  0.1027583  -0.3331437  -0.1955527\n",
            "  0.42992005  0.80477905  0.12432173  0.40388742 -0.44613945 -0.40964472\n",
            "  0.22137718  0.302305    0.22739486  0.47560206 -0.5811796  -0.895423\n",
            " -0.04245933 -0.42711082  0.06538643  0.3475599  -0.05656205  0.90137786\n",
            " -0.16555946  0.40316078  0.62944657 -0.02373476 -0.21070673  0.9099062\n",
            "  0.11251789 -0.26934597  0.43006074  0.13308924  0.01266011  1.1084347\n",
            " -0.6444152  -0.26977268  0.49808946  0.55584484 -0.5843349   0.04482013\n",
            " -0.515387    0.1402223  -0.8183983   0.74695754  0.03560394  0.00817245\n",
            "  0.1655308  -0.06610072  0.02152579  0.33753836  0.07367989 -0.6238033\n",
            " -0.21597773 -0.93055934 -0.8118985   0.25342715 -1.4357344   1.415712\n",
            "  0.1332011  -0.17643274  0.13542676  0.33899653 -0.06463601 -0.5165546\n",
            "  0.2837182  -0.6124945   0.10158012  0.9429001   0.36293688 -0.5771522\n",
            "  0.73517877  0.05387988 -0.01229002  1.074562   -0.2979187   0.2259622\n",
            " -0.7895591   0.07621132 -0.8676872   0.05503438 -0.4220234  -0.2472229\n",
            "  0.7179516  -0.48599708  0.09425151  0.1313007   0.03791373  0.5376577\n",
            " -0.02484883 -0.54776096  0.5781705  -0.0223493  -0.43399248  0.07913791\n",
            " -0.20445873 -0.84903425  0.3494067   0.07521513  0.01585904 -0.03700646\n",
            " -0.5393519   0.9967479  -0.5291936   0.07660475 -0.6298866   0.376946\n",
            "  0.6465481  -0.6805194  -0.23274113  0.13296975 -0.32255557  0.03900522\n",
            " -0.21039577  0.15832056 -0.04002943  0.3242421  -0.3668038  -0.44330278\n",
            "  0.12302208 -0.6230958  -0.32114607 -0.44064736 -0.44129393  0.04315658\n",
            "  0.7388205  -0.11662663 -0.2153499  -0.48798725 -0.5543091   0.28348884\n",
            " -0.15103951 -0.11734018  0.2792718   0.11508223  0.5487623  -0.22284293\n",
            "  0.22007082  0.04879246  0.48193893 -0.3605679  -0.66778284 -0.10734018\n",
            "  0.8990573   0.33693364  0.37071463  1.1998435   0.18248914  0.20661233\n",
            " -0.603693    0.2983481   0.13383748 -0.07413048 -0.10890191 -0.36767673\n",
            "  0.28916538  0.05724201 -0.5893344  -0.22025685 -0.495118    0.89435405\n",
            " -1.0165033   0.20144749  0.09521285 -0.03948726  0.22474593 -0.578797\n",
            "  0.15089352  0.80459094  0.3010153   0.33656967 -0.6245528   0.34750733\n",
            "  0.26642266  0.96951205  0.32540706 -0.21359466  0.48912755  0.2922689\n",
            " -0.53395116 -0.38122544 -0.2301306   0.38195443 -0.47305295 -0.22562377\n",
            " -0.12222699 -0.16269344 -0.0659432   0.1194461   0.00626665  1.0400691 ]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-ha_CxEIB45"
      },
      "source": [
        "We recommend visiting the documentation to view other uses of these sentence embeddings, such as [clustering](https://www.sbert.net/examples/applications/clustering/README.html), [Paraphrase Mining](https://www.sbert.net/examples/applications/paraphrase-mining/README.html), [Semantic Search](https://www.sbert.net/examples/applications/semantic-search/README.html), [Retreive and Rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html), and even multi-modal [Image Search](https://www.sbert.net/examples/applications/image-search/README.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM-2pRuUa5Os"
      },
      "source": [
        "# BERTology\n",
        "\n",
        "\"BERTology\" refers to the growing number of papers disecting BERT and it's inner workings. \n",
        "\n",
        "- [A Primer in BERTology: What we know about how BERT works](https://arxiv.org/abs/2002.12327)\n",
        "\n",
        "- [Huggingface Transformers page on BERTology](https://huggingface.co/transformers/bertology.html)\n",
        "\n",
        "### Visualizing Attention\n",
        "\n",
        "Visualising the Attention layers are a popular way to understand BERT.\n",
        "\n",
        "**We highly recommend this visual blog on [explaining Transformers](http://jalammar.github.io/explaining-transformers/\n",
        ") and this visual blog on [visualising hidden states](http://jalammar.github.io/hidden-states/\n",
        ") before proceeding**\n",
        "\n",
        "The package \"bertviz\" [(GitHub link)](https://github.com/jessevig/bertviz) allows us to look into specific layers of multiple Transformers based models.\n",
        "\n",
        "This [blog post](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1) walks through the architecture of BERT and what exactly we can visualise.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exm70noGa81x"
      },
      "source": [
        "# code which visualises BERT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsnU6W6p4e0J"
      },
      "source": [
        "# Homework\n",
        "\n",
        "In this notebook we saw some of the state of the art language models, as well as sequence to sequence models which served as the very early versions of such models. Text and language models had its own ImageNet moment with the deluge of large pre-trained language models in 2018 and onwards.\n",
        "\n",
        "For the homework, you will be using BERT, GPT, or a similar large Transformer based model to analyse your social scientific textual dataset.\n",
        "\n",
        "There are a total of 6 standard questions, you will be graded for 4/6, and there are two bonus questions.\n",
        "\n",
        "**1a)** Use a large pre-trained language model for sequence classication, question answering, language modelling, or any other NLP task. Fine-tune the model on your dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggij-U914fvz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2rdyH-GLyj2"
      },
      "source": [
        "**1b)** What model did you use? How did your model perform?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5Yn_re2L300"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpTIL2NbL5TB"
      },
      "source": [
        "**2a)** Use a different large pre-trained language model for sequence classication, question answering, language modelling, or any other NLP task. Fine-tune the model on your dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FINzXTJBL_Ft"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ-NR_slMBMO"
      },
      "source": [
        "**2b)** What model did you use? How did your model perform?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtMzWp1uMCzV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qSjegARMEms"
      },
      "source": [
        "**3a)** Use BERT or a related model to create word and sentence embeddings from your corpus, and perform different similarity metrics on your embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRwBdNBXMjPd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPN6WV1eMjkB"
      },
      "source": [
        "**3b)** How does fine-tuning your model change the similarity metrics?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1os9EzltMn9Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZoyb9z3NADe"
      },
      "source": [
        "**4)** BONUS: Fine tune a Transformers based model and use bertviz to visualise its attention layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlbGm4W4NIqS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hGeVM6MMogG"
      },
      "source": [
        "**5)** BONUS: use a PyTorch or Keras sequence to sequence model to perform a task other than machine translation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euMHNChbMyT-"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}